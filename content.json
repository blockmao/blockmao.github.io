{"pages":[{"title":"分类","text":"","link":"/categories/index.html"},{"title":"标签","text":"","link":"/tags/index.html"}],"posts":[{"title":"Class.forName和ClassLoader的区别","text":"原文地址：http://www.importnew.com/29389.html 在java中Class.forName()和ClassLoader都可以对类进行加载。ClassLoader就是遵循双亲委派模型最终调用启动类加载器的类加载器，实现的功能是“通过一个类的全限定名来获取描述此类的二进制字节流”，获取到二进制流后放到JVM中。 Class.forName()方法实际上也是调用的ClassLoader来实现的。 123456789101112131415161718192021222324public static Class&lt;?&gt; forName(String className) throws ClassNotFoundException { Class&lt;?&gt; caller = Reflection.getCallerClass(); return forName0(className, true, ClassLoader.getClassLoader(caller), caller);}public static Class&lt;?&gt; forName(String name, boolean initialize, ClassLoader loader) throws ClassNotFoundException { Class&lt;?&gt; caller = null; SecurityManager sm = System.getSecurityManager(); if (sm != null) { // Reflective call to get caller class is only needed if a security manager // is present. Avoid the overhead of making this call otherwise. caller = Reflection.getCallerClass(); if (sun.misc.VM.isSystemDomainLoader(loader)) { ClassLoader ccl = ClassLoader.getClassLoader(caller); if (!sun.misc.VM.isSystemDomainLoader(ccl)) { sm.checkPermission( SecurityConstants.GET_CLASSLOADER_PERMISSION); } } } return forName0(name, initialize, loader, caller);} 在forName0方法中的第二个参数被默认设置为了true，这个参数代表是否对加载的类进行初始化，设置为true时会类进行初始化，代表会执行类中的静态代码块，以及对静态变量的赋值等操作。 举例一个含有静态代码块、静态变量、赋值给静态变量的静态方法的类。代码如下： 123456789101112131415public class ClassForName { //静态代码块 static { System.out.println(&quot;执行了静态代码块&quot;); } //静态变量 private static String staticFiled = staticMethod(); //赋值静态变量的静态方法 public static String staticMethod(){ System.out.println(&quot;执行了静态方法&quot;); return &quot;给静态字段赋值了&quot;; }} 测试方法： 1234567891011public class MyTest { @Test public void test44(){ try { Class.forName(&quot;com.test.mytest.ClassForName&quot;); System.out.println(&quot;#########分割符(上面是Class.forName的加载过程，下面是ClassLoader的加载过程)##########&quot;); ClassLoader.getSystemClassLoader().loadClass(&quot;com.test.mytest.ClassForName&quot;); } catch (ClassNotFoundException e) { e.printStackTrace(); }} 运行结果： 123执行了静态代码块执行了静态方法#########分割符(上面是Class.forName的加载过程，下面是ClassLoader的加载过程)########## 根据运行结果得出Class.forName加载类是将类进了初始化，而ClassLoader的loadClass并没有对类进行初始化，只是把类加载到了虚拟机中。 应用场景在我们熟悉的Spring框架中的IOC的实现就是使用的ClassLoader，而在我们使用JDBC时通常是使用Class.forName()方法来加载数据库连接驱动。这是因为在JDBC规范中明确要求Driver(数据库驱动)类必须向DriverManager注册自己。","link":"/posts/42138.html"},{"title":"Dubbo：SPI机制","text":"Dubbo SPI概述Dubbo采用微内核+插件模式的设计原则，微内核负责组装插件，也就是Dubbo的所有功能点都可被用户自定义扩展所替换。 微内核是由Dubbo SPI机制实现的，因此了解Dubbo SPI是非常重要的。Dubbo SPI从JDK标准的SPI (Service Provider Interface) 扩展点发现机制加强而来，改进了JDK标准的 SPI 的以下问题： 无法获取指定的扩展实现 。JDK 标准的 SPI 会一次性实例化扩展点所有实现，如果有扩展实现初始化很耗时，但如果没用上也加载，会很浪费资源。 如果扩展点加载失败，连扩展点的名称都拿不到了。比如：JDK标准的ScriptEngine，通过getName() 获取脚本类型的名称，但如果RubyScriptEngine因为所依赖的 jruby.jar不存在导致 RubyScriptEngine 类加载失败，这个失败原因被吃掉了，和ruby对应不起来，当用户执行ruby脚本时，会报不支持ruby，而不是真正失败的原因。 增加了对扩展点IoC和AOP的支持，一个扩展点可以直接setter注入其它扩展点。 如果不了解JDK SPI机制，可以看我写的JDK SPI源码详解。 Dubbo SPI机制源码分析JDK SPI机制是由java.util.ServiceLoader这个工具类实现的，同样Dubbo也提供了类似的类(com.alibaba.dubbo.common.extension.ExtensionLoader)来实现的Dubbo SPI机制。 下面的内容主要讲解ExtensionLoader如何获取指定的扩展点和自适应扩展点 获取指定的扩展点通过获取DubboProtocol为例，其代码如下： 1Protocol protocol = ExtensionLoader.getExtensionLoader(Protocol.class).getExtension(&quot;dubbo&quot;); 通过具体的源码分析，其运行过程如下： 1、创建ExtensionLoader实例 1234// Dubbo SPI只支持接口并且被@SPI修饰ExtensionLoader loader = new ExtensionLoader(Protocol.class, ExtensionLoader.getExtensionLoader(ExtensionFactory.class).getAdaptiveExtension()) 2、获取扩展点所有实现类的Class对象 1234567891011/** * 加载META-INF/services/、META-INF/dubbo/internal/、META-INF/dubbo/目录下type.getName文件 * 并解析内容，然后将type基本实现类（不包括包装类，没有Adaptive注解）存储在extensionClasse中。 */private Map&lt;String, Class&lt;?&gt;&gt; loadExtensionClasses() { Map&lt;String, Class&lt;?&gt;&gt; extensionClasses = new HashMap&lt;String, Class&lt;?&gt;&gt;(); loadFile(extensionClasses, DUBBO_INTERNAL_DIRECTORY); loadFile(extensionClasses, DUBBO_DIRECTORY); loadFile(extensionClasses, SERVICES_DIRECTORY); return extensionClasses;} 3、创建DubboProtocol对象并装配其依赖的对象 123456// 获取名为dubbo对应的扩展点实现类Class&lt;?&gt; clazz = getExtensionClasses().get(name);// 创建对象T instance = clazz.newInstance();// 自动装配injectExtension(instance); 4、返回ProtocolListenerWrapper对象 123456789Set&lt;Class&lt;?&gt;&gt; wrapperClasses = cachedWrapperClasses;// 循环遍历初始化包装类if (wrapperClasses != null &amp;&amp; wrapperClasses.size() &gt; 0) { for (Class&lt;?&gt; wrapperClass : wrapperClasses) { instance = injectExtension((T) wrapperClass.getConstructor(type) .newInstance(instance)); }}return instance; 自动注入过程获取需要依赖的对象，是通过ExtensionLoader.objectFactory对象获取的。 获取自适应扩展点通过获取Procotol自适应扩展点为例，其代码如下： 1Protocol protocol = ExtensionLoader.getExtensionLoader(Protocol.class).getAdaptiveExtension(); 其过程如下： 123456789101112131415161718192021222324252627// 创建Protocol自适应扩展点实例private T createAdaptiveExtension() { return injectExtension((T) getAdaptiveExtensionClass().newInstance());}/** * 之所以贴出这段代码，因为获取自适应扩展点会触发获取扩展点所有实现类的Class对象。目前 * dubbo只有被@Adaptive注解的类仅有AdaptiveCompiler和AdaptiveExtensionFactory，因此除了 * Complie和ExtensionFactory外都需要动态创建其自适应扩展点的Class */private Class&lt;?&gt; getAdaptiveExtensionClass() { getExtensionClasses(); if (cachedAdaptiveClass != null) { return cachedAdaptiveClass; } return cachedAdaptiveClass = createAdaptiveExtensionClass();}// 创建自适应扩展点类Class对象private Class&lt;?&gt; createAdaptiveExtensionClass() { String code = createAdaptiveExtensionClassCode(); ClassLoader classLoader = findClassLoader(); com.alibaba.dubbo.common.compiler.Compiler compiler = ExtensionLoader.getExtensionLoader(com.alibaba.dubbo.common.compiler.Compiler.class) .getAdaptiveExtension(); return compiler.compile(code, classLoader);} 动态生成的自适应扩展点Protocol$Adpative类源码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class Protocol$Adpative implements com.alibaba.dubbo.rpc.Protocol { public void destroy() { throw new UnsupportedOperationException(&quot;&quot;); } public int getDefaultPort() { throw new UnsupportedOperationException(&quot;&quot;); } public com.alibaba.dubbo.rpc.Invoker refer(java.lang.Class arg0, com.alibaba.dubbo.common.URL arg1) throws java.lang.Class { if (arg1 == null) throw new IllegalArgumentException(&quot;url == null&quot;); com.alibaba.dubbo.common.URL url = arg1; String extName = (url.getProtocol() == null ? &quot;dubbo&quot; : url.getProtocol()); if (extName == null) throw new IllegalStateException(&quot;&quot;); com.alibaba.dubbo.rpc.Protocol extension = (com.alibaba.dubbo.rpc.Protocol) ExtensionLoader .getExtensionLoader(com.alibaba.dubbo.rpc.Protocol.class) .getExtension(extName); return extension.refer(arg0, arg1); } public com.alibaba.dubbo.rpc.Exporter export(com.alibaba.dubbo.rpc.Invoker arg0) throws com.alibaba.dubbo.rpc.Invoker { if (arg0 == null) throw new IllegalArgumentException(&quot;&quot;); if (arg0.getUrl() == null) throw new IllegalArgumentException(&quot;&quot;); com.alibaba.dubbo.common.URL url = arg0.getUrl(); String extName = (url.getProtocol() == null ? &quot;dubbo&quot; : url.getProtocol()); if (extName == null) throw new IllegalStateException(&quot;&quot;); com.alibaba.dubbo.rpc.Protocol extension = (com.alibaba.dubbo.rpc.Protocol) ExtensionLoader .getExtensionLoader(com.alibaba.dubbo.rpc.Protocol.class) .getExtension(extName); return extension.export(arg0); }} 可以发现自适应扩展点的作用：通过URL的参数信息获取对应扩展点，从而实现方法动态调用。对应Dubbo基本设计原二：采用URL作为配置信息的统一格式，所有扩展点都通过传递URL携带配置信息。","link":"/posts/60280.html"},{"title":"DDD：战术篇","text":"领域驱动设计DDD在战术建模上提供了一个元模型体系，如下图所示： 通过这个元模型我们会对战略建模过程中识别出来的问题子域进行抽象，而通过抽象来指导最后的落地实现。 实体实体拥有唯一标识符，且标识符在历经各种状态变更后仍能保持一致。对实体对象而言，重要的不是其属性，而是其延续性和标识，对象的延续性和标识会跨越甚至超出软件的生命周期。 比如商品是商品上下文的一个实体，通过唯一的商品 ID 来标识，不管这个商品的数据如何变化，商品的 ID 一直保持不变，它始终是同一个商品。 值对象值对象的定义：通过对象属性值来识别的对象，它将多个相关属性组合为一个概念整体。在 DDD 中用来描述领域的特定方面，并且是一个没有标识符的对象，叫作值对象。 值对象描述了领域中的一件东西，这个东西是不可变的，它将不同的相关属性组合成了一个概念整体。当度量和描述改变时，可以用另外一个值对象予以替换。它可以和其它值对象进行相等性比较，且不会对协作对象造成副作用。 简单来说，值对象本质上就是一个集。那这个集合里面有什么呢？若干个用于描述目的、具有整体概念和不可修改的属性。在领域建模的过程中，值对象可以保证属性归类的清晰和概念的完整性，避免属性零碎。 实体和值对象关系实体和值对象是最基础的领域对象，一起实现实体最基本的核心领域逻辑。 值对象和实体在某些场景下可以互换，很多 DDD 专家在这些场景下，其实也很难判断到底将领域对象设计成实体还是值对象？ 同样的对象在不同的场景下，可能会设计出不同的结果。有些场景中，地址会被某一实体引用，它只承担描述实体的作用，并且它的值只能整体替换，这时候你就可以将地址设计为值对象，比如收货地址。而在某些业务场景中，地址会被经常修改，地址是作为一个独立对象存在的，这时候它应该设计为实体，比如行政区划中的地址信息维护。 聚合社会是由一个个的个体组成的，象征着我们每一个人。随着社会的发展，慢慢出现了社团、机构、部门等组织，我们开始从个人变成了组织的一员，大家可以协同一致的工作，朝着一个最大的目标前进，发挥出更大的力量。 领域模型内的实体和值对象就好比个体，而能让实体和值对象协同工作的组织就是聚合，它用来确保这些领域对象在实现共同的业务逻辑时，能保证数据的一致性。 聚合就是由业务和逻辑紧密关联的实体和值对象组合而成的，聚合是数据修改和持久化的基本单元，每一个聚合对应一个仓储，实现数据的持久化。 聚合有一个聚合根和上下文边界，这个边界根据业务单一职责和高内聚原则，定义了聚合内部应该包含哪些实体和值对象，而聚合之间的边界是松耦合的。按照这种方式设计出来的微服务很自然就是“高内聚、低耦合”的。 聚合内实体以充血模型实现个体业务能力，以及业务逻辑的高内聚。跨多个实体的业务逻辑通过领域服务来实现，跨多个聚合的业务逻辑通过应用服务来实现。 比如有的业务场景需要同一个聚合的 A 和 B 两个实体来共同完成，我们就可以将这段业务逻辑用领域服务来实现；而有的业务逻辑需要聚合 C 和聚合 D 中的两个服务共同完成，这时你就可以用应用服务来组合这两个服务。 聚合根聚合根的主要目的是为了避免由于复杂数据模型缺少统一的业务规则控制，而导致聚合、实体之间数据不一致性的问题。 如果把聚合比作组织，那聚合根就是这个组织的负责人。聚合根也称为根实体，它不仅是实体，还是聚合的管理者。 首先它作为实体本身，拥有实体的属性和业务行为，实现自身的业务逻辑。 其次它作为聚合的管理者，在聚合内部负责协调实体和值对象按照固定的业务规则协同完成共同的业务逻辑。 最后在聚合之间，它还是聚合对外的接口人，以聚合根 ID 关联的方式接受外部任务和请求，在上下文内实现聚合之间的业务协同。也就是说，聚合之间通过聚合根 ID 关联引用，如果需要访问其它聚合的实体，就要先访问聚合根，再导航到聚合内部实体，外部对象不能直接访问聚合内实体。 聚合设计原则原则一：在一致性边界内建模真正的不变条件聚合用来封装真正的不变性，而不是简单地将对象组合在一起。聚合内有一套不变的业务规则，各实体和值对象按照统一的业务规则运行，实现对象数据的一致性，边界之外的任何东西都与该聚合无关，这就是聚合能实现业务高内聚的原因。 原则二：设计小聚合如果聚合设计得过大，聚合会因为包含过多的实体，导致实体之间的管理过于复杂，高频操作时会出现并发冲突或者数据库锁，最终导致系统可用性变差。而小聚合设计则可以降低由于业务过大导致聚合重构的可能性，让领域模型更能适应业务的变化。 如何确定聚合的边界并避免出现臃肿的设计，同时还要维持并保证真实业务不变性规则的一致性边界？ 将重点先放在聚合设计的原则二上：“聚合要设计得小巧”。每个聚合一开始创建时只允许包含一个实体，并且它将作为聚合根。 现在将重点放在聚合设计的原则一上：“聚合边界内保护业务不变性规则”。一个一个地检查每个聚合。在检查聚合A1时，问问领域专家需不需要更新其他已定义的聚合，来响应聚合A1发生的改变。为每个聚合和它的一致性规则制作一个清单，还要记录所有这些基于响应的更新的时间范围。 询问领域专家，每个基于响应的更新可以等待多长时间。答案会是两种： （a）即时发生； （b）在n秒/分/小时/天之内发生。一种可行的寻找正确的业务阈值的方法是，先抛出一个夸张到显然无法接受的时间范围（比如几周或几个月）。业务专家很可能会据此提出一个可接受的时间范围作为回应。 对每一个即时发生的时间范围（3a），应该坚定地考虑把这两个实体合并到同一个聚合的边界之内。 对于每一个在给定等待时间（3b）内更新的响应聚合，将使用聚合设计的规则四来更新它们：“利用最终一致性更新其他聚合”。 原则三：通过唯一标识引用其它聚合聚合之间是通过关联外部聚合根 ID 的方式引用，而不是直接对象引用的方式。外部聚合的对象放在聚合边界内管理，容易导致聚合的边界不清晰，也会增加聚合之间的耦合度。 原则四： 在边界之外使用最终一致性聚合内数据强一致性，而聚合之间数据最终一致性。在一次事务中，最多只能更改一个聚合的状态。如果一次业务操作涉及多个聚合状态的更改，应采用领域事件的方式异步修改相关的聚合，实现聚合之间的解耦（相关内容我会在领域事件部分详解）。","link":"/posts/2050.html"},{"title":"DDD：战略篇","text":"领域如何理解领域和子域？在研究和解决业务问题时，DDD 会按照一定的规则将业务领域进行细分，当领域细分到一定的程度后，DDD 会将问题范围限定在特定的边界内，在这个边界内建立领域模型，进而用代码实现该领域模型，解决相应的业务问题。 简言之，DDD 的领域就是这个边界内要解决的业务问题域。 既然领域是用来限定业务边界和范围的，那么就会有大小之分，领域越大，业务范围就越大，反之则相反。领域可以进一步划分为子领域。把划分出来的多个子领域称为子域，每个子域对应一个更小的问题域或更小的业务范围。 DDD 的研究方法与自然科学的研究方法类似。当人们在自然科学研究中遇到复杂问题时，通常的做法就是将问题一步一步地细分，再针对细分出来的问题域，逐个深入研究，探索和建立所有子域的知识体系。当所有问题子域完成研究时，就建立了全部领域的完整知识体系了。 来看一下上面这张图。这个例子是在讲如何给桃树建立一个完整的生物学知识体系，初中生物课其实早就告诉我们研究方法了。它的研究过程是这样的。 确定研究对象，即研究领域，这里是一棵桃树。 对研究对象进行细分，将桃树细分为器官，器官又分为营养器官和生殖器官两种。其中营养器官包括根、茎和叶，生殖器官包括花、果实和种子。桃树的知识体系是我们已经确定要研究的问题域，对应 DDD 的领域。根、茎、叶、花、果实和种子等器官则是细分后的问题子域。这个过程就是 DDD 将领域细分为多个子域的过程。 对器官进行细分，将器官细分为组织。比如，叶子器官可细分为保护组织、营养组织和输导组织等。这个过程就是 DDD 将子域进一步细分为多个子域的过程。 对组织进行细分，将组织细分为细胞，细胞成为我们研究的最小单元。细胞之间的细胞壁确定了单元的边界，也确定了研究的最小边界。 细胞核、线粒体、细胞膜等物质共同构成细胞，这些物质一起协作让细胞具有这类细胞特定的生物功能。可以把细胞理解为 DDD 的聚合，细胞内的这些物质就可以理解为聚合里面的聚合根、实体以及值对象等，在聚合内这些实体一起协作完成特定的业务功能。 总结一下，就是说每一个细分的领域都会有一个知识体系，也就是 DDD 的领域模型。在所有子域的研究完成后，我们就建立了全域的知识体系了，也就建立了全域的领域模型。 子域类型 核心域（Sub Domain） 它是一个唯一的、定义明确的领域模型，要对它进行战略投资，并在一个明确的限界上下文中投入大量资源去精心打磨通用语言。它是组织中最重要的项目，因为这将是你与其他竞争者的区别所在。 正是因为你的组织无法在所有领域都出类拔萃，所以必须把核心域打造成组织的核心竞争力。做出这样的决定需要对核心域进行深入的学习与理解，而这需要承诺、协作与试验。 支撑子域（Supporting Subdomain） 这类建模场景提倡的是“定制开发”，因为找不到现成的解决方案。对它的投入无论如何也达不到与核心域相同的程度。也许会考虑使用外包的方式实现此类限界上下文，以避免因错误地认为其具有战略意义而进行巨额的投资。这类软件模型仍旧非常重要，核心域的成功离不开它。 通用子域（Generic Subdomain） 通用子域的解决方案可以采购现成的，也可以采用外包的方式，抑或是由内部团队实现，但我们不用为其分配与核心域同样优质的研发资源，甚至都不如支撑子域。 子域的划分主要目的? ​ 公司在 IT 系统建设过程中，由于预算和资源有限，对不同类型的子域应有不同的关注度和资源投入策略，记住好钢要用在刀刃上。 通用语言怎么理解通用语言这个概念呢？ 在事件风暴过程中，通过团队交流达成共识的，能够简单、清晰、准确描述业务涵义和规则的语言就是通用语言。也就是说，通用语言是团队统一的语言，不管你在团队中承担什么角色，在同一个领域的软件生命周期里都使用统一的语言进行交流。 那么，通用语言的价值也就很明确了，它可以解决交流障碍这个问题，使领域专家和开发人员能够协同合作，从而确保业务需求的正确表达。 但是，对这个概念的理解，到这里还不够。 通用语言包含术语和用例场景，并且能够直接反映在代码中。通用语言中的名词可以给领域对象命名，如商品、订单等，对应实体对象；而动词则表示一个动作或事件，如商品已下单、订单已付款等，对应领域事件或者命令。 下图描述了从事件风暴建立通用语言到领域对象设计和代码落地的完整过程。 在事件风暴的过程中，领域专家会和设计、开发人员一起建立领域模型，在领域建模的过程中会形成通用的业务术语和用户故事。事件风暴也是一个项目团队统一语言的过程。 通过用户故事分析会形成一个个的领域对象，这些领域对象对应领域模型的业务对象，每一个业务对象和领域对象都有通用的名词术语，并且一一映射。 微服务代码模型来源于领域模型，每个代码模型的代码对象跟领域对象一一对应。 限界上下文语言都有它的语义环境，同样，通用语言也有它的上下文环境。为了避免同样的概念或语义在不同的上下文环境中产生歧义，DDD 在战略设计上提出了“限界上下文”这个概念，用来确定语义所在的领域边界。 将限界上下文拆解为两个词：限界和上下文。限界就是领域的边界，而上下文则是语义环境。通过领域的限界上下文，我们就可以在统一的领域边界内用统一的语言进行交流。 综合一下，限界上下文的定义就是：用来封装通用语言和领域对象，提供上下文环境，保证在领域之内的一些术语、业务相关对象等（通用语言）有一个确切的含义，没有二义性。这个边界定义了模型的适用范围，使团队所有成员能够明确地知道什么应该在模型中实现，什么不应该在模型中实现。 可以通过一些例子进一步理解一下这个概念，不要小看它。在不同的时空和背景下，同样的一句话会有不同的涵义。 在一个明媚的早晨，孩子起床问妈妈：“今天应该穿几件衣服呀？”妈妈回答：“能穿多少就穿多少！” 那到底是穿多还是穿少呢？ 如果没有具体的语义环境，还真不太好理解。但是，如果你已经知道了这句话的语义环境，比如是寒冬腊月或者是炎炎夏日，那理解这句话的涵义就会很容易了。 上下文映射通常一个系统会存在多个限界上下文，那它们之间如何集成？ 合作关系 合作关系（Partnership）关系存在于两个团队之间，每个团队各自负责一个限界上下文，两个团队通过互相依赖的一套目标联合起来形成合作关系。一损俱损，一荣俱荣。 保持长期的合作关系很有挑战性，因此许多进入合作关系的团队可能会尽最大努力为这种关系设置一个期限。只有在能发挥彼此优势时才维持合作关系，而随着承诺的消失，这些优势会不复存在，而这种合作关系应该被重新映射成另外的一种关系 共享内核 共享内核（SharedKernel）用上图中两个限界上下文的交集表示，它描述了这样一种关系：两个（或更多）团队之间共享着一个小规模但却通用的模型。团队必须就要共享的模型元素达成一致。 客户—供应商 客户—供应商（Customer-Supplier）描述的是两个限界上下文之间和两个独立团队之间的一种关系：供应商位于上游，客户位于下游，支配这种关系的是供应商，因为它必须提供客户需要的东西。 客户需要与供应商共同制订规划来满足各种预期，但最终却是由供应商来决定客户获得的是什么以及何时获得。 跟随者 跟随者（Conformist）关系存在于上游团队和下游团队之间，上游团队没有任何动机满足下游团队的具体需求。由于各种原因，下游团队也无法投入资源去翻译上游模型的通用语言来适应自己的特定需求，因此只能顺应上游的模型。 例如，当一个团队需要与一个非常庞大复杂的模型集成，而且这个模型已经非常成熟时，团队往往会成为它的跟随者。 防腐层 防腐层（AnticorruptionLayer）是最具防御性的上下文映射关系，下游团队在其通用语言（模型）和位于它上游的通用语言（模型）之间创建了一个翻译层。防腐层隔离了下游模型与上游模型，并完成两者之间的翻译。 但凡有可能，你就应该尝试在下游模型和上游集成模型之间创建一个防腐层，这样才可以在你这端的集成中创造出特别适合业务需求的模型概念，并将外部概念完全地隔离。 开放主机服务 开放式主机服务（OpenHostService）会定义一套协议或接口，让限界上下文可以被当作一组服务访问。该协议是“开放的”，所有需要与限界上下文进行集成的客户端都可以相对轻松地使用它。 已发布语言 已发布语言（PublishedLanguage）是一种有着丰富文档的信息交换语言，可以被许多消费方的限界上下文简单地使用和翻译。需要读写信息的消费者们可以把共享语言翻译成自己的语言，反之亦然，而在此过程中它们对集成的正确性充满信心。","link":"/posts/56946.html"},{"title":"Dubbo：服务发布","text":"Dubbo服务发布主流程Dubbo官方文档说明了服务提供者暴露服务的主过程，如图所示： 首先ServiceConfig类拿到对外提供服务的实际类ref(如：HelloWorldImpl),然后通过ProxyFactory类的 getInvoker方法使用ref生成一个AbstractProxyInvoker实例，到这一步就完成具体服务到Invoker的转化。接下来就是Invoker转换到Exporter的过程。Dubbo 处理服务暴露的关键就在Invoker转换到Exporter的过程，上图中的红色部分。 源码分析入口分析——ServiceBean服务发布在spring的配置文件中配置如下： 1234567891011121314&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:dubbo=&quot;http://code.alibabatech.com/schema/dubbo&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd&quot;&gt; &lt;bean id=&quot;demoService&quot; class=&quot;com.alibaba.dubbo.demo.provider.DemoServiceImpl&quot; /&gt; &lt;dubbo:service interface=&quot;com.alibaba.dubbo.demo.DemoService&quot; ref=&quot;demoService&quot; /&gt;&lt;/beans&gt; \b如果熟悉Spring自定义标签，应该知道\b&lt;dubbo:service&gt;标签会转换成ServiceBean。\b通过DubboNamespaceHandler.init()方法可知，其源码如下： 12registerBeanDefinitionParser(&quot;service&quot;, new DubboBeanDefinitionParser(ServiceBean.class, true)); ServiceBean类结构如下图所示： dubbo暴露服务有两种情况，一种是设置了延迟暴露（比如delay=”5000”），另外一种是没有设置延迟暴露或者延迟设置为-1（delay=”-1”）： 1、设置了延迟暴露，dubbo在Spring实例化bean（initializeBean）的时候会对实现了InitializingBean的类进行回调，回调方法是afterPropertySet()，如果设置了延迟暴露，dubbo在这个方法中进行服务的发布。 2、没有设置延迟或者延迟为-1，dubbo会在Spring实例化完bean之后，在刷新容器最后一步发布ContextRefreshEvent事件的时候，通知实现了ApplicationListener的类进行回调onApplicationEvent，dubbo会在这个方法中发布服务。 但是不管延迟与否，都是使用ServiceConfig的export()方法进行服务的暴露。使用export初始化的时候会将Bean对象转换成URL格式，所有Bean属性转换成URL的参数。 ServiceConfig.export()方法ServiceConfig.export()的流程图如下： 1、export()方法先判断是否需要延迟暴露，如果设置了延迟，则通过一个后台线程调用doExport()方法；反之，直接调用doExport()方法。 2、doExport方法先执行一系列的检查方法，然后调用doExportUrls方法。检查方法会检测dubbo的配置是否在Spring配置文件中声明，没有的话读取properties文件初始化。 3、doExportUrls方法先调用loadRegistries获取所有的注册中心url，然后遍历调用doExportUrlsFor1Protocol方法。 4、doExportUrlsFor1Protocol()先将Bean属性转换成URL对象，然后根据不同协议将\b服务已URL形式发布。如果scope配置为none则不暴露，如果服务未配置成remote，则本地暴露exportLocal，如果未配置成local，则远程暴露。 暴露服务的核心是由doExportUrlsFor1Protocol()方法处理的，其源码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546private void doExportUrlsFor1Protocol(ProtocolConfig protocolConfig, List&lt;URL&gt; registryURLs) { // 1、Bean属性转换URL URL url = new URL(name, host, port, (contextPath == null || contextPath.length() == 0 ? &quot;&quot; : contextPath + &quot;/&quot;) + path, map); if (ExtensionLoader.getExtensionLoader(ConfiguratorFactory.class) .hasExtension(url.getProtocol())) { url = ExtensionLoader.getExtensionLoader(ConfiguratorFactory.class) .getExtension(url.getProtocol()).getConfigurator(url).configure(url); } String scope = url.getParameter(Constants.SCOPE_KEY); if (! Constants.SCOPE_NONE.toString().equalsIgnoreCase(scope)) { // scope配置不是remote的情况下做本地暴露 if (!Constants.SCOPE_REMOTE.toString().equalsIgnoreCase(scope)) { exportLocal(url); } // scope配置不是local则暴露为远程服务 if (!Constants.SCOPE_LOCAL.toString().equalsIgnoreCase(scope) ){ if (registryURLs != null &amp;&amp; registryURLs.size() &gt; 0 &amp;&amp; url.getParameter(&quot;register&quot;, true)) { for (URL registryURL : registryURLs) { url = url.addParameterIfAbsent(&quot;dynamic&quot;, registryURL.getParameter(&quot;dynamic&quot;)); URL monitorUrl = loadMonitor(registryURL); if (monitorUrl != null) { url = url.addParameterAndEncoded(Constants.MONITOR_KEY, monitorUrl.toFullString()); } // 2、具体服务到invoker的转换 Invoker&lt;?&gt; invoker = proxyFactory.getInvoker(ref, (Class) interfaceClass, registryURL.addParameterAndEncoded(Constants.EXPORT_KEY, url.toFullString())); // 3、invoker转换为exporter Exporter&lt;?&gt; exporter = protocol.export(invoker); exporters.add(exporter); } } else { Invoker&lt;?&gt; invoker = proxyFactory.getInvoker(ref, (Class) interfaceClass, url); Exporter&lt;?&gt; exporter = protocol.export(invoker); exporters.add(exporter); } } } this.urls.add(url);} 具体服务到Invoker转换在分析具体服务到Invoker之前，先来看看ServiceConfig的proxyFactory实例。其定义如下： 1private static final ProxyFactory proxyFactory = ExtensionLoader.getExtensionLoader(ProxyFactory.class).getAdaptiveExtension(); ProxyFactory的自适应扩展点的getInvoker方法源码如下： 12345678910111213public com.alibaba.dubbo.rpc.Invoker getInvoker(java.lang.Object arg0, java.lang.Class arg1, com.alibaba.dubbo.common.URL arg2) throws RpcException { if (arg2 == null) throw new IllegalArgumentException(&quot;url == null&quot;); com.alibaba.dubbo.common.URL url = arg2; // 如果url没有proxy参数，默认为javassist String extName = url.getParameter(&quot;proxy&quot;, &quot;javassist&quot;); if (extName == null) throw new IllegalStateException(&quot;Fail to get extension(com.alibaba.dubbo.rpc.ProxyFactory) name from url(&quot; + url.toString() + &quot;) use keys([proxy])&quot;); com.alibaba.dubbo.rpc.ProxyFactory extension = (com.alibaba.dubbo.rpc.ProxyFactory) ExtensionLoader.getExtensionLoader(com.alibaba.dubbo.rpc.ProxyFactory.class).getExtension(extName); return extension.getInvoker(arg0, arg1, arg2); } 通过上述源码我们知道，最终获取invoker是由JavassistProxyFactory.getInvoker方法实现的，其源码如下： 12345678910111213public &lt;T&gt; Invoker&lt;T&gt; getInvoker(T proxy, Class&lt;T&gt; type, URL url) { // 获取具体服务类的包装对象 final Wrapper wrapper = Wrapper.getWrapper(proxy.getClass().getName().indexOf('$') &lt; 0 ? proxy.getClass() : type); // 返回构造invoker实例 return new AbstractProxyInvoker&lt;T&gt;(proxy, type, url) { @Override protected Object doInvoke(T proxy, String methodName, Class&lt;?&gt;[] parameterTypes, Object[] arguments) throws Throwable { return wrapper.invokeMethod(proxy, methodName, parameterTypes, arguments); } };} 流程总结： 1、根据url的proxy参数获取对应ProxyFactory类，默认为JavassistProxyFactory； 2、获取具体服务类的包装对象； 3、返回构造invoker实例，将服务具体类的方法调用封装在doInvoke()方法中。 远程暴露通过debug源码，protocol.export(invoker)的时序图如下： 流程总结 1、构建invoker过滤链； 2、首先DubboProcotol的export方法将invoker转换成DubboExporter，启动Server服务，然后将DubboExporter包装为ListenerExporterWrapper。 3、RegistryProtocol的export方法向注册中心注册服务提供者url和订阅url，再次将ListenerExporterWrapper包装为Exporter，覆盖unexport()方法。 RegistryProtocol.export()方法的注册和订阅下面我们分析下RegistryProtocol.export()方法的注册和订阅，其源码如下： 1234567891011121314151617181920212223242526272829303132333435363738public &lt;T&gt; Exporter&lt;T&gt; export(final Invoker&lt;T&gt; originInvoker) throws RpcException { //export invoker final ExporterChangeableWrapper&lt;T&gt; exporter = doLocalExport(originInvoker); //registry provider final Registry registry = getRegistry(originInvoker); final URL registedProviderUrl = getRegistedProviderUrl(originInvoker); registry.register(registedProviderUrl); // 订阅override数据 // provider://172.16.6.216:20880/com.alibaba.dubbo.demo.DemoService?anyhost=true&amp;application=demo-provider&amp;category=configurators&amp;check=false&amp;dubbo=2.5.3&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;loadbalance=roundrobin&amp;methods=sayHello&amp;owner=william&amp;pid=2154&amp;side=provider&amp;timestamp=1520235259270 final URL overrideSubscribeUrl = getSubscribedOverrideUrl(registedProviderUrl); final OverrideListener overrideSubscribeListener = new OverrideListener(overrideSubscribeUrl); overrideListeners.put(overrideSubscribeUrl, overrideSubscribeListener); registry.subscribe(overrideSubscribeUrl, overrideSubscribeListener); //保证每次export都返回一个新的exporter实例 return new Exporter&lt;T&gt;() { public Invoker&lt;T&gt; getInvoker() { return exporter.getInvoker(); } public void unexport() { try { exporter.unexport(); } catch (Throwable t) { logger.warn(t.getMessage(), t); } try { registry.unregister(registedProviderUrl); } catch (Throwable t) { logger.warn(t.getMessage(), t); } try { overrideListeners.remove(overrideSubscribeUrl); registry.unsubscribe(overrideSubscribeUrl, overrideSubscribeListener); } catch (Throwable t) { logger.warn(t.getMessage(), t); } } };} 注册由于使用zookeeper作为注册中心，所以registry为ZookeeperRegistry。ZookeeperRegistry.registry过程如下： ZookeeperRegistry.doRegister方法如下： 1234567protected void doRegister(URL url) { try { zkClient.create(toUrlPath(url), url.getParameter(Constants.DYNAMIC_KEY, true)); } catch (Throwable e) { throw new RpcException(&quot;Failed to register &quot; + url + &quot; to zookeeper &quot; + getUrl() + &quot;, cause: &quot; + e.getMessage(), e); }} doRegister方法将url转换为注册zookeeper的path，创建临时节点。 临时节点在zookeeper会话失效后会自动删除的，Dubbo如何解决这个问题。 1234567891011121314public FailbackRegistry(URL url) { super(url); int retryPeriod = url.getParameter(Constants.REGISTRY_RETRY_PERIOD_KEY, Constants.DEFAULT_REGISTRY_RETRY_PERIOD); this.retryFuture = retryExecutor.scheduleWithFixedDelay(new Runnable() { public void run() { // 检测并连接注册中心 try { retry(); } catch (Throwable t) { // 防御性容错 logger.error(&quot;Unexpected error occur at failed retry, cause: &quot; + t.getMessage(), t); } } }, retryPeriod, retryPeriod, TimeUnit.MILLISECONDS);} 订阅12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364protected void doSubscribe(final URL url, final NotifyListener listener) { try { if (Constants.ANY_VALUE.equals(url.getServiceInterface())) { String root = toRootPath(); ConcurrentMap&lt;NotifyListener, ChildListener&gt; listeners = zkListeners.get(url); if (listeners == null) { zkListeners.putIfAbsent(url, new ConcurrentHashMap&lt;NotifyListener, ChildListener&gt;()); listeners = zkListeners.get(url); } ChildListener zkListener = listeners.get(listener); if (zkListener == null) { listeners.putIfAbsent(listener, new ChildListener() { public void childChanged(String parentPath, List&lt;String&gt; currentChilds) { for (String child : currentChilds) { if (! anyServices.contains(child)) { anyServices.add(child); subscribe(url.setPath(child).addParameters(Constants.INTERFACE_KEY, child, Constants.CHECK_KEY, String.valueOf(false)), listener); } } } }); zkListener = listeners.get(listener); } zkClient.create(root, false); List&lt;String&gt; services = zkClient.addChildListener(root, zkListener); if (services != null &amp;&amp; services.size() &gt; 0) { anyServices.addAll(services); for (String service : services) { subscribe(url.setPath(service).addParameters(Constants.INTERFACE_KEY, service, Constants.CHECK_KEY, String.valueOf(false)), listener); } } } else { List&lt;URL&gt; urls = new ArrayList&lt;URL&gt;(); // /dubbo/com.alibaba.dubbo.demo.DemoService/configurators for (String path : toCategoriesPath(url)) { ConcurrentMap&lt;NotifyListener, ChildListener&gt; listeners = zkListeners.get(url); if (listeners == null) { zkListeners.putIfAbsent(url, new ConcurrentHashMap&lt;NotifyListener, ChildListener&gt;()); listeners = zkListeners.get(url); } ChildListener zkListener = listeners.get(listener); if (zkListener == null) { listeners.putIfAbsent(listener, new ChildListener() { public void childChanged(String parentPath, List&lt;String&gt; currentChilds) { // ZookeeperRegistry.this.notify(url, listener, toUrlsWithEmpty(url, parentPath, currentChilds)); } }); zkListener = listeners.get(listener); } zkClient.create(path, false); List&lt;String&gt; children = zkClient.addChildListener(path, zkListener); if (children != null) { urls.addAll(toUrlsWithEmpty(url, path, children)); } } notify(url, listener, urls); } } catch (Throwable e) { throw new RpcException(&quot;Failed to subscribe &quot; + url + &quot; to zookeeper &quot; + getUrl() + &quot;, cause: &quot; + e.getMessage(), e); }} doSubscribe方法首先将NotifyListener转换为Zookeeper Listener，创建/dubbo/com.alibaba.dubbo.demo.DemoService/configurators持久节点并订阅。 为什么要订阅/dubbo/com.alibaba.dubbo.demo.DemoService/configurators这个节点。","link":"/posts/38052.html"},{"title":"Dubbo：服务引用","text":"服务引用主流程消费者引用一个服务的主过程，如下图所示： 首先ReferenceConfig类的init方法调用Protocol的refer方法生成Invoker 实例(如上图中的红色部分)，这是服务消费的关键。接下来把Invoker转换为客户端需要的接口(如：HelloWorld)。 源码分析入口分析——ReferenceBean ReferenceBean类实现了FactoryBean接口，通过getBean方法返回的不是FactoryBean本身，而是FactoryBean#getObject方法所返回的对象。 getObject方法返回ReferenceBean类的实例变量ref，其源码如下： 12345678910111213public Object getObject() throws Exception { return get();}public synchronized T get() { if (destroyed){ throw new IllegalStateException(&quot;Already destroyed!&quot;); } if (ref == null) { init(); } return ref;} init方法对ref变量进行赋值，源码如下： 1ref = createProxy(map); 接下来我们一起看看createProxy方法，源码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980private T createProxy(Map&lt;String, String&gt; map) { // 1、根据referenceBean属性构建tmpUrl URL tmpUrl = new URL(&quot;temp&quot;, &quot;localhost&quot;, 0, map); final boolean isJvmRefer; if (isInjvm() == null) { if (url != null &amp;&amp; url.length() &gt; 0) { //指定URL的情况下，不做本地引用 isJvmRefer = false; } else if (InjvmProtocol.getInjvmProtocol().isInjvmRefer(tmpUrl)) { //默认情况下如果本地有服务暴露，则引用本地服务. isJvmRefer = true; } else { isJvmRefer = false; } } else { isJvmRefer = isInjvm().booleanValue(); } // 本地引用 if (isJvmRefer) { URL url = new URL(Constants.LOCAL_PROTOCOL, NetUtils.LOCALHOST, 0, interfaceClass.getName()).addParameters(map); invoker = refprotocol.refer(interfaceClass, url); if (logger.isInfoEnabled()) { logger.info(&quot;Using injvm service &quot; + interfaceClass.getName()); } } else { // 用户指定URL，指定的URL可能是对点对直连地址，也可能是注册中心URL if (url != null &amp;&amp; url.length() &gt; 0) { String[] us = Constants.SEMICOLON_SPLIT_PATTERN.split(url); if (us != null &amp;&amp; us.length &gt; 0) { for (String u : us) { URL url = URL.valueOf(u); if (url.getPath() == null || url.getPath().length() == 0) { url = url.setPath(interfaceName); } if (Constants.REGISTRY_PROTOCOL.equals(url.getProtocol())) { urls.add(url.addParameterAndEncoded(Constants.REFER_KEY, StringUtils.toQueryString(map))); } else { urls.add(ClusterUtils.mergeUrl(url, map)); } } } } else { // 通过注册中心配置拼装URL List&lt;URL&gt; us = loadRegistries(false); if (us != null &amp;&amp; us.size() &gt; 0) { for (URL u : us) { URL monitorUrl = loadMonitor(u); if (monitorUrl != null) { map.put(Constants.MONITOR_KEY, URL.encode(monitorUrl.toFullString())); } urls.add(u.addParameterAndEncoded(Constants.REFER_KEY, StringUtils.toQueryString(map))); } } } // 2、调用procotol获取invoker对象 if (urls.size() == 1) { // 只存在一个注册中心 invoker = refprotocol.refer(interfaceClass, urls.get(0)); } else { List&lt;Invoker&lt;?&gt;&gt; invokers = new ArrayList&lt;Invoker&lt;?&gt;&gt;(); URL registryURL = null; for (URL url : urls) { invokers.add(refprotocol.refer(interfaceClass, url)); if (Constants.REGISTRY_PROTOCOL.equals(url.getProtocol())) { // 用了最后一个registry url registryURL = url; } } if (registryURL != null) { // 有 注册中心协议的URL // 对有注册中心的Cluster 只用 AvailableCluster URL u = registryURL.addParameter(Constants.CLUSTER_KEY, AvailableCluster.NAME); invoker = cluster.join(new StaticDirectory(u, invokers)); } else { // 不是 注册中心的URL invoker = cluster.join(new StaticDirectory(invokers)); } } } // 3、创建invoker的代理对象 return (T) proxyFactory.getProxy(invoker);} createProxy方法的过程如下： 1、根据ReferenceBean的属性拼接URL； 2、调用refprotocol.refer()方法获取invoker对象； 3、创建invoker对象的代理对象。 获取invoker对象的过程refprotocol.refer方法的时序图如下： refprotocol.refer方法主要逻辑由doRefer方法完成的，向注册中心注册消费者的url，然后订阅引用接口的信息，最后通过cluster.join(directory)获取invoke对象（这个过程比较简单，直接构造invoker对象）。doRefer方法源码如下： 123456789101112131415161718private &lt;T&gt; Invoker&lt;T&gt; doRefer(Cluster cluster, Registry registry, Class&lt;T&gt; type, URL url) { RegistryDirectory&lt;T&gt; directory = new RegistryDirectory&lt;T&gt;(type, url); directory.setRegistry(registry); directory.setProtocol(protocol); URL subscribeUrl = new URL(Constants.CONSUMER_PROTOCOL, NetUtils.getLocalHost(), 0, type.getName(), directory.getUrl().getParameters()); if (! Constants.ANY_VALUE.equals(url.getServiceInterface()) &amp;&amp; url.getParameter(Constants.REGISTER_KEY, true)) { // 向注册中心注册消费者的url registry.register(subscribeUrl.addParameters(Constants.CATEGORY_KEY, Constants.CONSUMERS_CATEGORY, Constants.CHECK_KEY, String.valueOf(false))); } // 订阅引用接口的提供者 directory.subscribe(subscribeUrl.addParameter(Constants.CATEGORY_KEY, Constants.PROVIDERS_CATEGORY + &quot;,&quot; + Constants.CONFIGURATORS_CATEGORY + &quot;,&quot; + Constants.ROUTERS_CATEGORY)); return cluster.join(directory);} 注册消费者url和发布服务的注册过程类似，都是创建zookeeper临时节点。我们重点关注其订阅过程： 12// this = doRefer()中directoryregistry.subscribe(url, this); 通过debug发现会订阅下面三个zookeeper目录： /dubbo/com.alibaba.dubbo.demo.DemoService/providers/dubbo/com.alibaba.dubbo.demo.DemoService/configurators/dubbo/com.alibaba.dubbo.demo.DemoService/routers 当这三个目录有子节点变化时，都会出发点RegisterDirectory的notity方法。源码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041public synchronized void notify(List&lt;URL&gt; urls) { List&lt;URL&gt; invokerUrls = new ArrayList&lt;URL&gt;(); List&lt;URL&gt; routerUrls = new ArrayList&lt;URL&gt;(); List&lt;URL&gt; configuratorUrls = new ArrayList&lt;URL&gt;(); for (URL url : urls) { String protocol = url.getProtocol(); String category = url.getParameter(Constants.CATEGORY_KEY, Constants.DEFAULT_CATEGORY); if (Constants.ROUTERS_CATEGORY.equals(category) || Constants.ROUTE_PROTOCOL.equals(protocol)) { routerUrls.add(url); } else if (Constants.CONFIGURATORS_CATEGORY.equals(category) || Constants.OVERRIDE_PROTOCOL.equals(protocol)) { configuratorUrls.add(url); } else if (Constants.PROVIDERS_CATEGORY.equals(category)) { invokerUrls.add(url); } else { logger.warn(&quot;Unsupported category &quot; + category + &quot; in notified url: &quot; + url + &quot; from registry &quot; + getUrl().getAddress() + &quot; to consumer &quot; + NetUtils.getLocalHost()); } } // configurators if (configuratorUrls != null &amp;&amp; configuratorUrls.size() &gt;0 ){ this.configurators = toConfigurators(configuratorUrls); } // routers if (routerUrls != null &amp;&amp; routerUrls.size() &gt;0 ){ List&lt;Router&gt; routers = toRouters(routerUrls); if(routers != null){ // null - do nothing setRouters(routers); } } List&lt;Configurator&gt; localConfigurators = this.configurators; // local reference // 合并override参数 this.overrideDirectoryUrl = directoryUrl; if (localConfigurators != null &amp;&amp; localConfigurators.size() &gt; 0) { for (Configurator configurator : localConfigurators) { this.overrideDirectoryUrl = configurator.configure(overrideDirectoryUrl); } } // providers refreshInvoker(invokerUrls);} 创建invoker的代理对象 \b创建invoker的代理对象\b由JavassistProxyFactory.getProxy方法完成的，源码如下： 1234public &lt;T&gt; T getProxy(Invoker&lt;T&gt; invoker, Class&lt;?&gt;[] interfaces) { return (T) Proxy.getProxy(interfaces) .newInstance(new InvokerInvocationHandler(invoker));} 生成的代理对象\b其源码如下： 1234567891011121314151617181920212223public class proxy0 implements DC, EchoService, DemoService { public static Method[] methods; private InvocationHandler handler; public String sayHello(String var1) { Object[] var2 = new Object[]{var1}; Object var3 = this.handler.invoke(this, methods[0], var2); return (String)var3; } public Object $echo(Object var1) { Object[] var2 = new Object[]{var1}; Object var3 = this.handler.invoke(this, methods[1], var2); return (Object)var3; } public proxy0() { } public proxy0(InvocationHandler var1) { this.handler = var1; }}","link":"/posts/48988.html"},{"title":"Git版本回退：git reset&amp;git revert","text":"原文地址：https://blog.csdn.net/yxlshk/article/details/79944535 作者：游笑天涯 多人合作程序开发的过程中，我们有时会出现错误提交的情况，此时我们希望能撤销提交操作，让程序回到提交前的样子，本文总结了两种解决方法：回退（reset）、反做（revert）。 使用git的每次提交，Git都会自动把它们串成一条时间线，这条时间线就是一个分支。如果没有新建分支，那么只有一条时间线，即只有一个分支，在Git里，这个分支叫主分支，即master分支。有一个HEAD指针指向当前分支（只有一个分支的情况下会指向master，而master是指向最新提交）。每个版本都会有自己的版本信息，如特有的版本号、版本名等。如下图，假设只有一个分支： git reset 原理 git reset的作用是修改HEAD的位置，即将HEAD指向的位置改变为之前存在的某个版本。reset后，目标版本之后的版本就不见了。 适用场景 如果想恢复到之前某个提交的版本，且那个版本之后提交的版本我们都不要了，就可以用这种方法。 使用git reset --hard 目标版本号命令将版本回退。 git revert 原理 git revert是用于“反做”某一个版本，以达到撤销该版本的修改的目的。 比如，我们commit了三个版本（版本一、版本二、 版本三），突然发现版本二不行（如：有bug），想要撤销版本二，但又不想影响撤销版本三的提交，就可以用 git revert 命令来反做版本二，生成新的版本四，这个版本四里会保留版本三的东西，但撤销了版本二的东西。 适用场景 如果我们想撤销之前的某一版本，但是又想保留该目标版本后面的版本，记录下这整个版本变动流程，就可以用这种方法。 使用git revert -n 版本号反做，并使用git commit -m 版本名提交：","link":"/posts/37221.html"},{"title":"JVM：如何处理对象分配、布局和访问？","text":"对象分配JVM遇到一条new指令时，首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有，那必须先执行相应的类加载过程。 在类加载检查通过后，虚拟机将为新生对象分配内存。对象所需内存的大小在类加载完成后便可完全确定，为对象分配空间的任务等同于把一块确定大小的内存从Java堆中划分出来。 内存空间划分的方法 指针碰撞 假设Java堆中内存是绝对规整的，所有用过的内存都放在一边，空闲的内存放在另一边，中间放着一个指针作为分界点的指示器。分配内存就仅仅是把那个指针向空闲空间那边挪动一段与对象大小相等的距离。 空闲列表 如果Java堆中内存并不是规整的，已使用的内存和空闲的内存相互交错，虚拟机就必须维护一个列表，记录上那些内存块是可用的，在分配的时候从列表中找到一个足够大的空间划分给对象实例，并更新列表上的记录。 可见，选择哪种内存分配方式由JVM堆内存是否规整决定，而JVM堆内存是否规整又由所采用的垃圾收集器是否有整理过程所决定，因此，使用Serial、ParNew等带有Compact过程的垃圾收集器时，采用“指针碰撞” 方式进行对象实例内存分配，而使用像CMS这种基于MARK_SWEEP垃圾回收算法的垃圾收集器时，JVM则采用“空闲列表”为对象实例进行内存分配。 分配过程中如何解决线程安全？在并发情况下可能出现正在给对象A分配内存，指针还没来得及修改，对象B又同时使用了原来的指针来分配内存的情况。虚拟机通过以下两种方式解决线程安全问题： 1、对分配内存空间的动作进行同步处理（实际上虚拟机采用CAS配上失败重试的方式保证更新操作的原子性）； 2、把内存分配的动作按照线程划分在不同的空间之中进行，即每个线程在Java堆中预先分配一小块内存，称为本地线程分配缓冲（Thread Local Allocation Buffer，TLAB）。那个线程要分配内存，就在哪个线程的TLAB上分配，只有TLAB用完并分配新的TLAB时，才需要同步锁定。虚拟机是否使用TLAB，可以通过-XX:+/-UseTLAB参数来设定。 对象分配流程 TLAB TLAB启用的情况下（默认开启），JVM会为每一个线程分配一块TLAB区域，TLAB占用的是Eden区的空间。 大对象 大对象到底多大：-XX:PreTenureSizeThreshold=n（仅适用于 DefNew / ParNew新生代垃圾回收器 ） G1回收器的大对象判断，则依据Region的大小（-XX:G1HeapRegionSize）来判断，如果对象大于Region50%以上，就判断为大对象Humongous Object。 对象布局在HotSpot虚拟机中，对象在内存中存储的布局可以分为3块区域：对象头（Header）、实例数据（Instance Data） 和对齐填充（Padding）。 对象头HotSpot虚拟机的对象头包括两部分信息，用于存储对象自身的运行时数据和类型指针。 存储对象自身的运行时数据 如哈希码、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等，这部分数据的长度在32位和64位的虚拟机（未开启压缩指针）中分别为32bit和64bit，官方称为“Mark Word”。 对象需要存储的运行时数据很多，其实已经超出了32位和64位Bitmap结构所能记录的限度，但是对象头信息是与对象自身定义的数据无关的额外存储成本，考虑到虚拟机的空间效率，Mark Word被设计成一个非固定的数据结构以便在极小的空间内存存储尽量多的信息，它会根据对象的状态复用自己的存储空间。 类型指针 即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。并不是所有的虚拟机实现都必须在对象数据上保留类型指针，换句话说，查找对象的元数据信息并不一定要经过对象本身。 另外，如果对象是一个Java数组，那在对象头中还必须有一块用于记录数组长度的数据，因为无法从数组的元数据中确定数组的大小。 实例数据实例数据是对象真正存储的有效信息，也是在程序代码中所定义的各种类型的字段内容。无论是从父类继承下来的，还是在子类中定义的，都需要记录起来 。这部分的存储顺序会受到虚拟机分配策略参数和字段在Java源码中定义顺序的影响。 对其填充对其填充并不是必然存在的，也没有特别的含义，它仅仅起着占位符的作用。 由于HotSpot VM的自动内存管理系统要求对象起始地址必须是8字节的整数倍，换句话说，就是对象的大小必须是8字节的整数倍。由于对象头部分正好是8字节的整数倍，因此，当对象实例数据部分没有对齐时，就需要通过对齐填充来补全。 对象访问定位建立对象是为了使用对象，Java程序需要通过栈上的reference数据来操作堆上的具体对象。由于reference类型在Java虚拟机规范中规定了一个指向对象的引用，并没有定义这个引用应该通过何种方式去定位、访问堆中对象的具体位置，所以对象访问方式也是取决于虚拟机实现而定的。 句柄访问 Java堆中会划分出一块内存来作为句柄池，reference中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息。 直接指针访问 使用句柄访问的最大好处就是reference中存储的是稳定的句柄地址，在对象被移动（垃圾收集时移动对象是非常普遍的）时只会改变句柄的实例数据指针，而reference本身不需要修改。 使用直接指针访问的最大好处就是速度更快，节省了一次指针定位的时间开销，由于对象的访问在Java中非常频繁，因此也是一项非常客观的执行成本。Sun HotSpot虚拟机采用直接指针进行对象访问的。","link":"/posts/48830.html"},{"title":"JVM：类加载机制","text":"虚拟机把描述类的数据从Class文件加载到内存，并对数据进行校验、转换解析和初始化，最终形成可以被虚拟机直接使用的Java类型，这就是虚拟机的类加载机制。 在Java语言里，类型的加载、连接和初始化过程都是在程序运行期间完成的，这种策略虽然会令类加载时稍微增加一些性能开销，但是会为Java应用程序提供高度的灵活性，Java的动态扩展特性就是依赖运行期动态加载和动态连接这个特点实现的。 类加载的时机类从被加载到虚拟机内存中开始，到卸载出内存为止，它的整个生命周期包括：加载（Loading）、验证（Verification）、准备（Preparation）、解析（Resolution）、初始化（Initialization）、使用（Using）和卸载（Unloading）7个阶段。其中验证、准备、解析3个部分统称为连接（Linking）。这7个阶段发生的顺序如图所示： 加载、验证、准备、初始化和卸载这5个阶段的顺序是确定的，类的加载过程必须按照这种顺序按部就班地开始，而解析阶段则不一定：它在某些情况下可以在初始化阶段之后再开始，这是为了支持Java语言的运行时绑定。 按部就班地“开始”，强调这点是因为这些阶段通常都是互相交叉地混合式进行的，通常会在这一阶段执行的过程中调用、激活另外一个阶段。 什么情况下需要开始类加载过程的第一个阶段：加载？Java虚拟机规范中并没有进行强制约束，这点可以交给虚拟机的具体实现来把握。但对于初始化阶段，虚拟机规范有严格的规定，有且只有5种情况必须立即对类进行“初始化”： 1、 遇到new、getstatic、setstatic或invokestatic这4条字节码指令时，如果类没有进行初始化，则需要先触发其初始化。生成这4条指令的最常见的Java代码场景是：使用new关键字实例化对象的时候、读取或设置一个类的静态子段（被final修饰、已在编译期把结果放入常量池的静态子段除外）的时候，以及调用一个类的静态方法的时机。 2、 使用java.lang.reflect包的方法对类进行反射调用的时候，如果类没有进行过初始化，则需要先触发其初始化。 3、当初始化一个类的时候，如果发现其父类还没有进行初始化，则需要先触发其父类的初始化。 4、当虚拟机启动时，用户需要指定一个要执行的主类（包含main()方法的那个类），虚拟机会先初始化这个主类。 5、当使用JDK 1.7的动态语言支持时，如果一个java.lang.invoke.MethodHandle实例最后的解析结果REF_getStatic、REF_putStatic、REF_invokeStatic的方法句柄，并且这个方法句柄所对应的类没有进行初始化，则需要先触发其初始化。 这5种场景中的行为称为对一个类进行主动引用，除此之外，所有引用类的方法都不会出发初始化，称为被动引用。 接口也有初始化过程，但是一个接口在初始化时，并不要要求其父接口全部都完成了初始化，只有在真正使用到父接口的时候（如引用接口中定义的常量）才会初始化。 类加载的过程Java虚拟机类加载的全过程是加载、验证、准备、解析和初始化这5个阶段所执行的具体动作。 加载在加载阶段，虚拟机需要完成以下3件事情： 1）通过一个类的全限定名来获取定义此类的二进制字节流； 2）将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构； 3）在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口； 数组类本身不通过类加载器创建，它是由Java虚拟机直接创建的。但数组类与类加载器仍然有很密切的关系，因为数组类的元素类型最终是要靠类加载器去创建。 验证验证是连接阶段的第一步，这一阶段的目的是为了确保Class文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。 从整体上来看，验证阶段大致上会完成下面4个阶段的校验动作：文件格式验证、元数据验证、字节码验证、符号引用验证。 准备准备阶段正式为类变量分配内存并设置类变量初始值的阶段，这些变量所使用的内存都将在方法区中进行分配。首先，这个时候进行内存分配的仅包括类变量（被static修饰的变量），而不包括实例变量，实例变量将会在对象实例化时随着对象一起分配在Java堆中；其次，这里所说的初始值“通常情况”下是数据类型的零值。 如果类子段的字段属性表中存在ConstantValue属性，那在准备阶段变量value就会被初始化为ConstantValue属性所指定的值。 解析解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。 符号引用 符号引用以一组符号来描述所引用的目标，符号可以是任何形式的字面量，只要使用时无歧义地定位到目标即可。符号引用与虚拟机实现的内存布局无关，引用的目标并不一定已经加载到内存中。各种虚拟机实现的内存布局可以各不相同，但是它们能接受的符号引用必须都是一致的，因为符号引用的字面量形式明确定义在Java虚拟机规范的Class文件格式中。 直接引用 直接引用可以是直接指向目标的指针、相对偏移量或一个能间接定位到目标的句柄。直接引用是和虚拟机实现的内存布局相关的，同一个符号引用在不同虚拟机实例上翻译出来的直接引用一般不会相同。如果有了直接引用，那引用的目标必定已经在内存中存在。 初始化在初始化阶段，才真正开始执行类中定义的Java程序代码，根据程序员通过程序制定的主观计划去初始化类变量和其他资源。或者可以从另外一个角度来表达：初始化阶段是执行类构造器&lt;clinit&gt;()方法的过程。 &lt;clinit&gt;()方法是由编译器自动收集类中的所有类变量的赋值动作和静态语句块中的语句合并产生的，编辑器收集的顺序是由语句在源文件中出现的顺序所决定的，静态语句块中只能访问到定义在静态语句块之前的变量，定义在它之后的变量可以赋值，但不能访问。 &lt;clinit&gt;()方法执行过程中一些可能会影响程序运行行为的特点和细节： 1、&lt;clinit&gt;()方法与类的构造函数不同，它不需要显式地调用父类构造函数，虚拟机会保证在子类的&lt;clinit&gt;()方法执行之前，父类的&lt;clinit&gt;()方法已经执行完毕。因此在虚拟机中第一个被执行的&lt;clinit&gt;()方法的类肯定是java.lang.Object。 2、&lt;clinit&gt;()方法对类和接口来说并不是必需的，如果一个类中没有静态语句块，也没有对变量的赋值操作，那么编译器可以不为这个类生成&lt;clinit&gt;()方法。 3、接口中不能使用静态语句块，但仍然有变量初始化的赋值操作，因此接口与类一样都会生成&lt;clinit&gt;()方法。但接口与类不同的是，执行接口的&lt;clinit&gt;()方法不需要先执行父接口的&lt;clinit&gt;()方法，只有当父接口中定义的使用时，父接口才会初始化。另外，接口的实现类在初始化也一样不会执行接口的&lt;clinit&gt;()方法。 4、虚拟机会保证一个类的&lt;clinit&gt;()方法在多线程环境中被正确地加锁、同步，如果多个线程同时去初始化一个类，那么只会有一个线程去执行这个类的&lt;clinit&gt;()方法，其他线程都需要阻塞等待，直到活动线程执行&lt;clinit&gt;()方法完毕。 掌握ClassLoaderClassLoader是Java的核心组件，所有的Class都是由ClassLoader进行加载的，ClassLoader负责通过各种方法将Class信息的二进制数据流读入系统，然后交给Java虚拟机进行连接、初始化等操作。 ClassLoader是一个抽象类，提供了一些重要的接口，用于自定义Class的加载流程和加载方式。ClassLoader的主要方法如下： public Class&lt;?&gt; loadClass(String name) throws ClassNotFoundException 给定一个类名，加载一个类，返回代表这个类的Class实例。如果找不到类，则抛出ClassNotFoundException异常。 protected final Class&lt;?&gt; defineClass(String name, byte[] b, int off, int len) 根据给定的字节码流b定义一个类，off和len参数表示实际Class信息在byte数组中的位置和长度，其中byte数组b是ClassLoader从外部获取的。 protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException 查找一个类，也是重载ClassLoader时，重要的系统扩展点。这个方法会在loadClass()时被调用，用于自定义查找类的逻辑。如果不需要修改类加载默认机制，只是想改变类加载的形式，就可以重载该方法。 ClassLoader的分类在标准的Java程序中，Java虚拟机会创建3类ClassLoader为整个应用程序服务。分别是Bootstrap ClassLoader（启动类加载器）、Extension ClassLoader（扩展类加载器）和AppClassLoader（应用类加载器、也称为系统类加载器）。 启动类加载器（Bootstrap ClassLoader） 负责将存在${JAVA_HOME}\\lib目录中的，或者是被-Xbootclasspath所指定的路径中，并且是虚拟机识别的类库加载到虚拟机内存中。 扩展类加载器（Extension Classloader） 负责加载${JAVA_HOME}\\lib\\ext目录中的，或者是被java.ext.dirs系统变量所指定的所有类库。 应用程序加载器（Application Classloader） 负责加载用户类路径（classpath）上所指定的类库，如果应用程序没有子定义过自己的类加载器，一般情况下这个是程序中默认的类加载器。 当系统需要使用一个类时，在判断类是否已经被加载时，会先从当前底层类加载器进行判断。当系统需要加载一个类时，会从顶层类开始加载，依次向下尝试，直到成功。 ClassLoader的双亲委托模式系统中的ClassLoader在协同工作时，默认会使用双亲委托模式。即在类加载的时候，系统会判断当前类是否已经被加载，如果已经被加载，就会直接返回可用的类，否则就会尝试加载，在尝试加载时，会先请求双亲处理，如果双亲请求失败，则会自己加载。 双亲委派是由java.lang.ClassLoader#loadClass(java.lang.String, boolean)实现的，源码如下： 1234567891011121314151617181920212223242526272829protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException{ synchronized (getClassLoadingLock(name)) { // 首先，检查请求的类是否已经被加载过 Class&lt;?&gt; c = findLoadedClass(name); if (c == null) { long t0 = System.nanoTime(); try { // 如果存在父加载器，则委托其父加载器加载请求的类;否则由启动类加载器加载。 if (parent != null) { c = parent.loadClass(name, false); } else { c = findBootstrapClassOrNull(name); } } catch (ClassNotFoundException e) { } if (c == null) { long t1 = System.nanoTime(); // 在父类无法加载的时候，再调用本身的findClass方法来进行加载 c = findClass(name); } } if (resolve) { resolveClass(c); } return c; }} 双亲为null有两种情况：第一，其双亲就是启动类加载器；第二，当前加载器就是启动类加载器。 双亲委托模式的弊端判断类是否加载时，应用类加载器会顺着双亲路径往上判断，直到启动类加载器。但是启动类加载器不会往下询问，这个委托路线是单向的。 虽然从结构上来说比较清晰，使各个ClassLoader的职责非常明确，但是同时会带来一个问题：顶层的ClassLoader无法访问底层的ClassLoader所加载的类。比如JDBC，Java提供java.sql.Driver核心接口，但其实现需要在应用中完成。 破坏双亲委派模型在Java的世界中大部分的类加载器都遵循这个模型，但也有例外，到目前为止，双亲委派模型主要出现过3较大规模的“被破坏”情况： 1、双亲委派模型在JDK 1.2之后才被引入，而类加载器和抽象类java.lang.ClassLoader则在JDK 1.0时代就已经存在，面对已经存在的用户自定义类加载的实现代码，Java设计者引入双亲模型时不得不做一些妥协。在此之前，用户继承java.lang.ClassLoader的唯一目的就是为了重写loadClass()方法，为了向前兼容，JDK 1.2之后的java.lang.ClassLoader添加了一个新的protected方法findClass()，推荐把自己的类加载逻辑写到findClass()方法中。 2、双亲委派模型很好地解决了各个类加载器的基础类的统一问题，之所以称为“基础类”，是因为它们总是作为被用户代码调用的API。如果基础类又要调用用户的代码呢，典型的例子就是JNDI服务。为了解决这个问题，Java设计团队引入一个不太优雅的设计：线程上下文类加载器。线程上下文加载器可以通过java.lang.Thread类的setContextClassLoader()方法进行设置，如果创建线程时还未设置，它将会从父线程中继承一个，如果在应用程序的全局范围内都没有设置过的话，那这个类加载器默认就是应用程序类加载器。 3、在OSGi环境下，类加载器不再是双亲委派模型中的树状结构，而是进一步发展为更加复杂的网状结构。","link":"/posts/29291.html"},{"title":"JVM：通过逃逸分析就能让对象在栈上分配？","text":"原文地址：https://mp.weixin.qq.com/s/Pub_K7PSCNE82F-96y2v6g?scene=25#wechat_redirect 经常会有面试官会问一个问题：Java 中的对象都是在”堆”中创建吗？ 然后跟求职者大谈特谈逃逸分析，说通过逃逸分析，JVM 会将实例对象分配在栈上。其实这种说法，是并不是很严谨，最起码目前在 HotSpot 中，并没有在栈中存储对象的实现代码！ 什么是逃逸分析？首先逃逸分析是一种算法，这套算法在 Java 即时编译器（JIT），编译 Java 源代码时使用。通过逃逸分析算法，可以分析出某一个方法中的某个对象，是否会被其它方法或者线程访问到。 如果分析结果显示，某对象并不会被其它线程访问，则有可能在编译期间，对其做一些深层次的优化，具体有哪些优化稍后讲解。 执行 java 程序时，可以通过如下参数开启或者关闭”逃逸分析”。 开启逃逸分析：-XX:+DoEscapeAnalysis 关闭逃逸分析：-XX:-DoEscapeAnalysis 逃逸分析原则在 HotSpot 源码中的 escape.hpp 中定义了对象进行逃逸分析后的几种状态。（路径：src/share/vm/opto/escape.hpp） 1、全局逃逸（GlobalEscape）即一个对象的作用范围，逃出了当前方法或者当前线程，有以下几种场景： 对象是一个静态变量； 对象作为当前方法的返回值； 如果复写了类的 finalize 方法，则此类的实例对象都是全局逃逸状态（因此为了提高性能，除非万不得已，不要轻易复写 finalize 方法）； 2、参数逃逸（ArgEscape）即一个对象，被作为方法参数传递，或者被参数引用，但在调用过程中，不会再被其它方法或者线程访问。 3、没有逃逸（NoEscape）即方法中的对象，没有发生逃逸，这种对象会被 Java 即时编译器进一步的优化。 逃逸分析优化经过「逃逸分析」之后，如果一个对象的逃逸状态是 GlobalEscape 或者 ArgEscape，则此对象必须被分配在「堆」内存中，但是对于 NoEscape 状态的对象，则不一定，具体会有以下几种优化情况。 1、锁消除比如以下代码。 在lockElimination()方法中，对象 a 永远不会被其它方法或者线程访问到，因此 a是非逃逸对象，这就导致synchronized(a) 没有任何意义，因为在任何线程中，a 都是不同的锁对象。所以 JVM 会对上述代码进行优化，删除同步相关代码，以下： 对于锁消除，还有一个比较经典的使用场景：StringBuffer。 StringBuffer 是一个使用同步方法的线程安全的类，可以用来高效地拼接不可变的字符串对象。StringBuffer 内部对所有 append() 方法都进行了同步操作，如下所示： 但是在平时开发中，有很多场景其实是不需要这层线程安全保障的，因此在 Java 5 中又引入了一个非同步的 StringBuilder 类来作为它的备选，StringBuilder 中的 append()方法并没有使用 synchronized 标识，如下所示： 调用 StringBuffer 的 append() 方法的线程，必须得获取到这个对象的内部锁（也叫监视器锁）才能进入到方法内部，在退出方法前也必须要释放掉这个锁。而 StringBuilder 就不需要进行这个操作，因此它的执行性能比 StringBuffer 的要高–至少乍看上去是这样的。 不过在 HotSpot 虚拟机引入了「逃逸分析」之后，在调用 StringBuffer 对象的同步方法时，就能够自动地把锁消除掉了。从而提高 StringBuffer 的性能，比如以下代码： 在getString()方法中的 StringBuffer 是方法内部的局部变量，并且并没有被当做方法返回值返回给调用者，因此 StringBuffer 是一个”非逃逸（NoEscape）”对象。 执行上述代码，结果如下： java TestLockEliminate 一共耗费：720 ms 我们可以通过 -XX:-EliminateLocks 参数关闭锁消除优化，重新执行上述代码，结果如下： java -XX:-EliminateLocks TestLockEliminate 一共耗费：1043 ms 可以看出，关闭锁消除后性能会降低，耗时更多。 2、对象分配消除除了锁消除，JVM 还会对无逃逸（NoEscape）对象进行对象分配消除优化。对象分配消除是指将本该在「堆」中分配的对象，转化为由「栈」中分配。乍听一下，很不可思议，但是我们可以通过一个案例来验证一下。 比如以下代码，在一个 1 千万次的循环中，分别创建 EscapeTest 对象 t1 和 t2。 使用如下命令执行上述代码 java -Xms2g -Xmx2g -XX:+PrintGCDetails -XX:-DoEscapeAnalysis EscapeTest 通过参数 -XX:-DoEscapeAnalysis 关闭「逃逸分析」，然后代码会在 System.in.read() 处停住，此时使用 jps 和 jmap 命令查看内存中 EscapeTest 对象的详细情况，如下： 可以看出，此时堆内存中有 2 千万个 EscapeTest 的实例对象(t1 和 t2 各 1 千万个)，GC 日志如下： 没有发生 GC 回收事件，但是 eden 区已经占用 96%，所有的 EscapeTest 对象都在”堆”中分配。 如果我们将执行命令修改为如下： java -Xms2g -Xmx2g -XX:+PrintGCDetails -XX:+DoEscapeAnalysis EscapeTest 开启「逃逸分析」，并重新查看 EscapeTest 对象情况如下： 可以看出，此时堆内存中只有 30 万个左右，并且 GC 日志如下： 没有发生 GC 回收时间，EscapeTest 只占用 eden 区的 8%，说明并没有在堆中创建 EscapeTest 对象，取而代之的是分配在「栈」中。 注意： 有的读者可能会有疑问：开启了「逃逸分析」，NoEscape 状态的对象，不是会在「栈」中分配吗？ 为什么这里还是会有 30 多万个对象在「堆」中分配？这是因为我使用的 JDK 是混合模式，通过 java -version 查看 java 的版本，结果如下： mixed mode 代表混合模式。 在 Hotspot 中采用的是解释器和编译器并行架构，所谓的混合模式，就是解释器和编译器搭配使用，当程序启动初期，采用解释器执行（同时会记录相关的数据，比如函数的调用次数，循环语句执行次数），节省编译的时间。在使用解释器执行期间，记录的函数运行的数据，通过这些数据发现某些代码是热点代码，采用编译器对热点代码进行编译，以及优化（逃逸分析就是其中一种优化技术）。 3、标量替换上文中，我提到当「逃逸分析」后，对象状态为 NoEscape 时会在「栈」中进行分配。但是实际上，这种说法并不是完全准确的，「栈」中直接分配对象难度太大，需要修改 JVM 中大量堆优先分配的代码，因此在 HotSpot 中并没有真正的实现”栈”中分配对象的功能，取而代之的是一个叫做「标量替换」的折中办法。 首先要明白标量和聚合量，基础类型和对象的引用可以理解为标量，它们不能被进一步分解。而能被进一步分解的量，就是聚合量。 对象就是聚合量，它可以被进一步分解成标量，将其成员变量分解为分散的变量，这就叫做「标量替换」。这样如果一个对象没有发生逃逸，那压根就不需要在「堆」中创建它，只会在栈或者寄存器上创建一些能够映射这个对象标量即可，节省了内存空间，也提升了应用程序性能。 比如以下两个计算和的方法： 乍看一下，sumPrimitive()方法比 sumMutableWrapper() 方法简单的多，那执行效率也肯定快许多吧？ 但是结果却是两个方法的执行效率相差无几。这是为什么呢？在 sumMutableWrapper()方法中，MutableWrapper 是不可逃逸对象，也就是说没有必要在「堆」中创建真正的 MutableWrapper 对象，Java 即时编译器会使用标量替换对其进行优化，优化结果为下： 仔细查看，上述优化够的代码中的 value 也是一个中间变量，通过内联之后，会被优化为如下： 1total += i; 也就是说，java 源代码中的一大坨在真正执行时，只有简单的一行操作。因此sumPrimitive和 sumMutableWrapper() 两个方法的执行效率基本一致。","link":"/posts/19168.html"},{"title":"Java 8：CompletableFuture","text":"","link":"/posts/5954.html"},{"title":"Java Agent","text":"什么是 Java AgentJava Agent 是从 JDK1.5 开始引入的，算是一个比较老的技术了。作为 Java 的开发工程师，我们常用的命令之一就是 java 命令，而 Java Agent 本身就是 java 命令的一个参数（即 -javaagent）。 -javaagent参数之后需要指定一个 jar 包，这个 jar 包需要同时满足下面两个条件： 在 META-INF 目录下的 MANIFEST.MF 文件中必须指定 premain-class 配置项。 premain-class 配置项指定的类必须提供了premain()方法。 premain() 方法有两个重载，如下所示，如果两个重载同时存在，【1】将会被忽略，只执行【2】： 12public static void premain(String agentArgs) [1]public static void premain(String agentArgs, Instrumentation inst); [2] agentArgs 参数：-javaagent 命令携带的参数。 inst 参数：java.lang.instrumen.Instrumentation 是 Instrumention 包中定义的一个接口，它提供了操作类定义的相关方法。 使用 Java Agent使用 Java Agent 的步骤大致如下： 定义一个 MANIFEST.MF 文件，在其中添加 premain-class 配置项。 创建 premain-class 配置项指定的类，并在其中实现 premain() 方法，方法签名如下： 123public static void premain(String agentArgs, Instrumentation inst){ ... } 将 MANIFEST.MF 文件和 premain-class 指定的类一起打包成一个 jar 包。 使用 -javaagent 指定该 jar 包的路径即可执行其中的 premain() 方法。 Java Agent示例首先，我们创建一个最基本的 Maven 项目，然后创建AgentDemo.java 这一个类，项目的整体结构如图所示。 AgentDemo代码如下所示： 12345678910public class AgentDemo { public static void premain(String agentArgs, Instrumentation instrumentation) { System.out.println(&quot;this is a java agent with two args&quot;); System.out.println(&quot;参数:&quot; + agentArgs + &quot;\\n&quot;); } public static void premain(String agentArgs) { System.out.println(&quot;this is a java agent only one args&quot;); System.out.println(&quot;参数:&quot; + agentArgs + &quot;\\n&quot;); }} 接下来还需要创建 MANIFEST.MF 文件并打包，直接使用 maven-assembly-plugin 打包插件来完成这两项功能。在 pom.xml 中引入maven-assembly-plugin 插件并添加相应的配置，如下所示： 1234567891011121314151617181920212223242526272829&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;2.4&lt;/version&gt; &lt;configuration&gt; &lt;appendAssemblyId&gt;false&lt;/appendAssemblyId&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;archive&gt; &lt;manifest&gt; &lt;addDefaultImplementationEntries&gt;true&lt;/addDefaultImplementationEntries&gt; &lt;addDefaultSpecificationEntries&gt;true&lt;/addDefaultSpecificationEntries&gt; &lt;/manifest&gt; &lt;manifestEntries&gt; &lt;Premain-Class&gt;com.codersm.agent.demo.AgentDemo&lt;/Premain-Class&gt; &lt;Can-Retransform-Classes&gt;true&lt;/Can-Retransform-Classes&gt; &lt;/manifestEntries&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt; 最后执行 maven 命令进行打包，如下： 1mvn package -Dcheckstyle.skip -DskipTests 测试 Agent再创建一个普通的 Maven 项目：agent-demo-test，项目结构与 agent-demo类似，如图所示： 在 Main 这个类中定义了该项目的入口 main() 方法，如下所示： 123456public class Main { public static void main(String[] args) throws InterruptedException { Thread.sleep(1000); System.out.println(&quot;TestMain Main!&quot;); }} 在启动agent-demo-test项目之前，需要在 VM options 中使用 -javaagent 命令指定前面创建的agent-demo.jar，如图所示： 启动agent-demo-test之后得到了如下输出： 修改类实现Java Agent 可以实现的功能远不止添加一行日志这么简单，这里需要关注一下 premain() 方法中的第二个参数：Instrumentation。 Instrumentation 位于 java.lang.instrument 包中，通过这个工具包，我们可以编写一个强大的 Java Agent 程序，用来动态替换或是修改某些类的定义。 Instrumentation类提供了如下API方法： addTransformer()/removeTransformer() 方法：注册/注销一个 ClassFileTransformer 类的实例，该 Transformer 会在类加载的时候被调用，可用于修改类定义。 redefineClasses() 方法：该方法针对的是已经加载的类，它会对传入的类进行重新定义。 getAllLoadedClasses()方法：返回当前 JVM 已加载的所有类。 getInitiatedClasses() 方法：返回当前 JVM 已经初始化的类。 getObjectSize()方法：获取参数指定的对象的大小。 下面我们通过一个示例演示 Instrumentation 如何与 Java Agent 配合修改类定义。首先我们提供一个普通的 Java 类：TestClass，其中提供一个 getNumber() 方法： 123public class TestClass { public int getNumber() { return 1; }} 编译生成 TestClass.class 文件之后，我们将 getNumber() 方法返回值修改为 2，然后再次编译，并将此次得到的 class 文件重命名为 TestClass.class.2 文件，如图所示，我们得到两个 TestClass.class 文件： 之后将 TestClass.getNumber() 方法返回值改回 1 ，重新编译。 然后编写一个 main() 方法，新建一个 TestClass 对象并输出其 getNumber() 方法的返回值： 12345public class Main { public static void main(String[] args) { System.out.println(new TestClass().getNumber()); }} 接下来编写 premain() 方法，并注册一个 Transformer 对象： 12345678910public class TestAgent { public static void premain(String agentArgs, Instrumentation inst) throws Exception { // 注册一个 Transformer，该 Transformer在类加载时被调用 inst.addTransformer(new Transformer(), true); // 让类重新加载，从而使得注册的类修改器能够重新修改类的字节码。 inst.retransformClasses(TestClass.class); System.out.println(&quot;premain done&quot;); }} Transformer 实现了 ClassFileTransformer，其中的 transform() 方法实现可以修改加载到的类的定义，具体实现如下： 1234567891011class Transformer implements ClassFileTransformer { public byte[] transform(ClassLoader l, String className, Class&lt;?&gt; c, ProtectionDomain pd, byte[] b) { if (!c.getSimpleName().equals(&quot;TestClass&quot;)) { return null; // 只修改TestClass的定义 } // 读取 TestClass.class.2这个 class文件，作为 TestClass类的新定义 return getBytesFromFile(&quot;TestClass.class.2&quot;); }} 最后，打包启动应用，得到的输出如下： 12premain done2 Attach API 基础在 Java 5 中，Java 开发者只能通过 Java Agent 中的 premain() 方法在 main() 方法执行之前进行一些操作，这种方式在一定程度上限制了灵活性。Java 6 针对这种状况做出了改进，提供了一个 agentmain() 方法，Java 开发者可以在 main() 方法执行以后执行 agentmain() 方法实现一些特殊功能。 agentmain()方法同样有两个重载，它们的参数与premain() 方法相同，而且前者优先级也是高于后者的： 123public static void agentmain (String agentArgs, Instrumentation inst);[1]public static void agentmain (String agentArgs); [2] agentmain() 方法主要用在 JVM Attach 工具中，Attach API 是 Java 的扩展 API，可以向目标 JVM “附着”（Attach）一个代理工具程序，而这个代理工具程序的入口就是 agentmain() 方法。","link":"/posts/60776.html"},{"title":"Java并发编程：CAS &amp; Unsafe","text":"原文地址：【死磕Java并发】—–深入分析CAS CAS（Compare And Swap）CAS（Compare And Swap），即比较并交换。Doug lea大神在同步组件中大量使用CAS技术鬼斧神工地实现了Java多线程的并发操作。整个AQS同步组件、Atomic原子类操作等都是以CAS实现的，甚至ConcurrentHashMap在1.8的版本中也调整为了CAS+Synchronized。可以说CAS是整个JUC的基石。 CAS分析在CAS中有三个参数：内存值V、旧的预期值A和要更新的值B，当且仅当内存值V的值等于旧的预期值A时才会将内存值V的值修改为B，否则什么都不干。其伪代码如下 123456if(this.value == A){ this.value = B return true;}else{ return false;} JUC下的atomic类都是通过CAS来实现的，下面就以AtomicInteger为例来阐述CAS的实现。如下： 1234567891011private static final Unsafe unsafe = Unsafe.getUnsafe();private static final long valueOffset;static { try { valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField(&quot;value&quot;)); } catch (Exception ex) { throw new Error(ex); }}private volatile int value; CAS可以保证一次的读-改-写操作是原子操作，在单处理器上该操作容易实现，但是在多处理器上实现就有点儿复杂了。 CPU提供了两种方法来实现多处理器的原子操作： 总线加锁：总线加锁就是就是使用处理器提供的一个LOCK#信号，当一个处理器在总线上输出此信号时，其他处理器的请求将被阻塞住，那么该处理器可以独占使用共享内存。 但是这种处理方式显得有点儿霸道，不厚道，把CPU和内存之间的通信锁住了，在锁定期间，其他处理器都不能其他内存地址的数据，其开销有点儿大。所以就有了缓存加锁。 缓存加锁：其实针对于上面那种情况，只需要保证在同一时刻对某个内存地址的操作是原子性的即可。缓存加锁就是缓存在内存区域的数据如果在加锁期间，当它执行锁操作写回内存时，处理器不在输出LOCK#信号，而是修改内部的内存地址，利用缓存一致性协议来保证原子性。 缓存一致性机制可以保证同一个内存区域的数据仅能被一个处理器修改，也就是说当CPU1修改缓存行中的i时使用缓存锁定，那么CPU2就不能同时缓存了i的缓存行。 CAS缺陷CAS虽然高效地解决了原子操作，但是还是存在一些缺陷的，主要表现在三个方法：循环时间太长、只能保证一个共享变量原子操作、ABA问题。 循环时间太长 如果CAS一直不成功呢？这种情况绝对有可能发生，如果自旋CAS长时间地不成功，则会给CPU带来非常大的开销。在JUC中有些地方就限制了CAS自旋的次数，例如BlockingQueue的SynchronousQueue。 只能保证一个共享变量原子操作 看了CAS的实现就知道这只能针对一个共享变量，如果是多个共享变量就只能使用锁了，当然如果你有办法把多个变量整成一个变量，利用CAS也不错。例如读写锁中state的高地位。 ABA问题 CAS需要检查操作值有没有发生改变，如果没有发生改变则更新。但是存在这样一种情况：如果一个值原来是A，变成了B，然后又变成了A，那么在CAS检查的时候会发现没有改变，但是实质上它已经发生了改变，这就是所谓的ABA问题。对于ABA问题其解决方案是加上版本号，即在每个变量都加上一个版本号，每次改变时加1，即A —&gt; B —&gt; A，变成1A —&gt; 2B —&gt; 3A。 CAS的ABA隐患问题，解决方案则是版本号，Java提供了AtomicStampedReference来解决。AtomicStampedReference通过包装[E,Integer]的元组来对对象标记版本戳stamp，从而避免ABA问题。 AtomicStampedReference#compareAndSet()方法定义如下： 12345678910111213141516171819 /** * @param expectedReference the expected value of the reference * @param newReference the new value for the reference * @param expectedStamp the expected value of the stamp * @param newStamp the new value for the stamp * @return {@code true} if successful */public boolean compareAndSet(V expectedReference, V newReference, int expectedStamp, int newStamp) { Pair&lt;V&gt; current = pair; return expectedReference == current.reference &amp;&amp; expectedStamp == current.stamp &amp;&amp; ( (newReference == current.reference &amp;&amp; newStamp == current.stamp) || casPair(current, Pair.of(newReference, newStamp)) );} 当预期的引用(expectedReference) == 当前引用(current.reference)并且预期的标识(expectedStamp) == 当前标识(current.stamp)，如果更新后的引用和标志和当前的引用和标志相等则直接返回true，否则通过Pair生成一个新的pair对象与当前pair CAS替换。 Pair为AtomicStampedReference的内部类，主要用于记录引用和版本戳信息（标识），定义如下： 12345678910111213private static class Pair&lt;T&gt; { final T reference; final int stamp; private Pair(T reference, int stamp) { this.reference = reference; this.stamp = stamp; } static &lt;T&gt; Pair&lt;T&gt; of(T reference, int stamp) { return new Pair&lt;T&gt;(reference, stamp); }}private volatile Pair&lt;V&gt; pair; Pair记录着对象的引用和版本戳，版本戳为int型。同时Pair是一个不可变对象，其所有属性全部定义为final，对外提供一个of方法，该方法返回一个新建的Pari对象。pair对象定义为volatile，保证多线程环境下的可见性。 在AtomicStampedReference中，大多方法都是通过调用Pair的of方法来产生一个新的Pair对象，然后赋值给变量pair。如set方法： 12345public void set(V newReference, int newStamp) { Pair&lt;V&gt; current = pair; if (newReference != current.reference || newStamp != current.stamp) this.pair = Pair.of(newReference, newStamp);} AtomicStampedReference和AtomicInteger区别的示例我们定义两个线程，线程1负责将100 —&gt; 110 —&gt; 100，线程2执行 100 —&gt;120，看两者之间的区别。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879public class AtomicStampedReferenceExample { private static AtomicInteger atomicInteger = new AtomicInteger(100); private static AtomicStampedReference atomicStampedReference = new AtomicStampedReference(100,1); public static void main(String[] args) throws InterruptedException { //AtomicInteger Thread at1 = new Thread(new Runnable() { @Override public void run() { atomicInteger.compareAndSet(100,110); atomicInteger.compareAndSet(110,100); } }); Thread at2 = new Thread(new Runnable() { @Override public void run() { try { TimeUnit.SECONDS.sleep(2); // at1,执行完 } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(&quot;AtomicInteger:&quot; + atomicInteger.compareAndSet(100,120)); } }); at1.start(); at2.start(); at1.join(); at2.join(); //AtomicStampedReference Thread tsf1 = new Thread(new Runnable() { @Override public void run() { try { //让 tsf2先获取stamp，导致预期时间戳不一致 TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } // 预期引用：100，更新后的引用：110，预期标识getStamp() // 更新后的标识getStamp() + 1 atomicStampedReference.compareAndSet(100,110, atomicStampedReference.getStamp(), atomicStampedReference.getStamp() + 1); atomicStampedReference.compareAndSet(110,100, atomicStampedReference.getStamp(), atomicStampedReference.getStamp() + 1); } }); Thread tsf2 = new Thread(new Runnable() { @Override public void run() { int stamp = atomicStampedReference.getStamp(); try { TimeUnit.SECONDS.sleep(2); //线程tsf1执行完 } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(&quot;AtomicStampedReference:&quot; + atomicStampedReference.compareAndSet(100,120,stamp,stamp + 1)); } }); tsf1.start(); tsf2.start(); }} Unsafe 原文地址：https://www.cnblogs.com/pkufork/p/java_unsafe.html Unsafe类是在sun.misc包下，不属于Java标准。但是很多Java的基础类库，包括一些被广泛使用的高性能开发库都是基于Unsafe类开发的，比如Netty、Cassandra、Hadoop、Kafka等。Unsafe类在提升Java运行效率，增强Java语言底层操作能力方面起了很大的作用。 Unsafe类使Java拥有了像C语言的指针一样操作内存空间的能力，同时也带来了指针的问题。过度的使用Unsafe类会使得出错的几率变大，因此Java官方并不建议使用的，官方文档也几乎没有。 通常我们最好也不要使用Unsafe类，除非有明确的目的，并且也要对它有深入的了解才行。 要想使用Unsafe类需要用一些比较tricky的办法。Unsafe类使用了单例模式，需要通过一个静态方法getUnsafe()来获取。但Unsafe类做了限制，如果是普通的调用的话，它会抛出一个SecurityException异常；只有由主类加载器加载的类才能调用这个方法。其源码如下 12345678public static Unsafe getUnsafe() { Class var0 = Reflection.getCallerClass(); if(!VM.isSystemDomainLoader(var0.getClassLoader())) { throw new SecurityException(&quot;Unsafe&quot;); } else { return theUnsafe; }} 网上也有一些办法来用主类加载器加载用户代码，比如设置bootclasspath参数。但更简单方法是利用Java反射，方法如下： 123Field f = Unsafe.class.getDeclaredField(&quot;theUnsafe&quot;);f.setAccessible(true);Unsafe unsafe = (Unsafe) f.get(null); 获取到Unsafe实例之后，我们就可以为所欲为了。Unsafe类提供了以下这些功能： 内存管理（分配内存、释放内存） ​ 该部分包括了allocateMemory（分配内存）、reallocateMemory（重新分配内存）、copyMemory（拷贝内存）、freeMemory（释放内存 ）、getAddress（获取内存地址）、addressSize、pageSize、getInt（获取内存地址指向的整数）、getIntVolatile（获取内存地址指向的整数，并支持volatile语义）、putInt（将整数写入指定内存地址）、putIntVolatile（将整数写入指定内存地址，并支持volatile语义）、putOrderedInt（将整数写入指定内存地址、有序或者有延迟的方法）等方法。getXXX和putXXX包含了各种基本类型的操作。 ​ 利用copyMemory方法，我们可以实现一个通用的对象拷贝方法，无需再对每一个对象都实现clone方法，当然这通用的方法只能做到对象浅拷贝。 非常规的对象实例化。 ​ allocateInstance()方法提供了另一种创建实例的途径。通常我们可以用new或者反射来实例化对象，使用allocateInstance()方法可以直接生成对象实例，且无需调用构造方法和其它初始化方法。 ​ 这在对象反序列化的时候会很有用，能够重建和设置final字段，而不需要调用构造方法。 操作类、对象、变量。 ​ 这部分包括了staticFieldOffset（静态域偏移）、defineClass（定义类）、defineAnonymousClass（定义匿名类）、ensureClassInitialized（确保类初始化）、objectFieldOffset（对象域偏移）等方法。 ​ 通过这些方法我们可以获取对象的指针，通过对指针进行偏移，我们不仅可以直接修改指针指向的数据（即使它们是私有的），甚至可以找到JVM已经认定为垃圾、可以进行回收的对象。 数组操作。 ​ 这部分包括了arrayBaseOffset（获取数组第一个元素的偏移地址）、arrayIndexScale（获取数组中元素的增量地址）等方法。arrayBaseOffset与arrayIndexScale配合起来使用，就可以定位数组中每个元素在内存中的位置。 ​ 由于Java的数组最大值为Integer.MAX_VALUE，使用Unsafe类的内存分配方法可以实现超大数组。实际上这样的数据就可以认为是C数组，因此需要注意在合适的时间释放内存。 多线程同步（锁机制、CAS操作）。 ​ 这部分包括了monitorEnter、tryMonitorEnter、monitorExit、compareAndSwapInt、compareAndSwap等方法。 ​ 其中monitorEnter、tryMonitorEnter、monitorExit已经被标记为deprecated，不建议使用。 ​ Unsafe类的CAS操作可能是用的最多的，它为Java的锁机制提供了一种新的解决办法，比如AtomicInteger等类都是通过该方法来实现的。compareAndSwap方法是原子的，可以避免繁重的锁机制，提高代码效率。这是一种乐观锁，通常认为在大部分情况下不出现竞态条件，如果操作失败，会不断重试直到成功。 挂起与恢复。 ​ 这部分包括了park、unpark等方法。 ​ 将一个线程进行挂起是通过park方法实现的，调用 park后，线程将一直阻塞直到超时或者中断等条件出现。unpark可以终止一个挂起的线程，使其恢复正常。整个并发框架中对线程的挂起操作被封装在 LockSupport类中，LockSupport类中有各种版本pack方法，但最终都调用了Unsafe.park()方法。 7. 内存屏障。 ​ 这部分包括了loadFence、storeFence、fullFence等方法。这是在Java 8新引入的，用于定义内存屏障，避免代码重排序。 ​ loadFence() 表示该方法之前的所有load操作在内存屏障之前完成。同理storeFence()表示该方法之前的所有store操作在内存屏障之前完成。fullFence()表示该方法之前的所有load、store操作在内存屏障之前完成。","link":"/posts/62686.html"},{"title":"Java并发编程：ConcurrentHashMap","text":"ConcurrentHashMap简介HashMap是我们用得非常频繁的一个集合，但是由于它是非线程安全的，在多线程环境下，put操作是有可能产生死循环的，导致CPU利用率接近100%。 为了解决该问题，提供了Hashtable和Collections.synchronizedMap(hashMap)两种解决方案，但是这两种方案都是对读写加锁、独占式，一个线程在读时其他线程必须等待，吞吐量较低，性能较为低下。 关键属性及类12345// 存储节点的数组transient volatile Node&lt;K,V&gt;[] table;// 仅扩容时才会使用private transient volatile Node&lt;K,V&gt;[] nextTable; Node节点 12345678910111213static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final int hash; final K key; volatile V val; volatile Node&lt;K,V&gt; next; Node(int hash, K key, V val, Node&lt;K,V&gt; next) { this.hash = hash; this.key = key; this.val = val; this.next = next; }} ForwardingNode类 12345678// static final int MOVED = -1;static final class ForwardingNode&lt;K,V&gt; extends Node&lt;K,V&gt; { final Node&lt;K,V&gt;[] nextTable; ForwardingNode(Node&lt;K,V&gt;[] tab) { super(MOVED, null, null, null); this.nextTable = tab; }} TreeBin类 1234static final class TreeBin&lt;K,V&gt; extends Node&lt;K,V&gt; { TreeNode&lt;K,V&gt; root; volatile TreeNode&lt;K,V&gt; first;} 重点方法讲解初始化ConcurrentHashMap提供了一系列的构造函数用于创建ConcurrentHashMap对象，如下： 12345678910111213141516171819202122232425262728293031public ConcurrentHashMap() {}public ConcurrentHashMap(int initialCapacity) { if (initialCapacity &lt; 0) throw new IllegalArgumentException(); int cap = ((initialCapacity &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(initialCapacity + (initialCapacity &gt;&gt;&gt; 1) + 1)); this.sizeCtl = cap;}public ConcurrentHashMap(Map&lt;? extends K, ? extends V&gt; m) { this.sizeCtl = DEFAULT_CAPACITY; putAll(m);}public ConcurrentHashMap(int initialCapacity, float loadFactor) { this(initialCapacity, loadFactor, 1);}public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) { if (!(loadFactor &gt; 0.0f) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); if (initialCapacity &lt; concurrencyLevel) // Use at least as many bins initialCapacity = concurrencyLevel; // as estimated threads long size = (long)(1.0 + (long)initialCapacity / loadFactor); int cap = (size &gt;= (long)MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : tableSizeFor((int)size); this.sizeCtl = cap;} put方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071final V putVal(K key, V value, boolean onlyIfAbsent) { // 1、检测key和value都不能为空 if (key == null || value == null) throw new NullPointerException(); // 2、计算key的hash值 int hash = spread(key.hashCode()); int binCount = 0; // “死循环” for (Node&lt;K,V&gt;[] tab = table;;) { Node&lt;K,V&gt; f; int n, i, fh; if (tab == null || (n = tab.length) == 0) // table是否初始化 // 初始化table tab = initTable(); else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) { // 要存储节点的槽为空，直接cas设置。 if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin } else if ((fh = f.hash) == MOVED) // 表示map正在数据迁移，辅助数据迁移 tab = helpTransfer(tab, f); else { V oldVal = null; synchronized (f) { if (tabAt(tab, i) == f) { if (fh &gt;= 0) { binCount = 1; for (Node&lt;K,V&gt; e = f;; ++binCount) { K ek; if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) { oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; } Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) { pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; } } } else if (f instanceof TreeBin) { Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) { oldVal = p.val; if (!onlyIfAbsent) p.val = value; } } } } if (binCount != 0) { if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) // oldVal不为空（替换操作），直接返回。 return oldVal; break; } } } addCount(1L, binCount); return null;} initTable方法123456789101112131415161718192021private final Node&lt;K,V&gt;[] initTable() { Node&lt;K,V&gt;[] tab; int sc; while ((tab = table) == null || tab.length == 0) { if ((sc = sizeCtl) &lt; 0) Thread.yield(); // lost initialization race; just spin else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) { try { if ((tab = table) == null || tab.length == 0) { int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = tab = nt; sc = n - (n &gt;&gt;&gt; 2); } } finally { sizeCtl = sc; } break; } } return tab;} helpTransfer方法123456789101112131415161718final Node&lt;K,V&gt;[] helpTransfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt; f) { Node&lt;K,V&gt;[] nextTab; int sc; if (tab != null &amp;&amp; (f instanceof ForwardingNode) &amp;&amp; (nextTab = ((ForwardingNode&lt;K,V&gt;)f).nextTable) != null) { int rs = resizeStamp(tab.length); while (nextTab == nextTable &amp;&amp; table == tab &amp;&amp; (sc = sizeCtl) &lt; 0) { if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || transferIndex &lt;= 0) break; if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) { transfer(tab, nextTab); break; } } return nextTab; } return table;} MAX_RESIZERS:辅助扩容的最大线程数，65535（(1 &lt;&lt; (32 - RESIZE_STAMP_BITS)) - 1）。 123456789101112131415161718while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) { // 一定返回负数 int rs = resizeStamp(n); if (sc &lt; 0) { // 说明map正在扩容 if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; // 辅助扩容 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); } else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); s = sumCount();} transfer方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) { int n = tab.length, stride; if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range if (nextTab == null) { // initiating nextTab try { Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; } catch (Throwable ex) { // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; } nextTable = nextTab; transferIndex = n; } int nextn = nextTab.length; //初始化ForwardingNode ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); boolean advance = true; boolean finishing = false; // to ensure sweep before committing nextTab for (int i = 0, bound = 0;;) { Node&lt;K,V&gt; f; int fh; // 计算每个扩容线程从那个索引位置（i）开始迁移数据 while (advance) { int nextIndex, nextBound; if (--i &gt;= bound || finishing) advance = false; else if ((nextIndex = transferIndex) &lt;= 0) { i = -1; advance = false; } else if (U.compareAndSwapInt(this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) { bound = nextBound; i = nextIndex - 1; advance = false; } } // 检测扩容操作是否完成及设置sizeCtl if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) { int sc; if (finishing) { nextTable = null; table = nextTab; sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); return; } if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) { if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return; finishing = advance = true; i = n; // recheck before commit } }else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); else if ((fh = f.hash) == MOVED) advance = true; // already processed else { // 原数组[i]不为null，则说明存在元素 synchronized (f) { if (tabAt(tab, i) == f) { Node&lt;K,V&gt; ln, hn; if (fh &gt;= 0) { int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) { int b = p.hash &amp; n; if (b != runBit) { runBit = b; lastRun = p; } } if (runBit == 0) { ln = lastRun; hn = null; }else { hn = lastRun; ln = null; } // for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) { int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); } setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; }else if (f instanceof TreeBin) { TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) { int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) { if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; } else { if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; } } ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; } } } } }} get方法12345678910111213141516171819public V get(Object key) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; int h = spread(key.hashCode()); if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) { if ((eh = e.hash) == h) { if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; } else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; while ((e = e.next) != null) { if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; } } return null;}","link":"/posts/58814.html"},{"title":"Java并发编程：DelayQueue","text":"java.util.concurrent.DelayQueue DelayQueue是一个支持延时获取元素的无界阻塞队列。队列使用PriorityBlockingQueue来实现。队列中的元素必须实现Delayed接口，在创建元素时可以指定多久才能从队列中获取当前元素。只有在延迟期满时才能从队列中提取元素。 DelayQueue运用在以下应用场景： 缓存系统的设计：可以用DelayQueue保存缓存元素的有效期，使用一个线程循环查询DelayQueue，一旦能从DelayQueue中获取元素时，表示缓存有效期到了。 任务超时处理：比如下单后15分钟内未付款，自动关闭订单。 如何实现Delayed接口DelayQueue队列的元素必须实现Delayed接口。可以参考ScheduledThreadPoolExecutor里ScheduledFutureTask类的实现，步骤如下： 在对象创建的对象，初始化基本数据。使用time记录当前对象延迟到什么时候可以使用，使用sequenceNumber来标识元素在队列中的先后顺序。代码如下： 12345678private static final AtomicLong sequencer = new AtomicLong();ScheduledFutureTask(Runnable r, V result, long ns) { super(r, result); this.time = ns; this.period = 0; this.sequenceNumber = sequencer.getAndIncrement();} 实现getDelay方法，该方法返回当前元素还需要延长多长时间，单位是纳秒。代码如下： 123public long getDelay(TimeUnit unit) { return unit.convert(time - now(), NANOSECONDS);} 通过构造函数可以看出延迟时间参数ns的单位是纳秒，自己设计的时候最好使用纳秒，实现getDelay()方法时可以指定任意单位，一旦以秒或分作为单位，而延时时间精确不到纳秒就麻烦了。使用时注意当time小于当前时间时，getDelay会返回负数。 实现compareTo方法来指定元素的顺序。例如，让延时时间最长的放在队列的末尾。代码如下 123456789101112131415161718public int compareTo(Delayed other) { if (other == this) // compare zero if same object return 0; if (other instanceof ScheduledFutureTask) { ScheduledFutureTask&lt;?&gt; x = (ScheduledFutureTask&lt;?&gt;)other; long diff = time - x.time; if (diff &lt; 0) return -1; else if (diff &gt; 0) return 1; else if (sequenceNumber &lt; x.sequenceNumber) return -1; else return 1; } long diff = getDelay(NANOSECONDS) - other.getDelay(NANOSECONDS); return (diff &lt; 0) ? -1 : (diff &gt; 0) ? 1 : 0;} 如何实现延时阻塞队列延时阻塞队列的实现很简单，当消费者从队列里获取元素时，如果元素没有达到延时时间，就阻塞当前线程。 12345678910111213141516171819202122232425262728293031323334353637private Thread leader = null; private final Condition available = lock.newCondition();public E take() throws InterruptedException { final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try { for (;;) { E first = q.peek(); if (first == null) available.await(); else { long delay = first.getDelay(NANOSECONDS); if (delay &lt;= 0) return q.poll(); first = null; // don't retain ref while waiting if (leader != null) available.await(); else { Thread thisThread = Thread.currentThread(); leader = thisThread; try { available.awaitNanos(delay); } finally { if (leader == thisThread) leader = null; } } } } } finally { if (leader == null &amp;&amp; q.peek() != null) available.signal(); lock.unlock(); }}","link":"/posts/42419.html"},{"title":"Java并发编程：LockSupport","text":"本文由【JDK1.8】JUC——LockSupport和【细谈Java并发】谈谈LockSupport这两篇文章整理而来。 LockSupport简介LockSupport是用来创建锁和其他同步类的基本线程阻塞原语。 此类以及每个使用它的线程与一个许可关联（从 Semaphore 类的意义上说）。如果该许可可用，并且可在进程中使用，则调用park将立即返回；否则可能阻塞。如果许可尚不可用，则可以调用unpark使其可用。（但与Semaphore 不同的是，许可不能累积，并且最多只能有一个许可。） park 和unpark方法提供了阻塞和解除阻塞线程的有效方法，并且不会遇到导致过时方法Thread.suspend和Thread.resume 因为以下目的变得不可用的问题：由于许可的存在，调用park 的线程和另一个试图将其unpark的线程之间的竞争将保持活性。 此外，如果调用者线程被中断，并且支持超时，则park将返回。park方法还可以在其他任何时间“毫无理由”地返回，因此通常必须在重新检查返回条件的循环里调用此方法。从这个意义上说，park是“忙碌等待”的一种优化，它不会浪费这么多的时间进行自旋，但是必须将它与unpark配对使用才更高效。 LockSupport定义了如下静态方法： 三种形式的park还各自支持一个blocker对象参数。此对象在线程受阻塞时被记录，以允许监视工具和诊断工具确定线程受阻塞的原因。（这样的工具可以使用方法 getBlocker(java.lang.Thread) 访问blocker。）建议最好使用这些形式，而不是不带此参数的原始形式。在锁实现中提供的作为blocker的普通参数是this。 这些方法被设计用来作为创建高级同步实用工具的工具，对于大多数并发控制应用程序而言，它们本身并不是很有用。park 方法仅设计用于以下形式的构造： 123while (!canProceed()) { ... LockSupport.park(this);} 在这里，在调用park之前，canProceed和其他任何动作都不会锁定或阻塞。因为每个线程只与一个许可关联，park的任何中间使用都可能干扰其预期效果。 LockSupport成员变量分析12345678910111213141516171819private static final sun.misc.Unsafe UNSAFE;private static final long parkBlockerOffset;private static final long SEED;private static final long PROBE;private static final long SECONDARY;static { try { UNSAFE = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; tk = Thread.class; parkBlockerOffset = UNSAFE.objectFieldOffset (tk.getDeclaredField(&quot;parkBlocker&quot;)); SEED = UNSAFE.objectFieldOffset (tk.getDeclaredField(&quot;threadLocalRandomSeed&quot;)); PROBE = UNSAFE.objectFieldOffset (tk.getDeclaredField(&quot;threadLocalRandomProbe&quot;)); SECONDARY = UNSAFE.objectFieldOffset (tk.getDeclaredField(&quot;threadLocalRandomSecondarySeed&quot;)); } catch (Exception ex) { throw new Error(ex); }} 首先要明确的就是sun.misc.Unsafe这个类，它是一个final class，里面有100多个方法，锁的实现也是依赖了这个类，其中基本上都是native方法。Java避免了程序员直接操作内存，但这不是绝对的，通过使用Unsafe类，还是能够操作内存。 parkBlockerOffset。从字面上看就是parkBlocker的偏移量，那么parkBlocker是干嘛的呢，从static代码块中可以看到，它属于Thread类，于是进去看看： 1234567/** * The argument supplied to the current call to * java.util.concurrent.locks.LockSupport.park. * Set by (private) java.util.concurrent.locks.LockSupport.setBlocker * Accessed using java.util.concurrent.locks.LockSupport.getBlocker */volatile Object parkBlocker; 从注释上看，就是给LockSupport的setBlocker和getBlocker调用。另外在LockSupport的java doc中也写到： This object is recorded while the thread is blocked to permit monitoring and diagnostic tools to identify the reasons that threads are blocked. (Such tools may access blockers using method [getBlocker(Thread).) The use of these forms rather than the original forms without this parameter is strongly encouraged. The normal argument to supply as a blockerwithin a lock implementation is this. 大致是说，parkBlocker是当线程被阻塞的时候被记录，以便监视和诊断工具来识别线程被阻塞的原因。 Unsafe类提供了获取某个字段相对Java对象的“起始地址”的偏移量的方法objectFieldOffset，从而能够获取该字段的值。那么为什么记录该blocker在对象中的偏移量，而不是直接调用Thread.getBlocker()，这样不是更好。 parkBlocker就是在线程处于阻塞的情况下才会被赋值。线程都已经阻塞了，如果不通过这种内存的方法，而是直接调用线程内的方法，线程是不会回应调用的。 LockSupport的重要方法park()park方法阻塞的是当前的线程，也就是说在哪个线程中调用，那么哪个线程就被阻塞（在没有获得许可的情况下）。 123public static void park() { UNSAFE.park(false, 0L);} UNSAFE.park的两个参数，第一个参数为true的时候表示传入的是绝对时间，false表示相对时间，即从当前时间开始算。第二个参数就是等待的时间，0L表示永久等待。 根据java doc中的描述，调用park后有三种情况，能使线程继续执行下去： 有某个线程调用了当前线程的unpark。 其他线程中断（interrupt）了当前线程 该调用不合逻辑地（即毫无理由地）返回。 验证一： 123456789101112131415161718192021222324252627282930public class UnparkTest { public static void main(String[] args) throws InterruptedException { Thread ut = new Thread(new UnparkThread(Thread.currentThread())); ut.start(); System.out.println(&quot;I'm going to call park&quot;); // Thread.sleep(1000L); LockSupport.park(); System.out.println(&quot;oh, I'm running again&quot;); }}class UnparkThread implements Runnable { private final Thread t; UnparkThread(Thread t) { this.t = t; } @Override public void run() { try { Thread.sleep(1000L); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(&quot;I'm in unpark&quot;); LockSupport.unpark(t); System.out.println(&quot;I called unpark&quot;); }} 运行结果： LockSupport对park和unpark的调用顺序并没有要求，将两个Thread.sleep(1000L);注释切换一下就可以发现，先调用unpark，再调用park，依旧可以获得许可，让线程继续运行。这一点与Object的wait和notify 要求固定的顺序不同。 验证二： 123456789101112131415161718public class LockSupportInterrupt { public static void main(String[] args) throws InterruptedException { Thread t = new Thread(new InterruptThread()); t.start(); Thread.sleep(1000L); System.out.println(&quot;I'm going to interrupt&quot;); t.interrupt(); }}class InterruptThread implements Runnable { @Override public void run() { System.out.println(&quot;I'm going to park&quot;); LockSupport.park(); System.out.println(&quot;I'm going to again&quot;); }} 运行结果： LockSupport的park能够能响应interrupt事件，且不会抛出InterruptedException异常。 park(Object blocker)123456public static void park(Object blocker) { Thread t = Thread.currentThread(); setBlocker(t, blocker); UNSAFE.park(false, 0L); setBlocker(t, null);} 在调用park阻塞当前线程之前，先记录当前线程的blocker。 调用park阻塞当前线程 当前面提到的三个让线程继续执行下去的情况时，再将parkBlocker设置为null，因为当前线程已经没有被blocker住了，如果不设置为null，那诊断工具获取被阻塞的原因就是错误的，这也是为什么要有两个setBlocker的原因。 再看一下setBlocker的代码： 1234private static void setBlocker(Thread t, Object arg) { // Even though volatile, hotspot doesn't need a write barrier here. UNSAFE.putObject(t, parkBlockerOffset, arg);} 方法是私有的，嗯，为了保证正确性，肯定不能被其他类调用。另外就是利用了之前提到的偏移量以及unsafe对象将blocker值设置进了线程t当中。 unpark(Thread thread)1234public static void unpark(Thread thread) { if (thread != null) UNSAFE.unpark(thread);} 判断是否为空，然后调用unsafe的unpark方法。由此更可见unsafe这个类的重要性。 park和unpark实现原理可能有些朋友还是不理解“许可”这个概念，我们深入HotSpot的源码来看看。 每个java线程都有一个Parker实例，Parker类是这样定义的： 可以看到Parker类实际上用Posix的mutex，condition来实现的。在Parker类里的_counter字段，就是用来记录所谓的“许可”的。 当调用park时，先尝试直接能否直接拿到“许可”，即 _counter&gt;0时，如果成功，则把 _counter设置为0,并返回： 如果不成功，则构造一个ThreadBlockInVM，然后检查 _counter是不是&gt;0，如果是，则把 _counter设置为0，unlock mutex并返回： 否则，再判断等待的时间，然后再调用pthreadcondwait函数等待，如果等待返回，则把_counter设置为0，unlock mutex并返回： 当unpark时，则简单多了，直接设置 _counter为1，再unlock mutext返回。如果 _counter之前的值是0，则还要调用pthreadcondsignal唤醒在park中等待的线程： 简而言之，是用mutex和condition保护了一个_counter的变量，当park时，这个变量置为了0，当unpark时，这个变量置为1。 值得注意的是在park函数里，调用pthreadcondwait时，并没有用while来判断，所以posix condition里的”Spurious wakeup”一样会传递到上层Java的代码里。关于”Spurious wakeup”，可以参考：并行编程之条件变量（posix condition variables） 各种例子jstack查看parkBlocker12345678public class JstackTest { public static void main(String[] args) { // 给main线程设置名字，好查找一点 Thread.currentThread().setName(&quot;jstacktest&quot;); LockSupport.park(&quot;block&quot;); }} 利用park(blocker)来阻塞main线程，传入string作为parkBlocker。 运行之后，运行jps命令: 然后再利用jstack来查看： 利用LockSupport实现先进先出锁1234567891011121314151617181920212223242526272829public class FIFOMutex { private final AtomicBoolean locked = new AtomicBoolean(false); private final Queue&lt;Thread&gt; waiters = new ConcurrentLinkedQueue&lt;Thread&gt;(); public void lock() { boolean wasInterrupted = false; Thread current = Thread.currentThread(); waiters.add(current); // Block while not first in queue or cannot acquire lock while (waiters.peek() != current || !locked.compareAndSet(false, true)) { LockSupport.park(this); if (Thread.interrupted()) // ignore interrupts while waiting wasInterrupted = true; } waiters.remove(); if (wasInterrupted) // reassert interrupt status on exit current.interrupt(); } public void unlock() { locked.set(false); LockSupport.unpark(waiters.peek()); }} 先进先出锁就是先申请锁的线程最先获得锁的资源，实现上采用了队列再加上LockSupport.park。 将当前调用lock的线程加入队列 如果等待队列的队首元素不是当前线程或者locked为true，则说明有线程已经持有了锁，那么调用park阻塞其余的线程。 如果队首元素是当前线程且locked为false，则说明前面已经没有人持有锁，删除队首元素也就是当前的线程，然后当前线程继续正常执行。 执行完后调用unlock方法将锁变量修改为false,并解除队首线程的阻塞状态。此时的队首元素继续之前的判断。","link":"/posts/16452.html"},{"title":"Java并发编程：Lock（锁）","text":"锁是用来控制多个线程访问共享资源的方式，一般来说，一个锁能够防止多线程同时访问共享资源（但是有些锁可以允许多个线程并发的访问共享资源，比如读写锁）。 Lock接口（以及相关实现类）是JavaSE 5之后新增的用来实现锁功能，它提供了与synchronized关键字类似的同步功能，只是在使用时需要显式地获取和释放锁。 LockLock接口提供的方法如下： void lock() 获取锁，调用该方法当前线程将会获取锁，当锁获得后，从该方法返回。 void lockInterruptibly() throws InterruptedException 可中断地获取锁，即在锁的获取中可以中断当前线程。 boolean tryLock() 尝试非阻塞获取锁 boolean tryLock(long time, TimeUnit unit) throws InterruptedException 超时的获取锁 void unlock() 释放锁 Condition newCondition() 获取等待通知组件，提供类似wait()、nofity()和notifyAll的功能 Lock Usage12345678910// 显式创建lock对象Lock lock = new ReentrantLock();// 加锁lock.lock();try {} finally { // 释放锁 lock.unlock();} 在finally块中释放锁，目的是保证在获取到锁之后，最终能够被释放。不要将获取锁的过程写在try块中，因为如果在获取锁（自定义锁的实现）时发生了异常，异常抛出的同时，也会导致锁无故释放。 虽然它缺少了（通过synchronized块或者方法所提供的）隐式获取、释放锁的便捷性，但是却拥有了锁获取与释放的可操作性、可中断的获取锁以及超时获取锁等多种synchronized关键字所不具备的同步特性。如下： 尝试非阻塞地获取锁 当前线程尝试获取锁，如果这一时刻没有被其他线程获取到，则成功获取并持有锁。 能被中断地获取锁 与synchronized不同，获取到锁的线程能够响应中断，当获取到锁的线程被中断时，中断异常将会被抛出，同时锁会被释放。 超时获取锁 在指定的截止时间之前获取锁，如果截止时间到了依旧无法获取锁，则返回。 重入锁（ReentrantLock）重入锁ReentrantLock，顾名思义，就是支持重进入的锁，它表示该锁能够支持一个线程对资源的重复加锁。除此之外，该锁的还支持获取锁时的公平和非公平性选择。 ReentrantLock虽然没能像synchronized关键字一样支持隐式的重进入，但是在调用lock()方法时，已经获取到锁的线程，能够再次调用lock()方法获取锁而不被阻塞。 关于锁获取的公平性问题，如果在绝对时间上，先对锁进行获取的请求一定先被满足，那么这个锁是公平的，反之，是不公平的。公平的获取锁，也就是等待时间最长的线程最优先获取锁，也可以说锁获取是顺序的。 ReentrantLock提供了一个构造函数，能够控制锁是否是公平的。 1234567public ReentrantLock() { sync = new NonfairSync();}public ReentrantLock(boolean fair) { sync = fair ? new FairSync() : new NonfairSync();} 事实上，公平的锁机制往往没有非公平的效率高，但是，并不是任何场景都是以TPS作为唯一的指标，公平锁能够减少“饥饿”发生的概率，等待越久的请求越是能够得到优先满足。 实现重进入重进入是指任意线程在获取到锁之后能够再次获取该锁而不会被锁所阻塞，该特性的实现需要解决以下两个问题： 1）线程再次获取锁：锁需要去识别获取锁的线程是否为当前占据锁的线程，如果是，则再次成功获取。 2）锁的最终释放：线程重复n次获取了锁，随后在第n次释放该锁后，其他线程能够获取到该锁。锁的最终释放要求：锁对于获取进行计数自增，计数表示当前锁被重复获取的次数，而锁被释放时，计数自减，当计数等于0时表示锁已经成功释放。 以非公平性（默认的）实现为例，获取同步状态的代码如下： 123456789101112131415161718final boolean nonfairTryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { if (compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; } return false;} nonfairTryAcquire()增加了再次获取同步状态的处理逻辑：通过判断当前线程是否为获取锁的线程来决定获取操作是否成功，如果是获取锁的线程再次请求，则将同步状态值进行增加并返回true，表示获取同步状态成功。 成功获取锁的线程再次获取锁，只是增加了同步状态值，这也就要求ReentrantLock在释放同步状态时减少同步状态值，该方法的代码如下： 123456789101112protected final boolean tryRelease(int releases) { int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) { free = true; setExclusiveOwnerThread(null); } setState(c); return free;} 如果该锁被获取了n次，那么前(n-1)次tryRelease(int releases)方法必须返回false，而只有同步状态完全释放了，才能返回true。可以看到，该方法将同步状态是否为0作为最终释放的条件，当同步状态为0时，将占有线程设置为null，并返回true，表示释放成功。 公平与非公平获取锁的区别公平性与否是针对获取锁而言的，如果一个锁是公平的，那么锁的获取顺序就应该符合请求的绝对时间顺序，也就是FIFO。 公平锁的tryAcquire()方法代码如下： 12345678910111213141516171819protected final boolean tryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc &lt; 0) throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; } return false;} hasQueuedPredecessors()方法，即加入了同步队列中当前节点是否有前驱节点的判断，如果该方法返回true，则表示有线程比当前线程更早地请求获取锁，因此需要等待前驱线程获取并释放锁之后才能继续获取锁。 读写锁（ReentrantReadWriteLock）读写锁在同一时刻可以允许多个读线程访问，但是在写线程访问时，所有的读线程和其他写线程均被阻塞。读写锁维护了一对锁，一个读锁和一个写锁，通过分离读锁和写锁，使得并发性相比一般的排他锁有了很大提升。 12345678910111213141516public class ReentrantReadWriteLock implements ReadWriteLock, java.io.Serializable { private final ReentrantReadWriteLock.ReadLock readerLock; private final ReentrantReadWriteLock.WriteLock writerLock; public ReentrantReadWriteLock() { this(false); } public ReentrantReadWriteLock(boolean fair) { sync = fair ? new FairSync() : new NonfairSync(); readerLock = new ReadLock(this); writerLock = new WriteLock(this); }} 一般情况下，读写锁的性能都会比排它锁好，因为大多数场景读是多于写的。在读多于写的情况下，读写锁能够提供比排它锁更好的并发性和吞吐量。 使用方式12345678910111213141516171819202122232425262728293031323334353637383940public class Cache { static Map&lt;String,Object&gt; map = new HashMap&lt;String,Object&gt;(); static ReentrantReadWriteLock rwl = new ReentrantReadWriteLock(); static Lock r = rwl.readLock(); static Lock w = rwl.writeLock(); // 获取一个key对应的value public static final Object get(String key) { r.lock(); try { return map.get(key); }finally { r.unlock(); } } // 设置key对应的value，并返回旧的value public static final Object put(String key,Object value) { w.lock(); try { return map.put(key, value); }finally { w.unlock(); } } // 清空所有的内容 public static final void clear() { w.lock(); try { map.clear(); }finally { w.unlock(); } }} 读写锁的实现分析读写锁同样依赖自定义同步器来实现同步功能，而读写状态就是其同步器的同步状态。读写锁的自定义同步器需要在同步状态（一个整型变量）上维护多个读线程和一个写线程的状态，使得该状态的设计成为读写锁实现的关键。 如果在一个整型变量上维护多种状态，就一定需要“按位切割使用”这个变量，读写锁将变量切分成了两个部分，高16位表示读，低16位表示写，划分方式如图所示： 当前同步状态表示一个线程已经获取了写锁，且重进入了两次，同时也连续获取了两次读锁。读写锁是如何迅速确定读和写各自的状态呢？ 答案是通过位运算。假设当前同步状态值为S，写状态等于S&amp;0x0000FFFF（将高16位全部抹去），读状态等于S&gt;&gt;&gt;16（无符号补0右移16位）。当写状态增加1时，等于S+1，当读状态增加1时，等于S+(1&lt;&lt;16)，也就是S+0x00010000。 写锁的获取与释放写锁是一个支持重进入的排它锁。如果当前线程已经获取了写锁，则增加写状态。如果当前线程在获取写锁时，读锁已经被获取（读状态不为0）或者该线程不是已经获取写锁的线程，则当前线程进入等待状态，获取写锁的代码如下： 123456789101112131415161718192021protected final boolean tryAcquire(int acquires) { Thread current = Thread.currentThread(); int c = getState(); int w = exclusiveCount(c); if (c != 0) { // 如果c != 0 and w == 0时，说明读锁已存在。 if (w == 0 || current != getExclusiveOwnerThread()) return false; // 写锁重人情况 if (w + exclusiveCount(acquires) &gt; MAX_COUNT) throw new Error(&quot;Maximum lock count exceeded&quot;); setState(c + acquires); return true; } // 如果当前线程节点Node存在前驱节点或CAS操作失败，直接返回。 if (writerShouldBlock() || !compareAndSetState(c, c + acquires)) return false; setExclusiveOwnerThread(current); return true;} 该方法除了重入条件（当前线程为获取了写锁的线程）之外，增加了一个读锁是否存在的判断。如果存在读锁，则写锁不能被获取，原因在于：读写锁要确保写锁的操作对读锁可见，如果允许读锁在已被获取的情况下对写锁的获取，那么正在运行的其他读线程就无法感知到当前写线程的操作。因此，只有等待其他读线程都释放了读锁，写锁才能被当前线程获取，而写锁一旦被获取，则其他读写线程的后续访问均被阻塞。 写锁的释放与ReentrantLock的释放过程基本类似，每次释放均减少写状态，当写状态为0时表示写锁已被释放，从而等待的读写线程能够继续访问读写锁，同时前次写线程的修改对后续读写线程可见。 读锁的获取与释放读锁是一个支持重进入的共享锁，它能够被多个线程同时获取，在没有其他写线程访问（或者写状态为0）时，读锁总会被成功地获取，而所做的也只是（线程安全的）增加读状态。如果当前线程已经获取了读锁，则增加读状态。如果当前线程在获取读锁时，写锁已被其他线程获取，则进入等待状态。 1234567891011121314151617181920212223242526272829final boolean tryReadLock() { Thread current = Thread.currentThread(); for (;;) { int c = getState(); // 当存在写锁时且当前线程不是拥有写锁的线程时，则直接返回获取失败； if (exclusiveCount(c) != 0 &amp;&amp; getExclusiveOwnerThread() != current) return false; int r = sharedCount(c); if (r == MAX_COUNT) throw new Error(&quot;Maximum lock count exceeded&quot;); if (compareAndSetState(c, c + SHARED_UNIT)) { if (r == 0) { firstReader = current; firstReaderHoldCount = 1; } else if (firstReader == current) { firstReaderHoldCount++; } else { HoldCounter rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) cachedHoldCounter = rh = readHolds.get(); else if (rh.count == 0) readHolds.set(rh); rh.count++; } return true; } }} 在tryAcquireShared(int unused)方法中，如果其他线程已经获取了写锁，则当前线程获取读锁失败，进入等待状态。如果当前线程获取了写锁或者写锁未被获取，则当前线程（线程安全，依靠CAS保证）增加读状态，成功获取读锁。 读锁的每次释放（线程安全的，可能有多个读线程同时释放读锁）均减少读状态，减少的值是（1&lt;&lt;16）。 锁降级锁降级指的是写锁降级成为读锁，一个拥有写锁的线程\u001c，再获取到读锁，随后释放（先前拥有的）写锁的过程。如果当前线程拥有写锁，然后将其释放，最后再获取读锁，这种分段完成的过程不能称之为锁降级。 锁降级的简单示例,代码如下： 123456789101112131415161718192021222324public void processData() { readLock.lock(); if (!update) { // 必须先释放读锁 readLock.unlock(); // 锁降级从写锁获取到开始 writeLock.lock(); try { if (!update) { // 准备数据的流程（略） update = true; } readLock.lock(); } finally { writeLock.unlock(); } // 锁降级完成，写锁降级为读锁 } try { // 使用数据的流程（略） } finally { readLock.unlock(); }} 当数据发生变更后，update变量（布尔类型且volatile修饰）被设置为false，此时所有访问processData()方法的线程都能够感知到变化，但只有一个线程能够获取到写锁，其他线程会被阻塞在读锁和写锁的lock()方法上。当前线程获取写锁完成数据准备之后，再获取读锁，随后释放写锁，完成锁降级。 锁降级中读锁的获取是否必要呢？ 答案是必要的。主要是为了保证数据的可见性，如果当前线程不获取读锁而是直接释放写锁，假设此刻另一个线程（记作线程T）获取了写锁并修改了数据，那么当前线程无法感知线程T的数据更新。如果当前线程获取读锁，即遵循锁降级的步骤，则线程T将会被阻塞，直到当前线程使用数据并释放读锁之后，线程T才能获取写锁进行数据更新。","link":"/posts/4844.html"},{"title":"Java并发编程：ScheduledThreadPoolExecutor","text":"ScheduledThreadPoolExecutor概述A ThreadPoolExecutor that can additionally schedule commands to run after a given delay, or to execute periodically（***周期性地*）. This class is preferable to（优先于……） Timer when multiple worker threads are needed, or when the additional flexibility（灵活性） or capabilities of ThreadPoolExecutor (which this class extends) are required. Delayed tasks execute no sooner than they are enabled, but without any real-time guarantees about when, after they are enabled, they will commence. Tasks scheduled for exactly the same execution time are enabled in first-in-first-out (FIFO) order of submission. When a submitted task is cancelled before it is run, execution is suppressed. By default, such a cancelled task is not automatically removed from the work queue until its delay elapses. While this enables further inspection and monitoring, it may also cause unbounded retention of cancelled tasks. To avoid this, set setRemoveOnCancelPolicy to true, which causes tasks to be immediately removed from the work queue at time of cancellation. 类层次结构 构造方法1234567891011121314151617public ScheduledThreadPoolExecutor(int corePoolSize) { super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS,new DelayedWorkQueue());}public ScheduledThreadPoolExecutor(int corePoolSize, ThreadFactory threadFactory) { super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS,new DelayedWorkQueue(), threadFactory);}public ScheduledThreadPoolExecutor(int corePoolSize,RejectedExecutionHandler handler) { super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS,new DelayedWorkQueue(), handler);}public ScheduledThreadPoolExecutor(int corePoolSize,ThreadFactory threadFactory, RejectedExecutionHandler handler) { super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS,new DelayedWorkQueue(), threadFactory, handler);} DelayedWorkQueue：延迟队列，类似于java.util.concurrent.DelayQueue。 执行和调度任务方法123456789101112131415public ScheduledFuture&lt;?&gt; schedule(Runnable command,long delay,TimeUnit unit) public &lt;V&gt; ScheduledFuture&lt;V&gt; schedule(Callable&lt;V&gt; callable,long delay, TimeUnit unit) public ScheduledFuture&lt;?&gt; scheduleAtFixedRate(Runnable command,long initialDelay,long period, TimeUnit unit) public ScheduledFuture&lt;?&gt; scheduleWithFixedDelay(Runnable command,long initialDelay, long delay,TimeUnit unit) public Future&lt;?&gt; submit(Runnable task)public &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result)public &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task) 实现原理ScheduledThreadPoolExecutor调度和执行任务的过程可以抽象如下图所示： 将Callable或Runnable对象转换ScheduledFutureTask对象； 将转换后的ScheduledFutureTask对象添加到延迟队列并开启线程执行任务； 工作线程从队列获取ScheduledFutureTask任务执行任务。 通过上述描述可以发现，ScheduledFutureTask是ScheduledThreadPoolExecutor实现的关键。 源码分析1234567891011121314151617181920212223242526272829303132333435363738394041public ScheduledFuture&lt;?&gt; schedule(Runnable command, long delay, TimeUnit unit) { // 步骤1 RunnableScheduledFuture&lt;?&gt; t = decorateTask(command, new ScheduledFutureTask&lt;Void&gt;(command, null, triggerTime(delay, unit))); // 步骤2 delayedExecute(t); return t;}public ScheduledFuture&lt;?&gt; scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit) { ScheduledFutureTask&lt;Void&gt; sft = new ScheduledFutureTask&lt;Void&gt;(command, null, triggerTime(initialDelay, unit), unit.toNanos(period)); RunnableScheduledFuture&lt;Void&gt; t = decorateTask(command, sft); sft.outerTask = t; delayedExecute(t); return t;}// 将任务添加到延迟队列private void delayedExecute(RunnableScheduledFuture&lt;?&gt; task) { if (isShutdown()) reject(task); else { super.getQueue().add(task); if (isShutdown() &amp;&amp; !canRunInCurrentRunState(task.isPeriodic()) &amp;&amp; remove(task)) task.cancel(false); else ensurePrestart(); }} ScheduledFutureTask类层次结构 构造方法123456789101112131415161718192021ScheduledFutureTask(Runnable r, V result, long ns) { super(r, result); this.time = ns; this.period = 0; // sequenceNumber：当执行时间相同时，按照序号排序。 this.sequenceNumber = sequencer.getAndIncrement();}ScheduledFutureTask(Runnable r, V result, long ns, long period) { super(r, result); this.time = ns; this.period = period; this.sequenceNumber = sequencer.getAndIncrement();}ScheduledFutureTask(Callable&lt;V&gt; callable, long ns) { super(callable); this.time = ns; this.period = 0; this.sequenceNumber = sequencer.getAndIncrement();} 任务执行1234567891011121314151617181920212223public void run() { boolean periodic = isPeriodic(); if (!canRunInCurrentRunState(periodic)) cancel(false); else if (!periodic) // 非周期性任务，直接执行 ScheduledFutureTask.super.run(); else if (ScheduledFutureTask.super.runAndReset()) { // 执行任务，重置状态 // 计算下一次执行时间 setNextRunTime(); // 重新添加到延迟队列 reExecutePeriodic(outerTask); }}void reExecutePeriodic(RunnableScheduledFuture&lt;?&gt; task) { if (canRunInCurrentRunState(true)) { super.getQueue().add(task); if (!canRunInCurrentRunState(true) &amp;&amp; remove(task)) task.cancel(false); else ensurePrestart(); }}","link":"/posts/57239.html"},{"title":"Java并发编程：ThreadLocal","text":"原文地址：http://cmsblogs.com/?p=2442 ThreadLocal介绍ThreadLocal提供了一种解决多线程环境下成员变量的问题，但是它并不是解决多线程共享变量的问题。那么ThreadLocal到底是什么呢？ API是这样介绍的：This class provides thread-local variables. These variables differ from their normal counterparts in that each thread that accesses one (via its get or set method) has its own, independently initialized copy of the variable. ThreadLocal instances are typically private static fields in classes that wish to associate state with a thread (e.g., a user ID or Transaction ID). 该类提供了线程局部（thread-local）变量。这些变量不同于普通对应物，因为访问某个变量（通过其get或set方法）的每个线程都有自己的局部变量，它独立于变量的初始化副本。ThreadLocal实例通常是类中的private static字段，它们希望将状态与某一个线程（例如，用户ID或事务ID）相关联。 ThreadLocal与线程同步机制不同，线程同步机制是多个线程共享同一个变量，而ThreadLocal为了每一个线程创建一个单独的变量副本，故而每个线程都可以独立地改变自己所拥有的变量副本，而不会影响其他线程所对应的副本。 ThreadLocal使用示例，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class SeqCount { private static ThreadLocal&lt;Integer&gt; seqCount = new ThreadLocal&lt;Integer&gt;() { // 实现initialValue() public Integer initialValue() { return 0; } }; public int nextSeq() { seqCount.set(seqCount.get() + 1); return seqCount.get(); } public static void main(String[] args) { SeqCount seqCount = new SeqCount(); SeqThread thread1 = new SeqThread(seqCount); SeqThread thread2 = new SeqThread(seqCount); SeqThread thread3 = new SeqThread(seqCount); SeqThread thread4 = new SeqThread(seqCount); thread1.start(); thread2.start(); thread3.start(); thread4.start(); } private static class SeqThread extends Thread { private SeqCount seqCount; SeqThread(SeqCount seqCount) { this.seqCount = seqCount; } public void run() { for (int i = 0; i &lt; 3; i++) { System.out.println(Thread.currentThread().getName() + &quot; seqCount :&quot; + seqCount.nextSeq()); } } }} ThreadLocal实现原理 ThreadLocal的实现是这样的：每个Thread维护一个ThreadLocalMap映射表，这个映射表的key是 ThreadLocal实例本身，value是真正需要存储的Object。 也就是说ThreadLocal本身并不存储值，它只是作为一个key来让线程从ThreadLocalMap获取 value。值得注意的是图中的虚线，表示ThreadLocalMap是使用ThreadLocal的弱引用作为Key的，弱引用的对象在GC时会被回收。 ThreadLocal源码分析ThreadLocalMapThreadLocalMap的构造函数如下： 1234567ThreadLocalMap(ThreadLocal&lt;?&gt; firstKey, Object firstValue) { table = new Entry[INITIAL_CAPACITY]; int i = firstKey.threadLocalHashCode &amp; (INITIAL_CAPACITY - 1); table[i] = new Entry(firstKey, firstValue); size = 1; setThreshold(INITIAL_CAPACITY);} 由上可知，ThreadLocalMap其内部利用Entry来实现key-value的存储，如下： 123456789static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; { /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) { super(k); value = v; }} 可以看出Entry的key就是ThreadLocal，而value就是值。同时，Entry也继承WeakReference，所以说Entry所对应key（ThreadLocal实例）的引用为一个弱引用。 接下来，看看ThreadLocalMap最核心的方法set(ThreadLocal&gt; key, Object value)、getEntry()方法。 1、set(ThreadLocal&lt;?&gt; key, Object value) 1234567891011121314151617181920212223242526272829303132private void set(ThreadLocal&lt;?&gt; key, Object value) { Entry[] tab = table; int len = tab.length; // 根据 ThreadLocal 的散列值，查找对应元素在数组中的位置 int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) { ThreadLocal&lt;?&gt; k = e.get(); // key 存在，直接覆盖 if (k == key) { e.value = value; return; } // key == null，但是存在值（因为此处的e != null），说明之前的ThreadLocal对象已经被回收 if (k == null) { // 用新元素替换陈旧的元素 replaceStaleEntry(key, value, i); return; } } // ThreadLocal对应的key实例不存在则创建 tab[i] = new Entry(key, value); int sz = ++size; // cleanSomeSlots 清楚陈旧的Entry（key == null） // 如果没有清理陈旧的 Entry 并且数组中的元素大于了阈值，则进行 rehash if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) rehash(); } set()操作除了存储元素外，还有一个很重要的作用，就是replaceStaleEntry()和cleanSomeSlots()，这两个方法可以清除掉key == null 的实例，防止内存泄漏。在set()方法中还有一个变量很重要：threadLocalHashCode，定义如下： 1private final int threadLocalHashCode = nextHashCode(); 从名字上面我们可以看出threadLocalHashCode应该是ThreadLocal的散列值，定义为final，表示ThreadLocal一旦创建其散列值就已经确定了，生成过程则是调用nextHashCode()： 1234567private static AtomicInteger nextHashCode = new AtomicInteger();private static final int HASH_INCREMENT = 0x61c88647;private static int nextHashCode() { return nextHashCode.getAndAdd(HASH_INCREMENT);} nextHashCode表示分配下一个ThreadLocal实例的threadLocalHashCode的值，HASH_INCREMENT则表示分配两个ThradLocal实例的threadLocalHashCode的增量。 2、getEntry() 12345678private Entry getEntry(ThreadLocal&lt;?&gt; key) { int i = key.threadLocalHashCode &amp; (table.length - 1); Entry e = table[i]; if (e != null &amp;&amp; e.get() == key) return e; else return getEntryAfterMiss(key, i, e);} 采用了开放定址法，所以当前key的散列值和元素在数组的索引并不是完全对应的，首先取一个探测数（key的散列值），如果所对应的key就是我们所要找的元素，则返回，否则调用getEntryAfterMiss()。 123456789101112131415161718private Entry getEntryAfterMiss(ThreadLocal&lt;?&gt; key, int i, Entry e) { Entry[] tab = table; int len = tab.length; while (e != null) { ThreadLocal&lt;?&gt; k = e.get(); if (k == key) return e; // 当key == null时，调用了expungeStaleEntry()方法，该方法用于处理key == null， // 有利于GC回收，能够有效地避免内存泄漏。 if (k == null) expungeStaleEntry(i); else i = nextIndex(i, len); e = tab[i]; } return null; } ThreadLocal核心方法set(T value)：设置当前线程的线程局部变量的值123456789101112public void set(T value) { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value);}void createMap(Thread t, T firstValue) { t.threadLocals = new ThreadLocalMap(this, firstValue);} 获取当前线程所对应的ThreadLocalMap，如果不为空，则调用ThreadLocalMap的set()方法，key就是当前ThreadLocal，如果不存在，则调用createMap()方法创建。 get()：返回当前线程所对应的线程变量1234567891011121314public T get() { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) { ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) { @SuppressWarnings(&quot;unchecked&quot;) T result = (T)e.value; return result; } } // 如果ThreadLocalMap不存在，返回初始值。 return setInitialValue();} 首先通过当前线程获取所对应的成员变量ThreadLocalMap，然后通过ThreadLocalMap获取当前ThreadLocal的Entry，最后通过所获取的Entry获取目标值result。 initialValue()：返回该线程局部变量的初始值123protected T initialValue() { return null;} 这个方法将在一个线程第一次使用get方法访问变量时被调用，除非线程先前调用了set方法，在这种情况下，线程不会调用initialValue方法。通常情况下，每个线程最多调用一次此方法，但在后续调用remove和get时，可能会再次调用此方法。 默认实现返回null，如果程序员希望线程局部变量具有非null的初始值，则必须对ThreadLocal进行子类化，并重写此方法。 remove()：将当前线程局部变量的值删除12345public void remove() { ThreadLocalMap m = getMap(Thread.currentThread()); if (m != null) m.remove(this);} 该方法的目的是减少内存的占用。当然，我们不需要显示调用该方法，因为一个线程结束后，它所对应的局部变量就会被垃圾回收。 ThreadLocal为什么会内存泄漏ThreadLocalMap使用ThreadLocal的弱引用作为key，如果一个ThreadLocal没有外部强引用来引用它，那么系统GC的时候，这个ThreadLocal势必会被回收，ThreadLocalMap中就会出现key为null的Entry，就没有办法访问这些key为null的Entry的value。如果当前线程再迟迟不结束的话，这些key为null的Entry的value就会一直存在一条强引用链：Thread Ref -&gt; Thread -&gt; ThreaLocalMap -&gt; Entry -&gt; value永远无法回收，造成内存泄漏。 其实，ThreadLocalMap的设计中已经考虑到这种情况，也加上了一些防护措施：在ThreadLocal的get(),set(),remove()的时候都会清除线程ThreadLocalMap里所有key为null的value。 但是这些被动的预防措施并不能保证不会内存泄漏： 使用static的ThreadLocal，延长了ThreadLocal的生命周期，可能导致的内存泄漏。 分配使用了ThreadLocal又不再调用get(),set(),remove()方法，那么就会导致内存泄漏。 ThreadLocal内存泄漏的根源是：由于ThreadLocalMap的生命周期跟Thread一样长，如果没有手动删除对应key就会导致内存泄漏，而不是因为弱引用。 理解了ThreadLocal内存泄漏的前因后果，那么怎么避免内存泄漏呢？ 每次使用完ThreadLocal，都调用它的remove()方法，清除数据。 在使用线程池的情况下，没有及时清理ThreadLocal，不仅是内存泄漏的问题，更严重的是可能导致业务逻辑出现问题。所以，使用ThreadLocal就跟加锁完要解锁一样，用完就清理。 参考资料【死磕Java并发】—–深入分析ThreadLocal 深入分析 ThreadLocal 内存泄漏问题","link":"/posts/35904.html"},{"title":"Java并发编程：并发编程的挑战","text":"并发编程的目的是为了让程序运行得更快，但并不是启动更多的线程就能让程序最大限度地并发执行。在进行并发编程时，如果希望通过过线程执行任务让程序运行得更快，会面临非常多的挑战，比如上下文切换的问题、死锁的问题，以及受限于硬件和软件的资源限制问题。 上下文切换即使是单核处理器也支持多线程执行代码，CPU通过给每个线程分配CPU时间片来实现这个机制。时间片是CPU分配给各个线程的时间，因为时间片非常短，所以CPU通过不停地切换线程执行，让我们感觉多个线程是同时执行的，时间片一般是几十毫秒(ms)。 CPU通过时间片分配算法来循环执行任务，当前任务执行一个时间片后会切换到下一个任务。但是，在切换前会保存上一个任务的状态，以便下次切换回这个任务时，可以再加载这个任务的状态，所以任务从保存到再加载的过程就是一次上下文切换。 上下文切换会影响多线程的执行速度，减少上下文切换的方法有无锁并发编程、CAS算法、使用最少线程和使用协程。 无锁并发编程 多线程竞争锁时，会引起上下文切换，所以多线程处理数据时，可以用一些办法来避免使用锁，如将数据的ID按照Hash算法取模分段，不同的线程处理不同段的数据。 CAS算法 Java的Atomic包使用CAS算法来更新数据，而不需要加锁。 使用最少线程 避免创建不需要的线程，比如任务很少，但是创建了很多线程来处理，这样会造成大量线程都处于等待状态。 协程 在单线程里实现多任务的调度，并在单线程里维持多个任务间的切换。 死锁锁是个非常有用的工具，运用场景非常多，因为它使用起来非常简单，而且易于理解。但同时它也会带来一些困扰，那就是可能会引起死锁，一旦产生死锁，就会造成系统功能不可用。 死锁代码示例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class DeadLock{ private Object lockOne = new Object(); private Object lockTwo = new Object(); public void methodOne() { synchronized (lockOne){ try { Thread.sleep(5000l); } catch (InterruptedException e) { // } methodTwo(); } } public void methodTwo() { synchronized (lockTwo){ try { Thread.sleep(5000l); } catch (InterruptedException e) { } methodOne(); } } public static void main(String[] args) { DeadLock deadLock = new DeadLock(); new Thread(new Runnable() { @Override public void run() { deadLock.methodOne(); } }).start(); new Thread(new Runnable() { @Override public void run() { deadLock.methodTwo(); } }).start(); }} 一旦出现死锁，业务是可感知的，因为不能继续提供服务了，那么只能通过dump线程查看到底是哪个线程出现了问题。避免死锁的几个常见方法： 避免一个线程同时获取多个锁。 避免一个线程在锁内同时占用多个资源，尽量保证每个锁只占用一个资源。 尝试使用定时锁，使用lock.tryLock(timeout)来替代使用内部锁机制。 对于数据库锁，加锁和解锁必须在一个数据库连接里，否则会出现解锁失败的情况。 资源限制的挑战 什么是资源限制 资源限制是指在进行并发编程时，程序的执行速度受限于计算机硬件资源或软件资源。例如，服务器的带宽只有2Mb/s，某个资源的下载速度是1Mb/s每秒，系统启动10个线程下载资源，下载速度不会变成10Mb/s，所以在进行并发编程时，要考虑这些资源的限制。硬件资源限制有带宽的上传/下载速度、硬盘读写速度和CPU的处理速度。软件资源限制有数据库的连接数和socket连接数等。 资源限制引发的问题 在并发编程中，将代码执行速度加快的原则是将代码中串行执行的部分变成并发执行，但是如果将某段串行的代码并发执行，因为受限于资源，仍然在串行执行，这时候程序不仅不会加快执行，反而会更慢，因为增加了上下文切换和资源调度的时间。 如何解决资源限制的问题 对于硬件资源限制，可以考虑使用集群并行执行程序。既然单机的资源有限制，那么就让程序在多机上运行。比如使用ODPS、Hadoop或者自己搭建服务器集群，不同的机器处理不同的数据。可以通过“数据ID%机器数”，计算得到一个机器编号，然后由对应编号的机器处理这笔数据。对于软件资源限制，可以考虑使用资源池将资源复用。比如使用连接池将数据库和Socket连接复用，或者在调用对方webservice接口获取数据时，只建立一个连接。 在资源限制情况下进行并发编程 如何在资源限制的情况下，让程序执行得更快呢？ 方法就是，根据不同的资源限制调整程序的并发度，比如下载文件程序依赖于两个资源——带宽和硬盘读写速度。有数据库操作时，涉及数据库连接数，如果SQL语句执行非常快，而线程的数量比数据库连接数大很多，则某些线程会被阻塞，等待数据库连接。 支持原子操作的实现原理 原子本意是“不能被进一步分割的最小粒子”，原子操作意为“不可被中断的一个或一系列操作”。 CPU术语定义 CAS实现原子操作的三大问题: CAS虽然很高效地解决了原子操作，但是C仍然存在三大问题。ABA问题，循环时间开销大，以及只能保证一个共享变量的原子操作。 ABA问题 CAS需要在操作值的时候，检查值有没有发生变化，如果没有发生变化则更新，但是如果一个值原来是A，变成了B，又变成了A，那么使用CAS进行检查时会发现它的值没有变化，但是实际上却变化了。 ABA问题的解决思路就是使用版本号。在变量前面追加上版本号，每次变量更新的时候把版本号加1，那么A—B—A就会变成1A—2B—3A。从Java 1.5开始，JDK的Atomic包里提供了一个类AtomicStampedReference来解决ABA问题。 循环时间长开销大 自旋CAS如果长时间不成功,会给CPU带来非常大的执行开销。如果JVM能支持处理器提供的pause指令,那么效率会有一定的提升。pause指令有两个作用:第一,它可以延迟流水线执行指令(de-pipeline),使CPU不会消耗过多的执行资源,延迟的时间取决于具体实现的版本,在一些处理器上延迟时间是零;第二,它可以避免在退出循环的时候因内存顺序冲突(Memory Order Violation)而引起CPU流水线被清空(CPU Pipeline Flush),从而提高CPU的执行效率。 只能保证一个共享变量的原子操作 当对一个共享变量执行操作时,我们可以使用循环CAS的方式来保证原子操作,但是对多个共享变量操作时,循环CAS就无法保证操作的原子性,这个时候就可以用锁。还有一个取巧的办法,就是把多个共享变量合并成一个共享变量来操作。比如,有两个共享变量i=2,j=a,合并一下ij=2a,然后用CAS来操作ij。从Java 1.5开始,JDK提供了AtomicReference类来保证引用对象之间的原子性,就可以把多个变量放在一个对象里来进行CAS操作。","link":"/posts/42947.html"},{"title":"MySQL：事务与锁机制","text":"事务概述什么是事务？事务具有哪些特性？ 简单来说，事务是指作为单个逻辑工作单元执行的一系列操作，这些操作要么全做，要么全不做，是一个不可分割的工作单元。 一个逻辑工作单元要成为事务，在关系型数据库管理系统中，必须满足 4 个特性，即所谓的 ACID：原子性、一致性、隔离性和持久性。 一致性：事务开始之前和事务结束之后，数据库的完整性限制未被破坏。 原子性：事务的所有操作，要么全部完成，要么全部不完成，不会结束在某个中间环节。 持久性：事务完成之后，事务所做的修改进行持久化保存，不会丢失。 隔离性：当多个事务并发访问数据库中的同一数据时，所表现出来的相互关系。 事务特性 一致性一致性其实包括两部分内容，分别是约束一致性和数据一致性。 约束一致性：大家应该很容易想到数据库中创建表结构时所指定的外键、Check、唯一索引等约束。可惜在 MySQL 中，是不支持 Check 的，只支持另外两种，所以约束一致性就非常容易理解了。 数据一致性：是一个综合性的规定，或者说是一个把握全局的规定。因为它是由原子性、持久性、隔离性共同保证的结果，而不是单单依赖于某一种技术。 原子性原子性就是前面提到的两个“要么”，即要么改了，要么没改。也就是说用户感受不到一个正在改的状态。MySQL 是通过 WAL（Write Ahead Log）技术来实现这种效果的。 原子性和 WAL 到底有什么关系呢？举例说明，如果事务提交了，那改了的数据就生效了，如果此时 Buffer Pool 的脏页没有刷盘，如何来保证改了的数据生效呢？就需要使用 Redo 日志恢复出来的数据。而如果事务没有提交，且 Buffer Pool 的脏页被刷盘了，那这个本不应该存在的数据如何消失呢？就需要通过 Undo 来实现了，==Undo 又是通过 Redo 来保证的==，所以最终原子性的保证还是靠 Redo 的 WAL 机制实现的。 持久性持久性是如何保证的呢？一旦事务提交，通过原子性，即便是遇到宕机，也可以从逻辑上将数据找回来后再次写入物理存储空间，这样就从逻辑和物理两个方面保证了数据不会丢失，即保证了数据库的持久性。 隔离性所谓隔离性，指的是一个事务的执行不能被其他事务干扰，即一个事务内部的操作及使用的数据对其他的并发事务是隔离的。锁和多版本控制就符合隔离性。 InnoDB 支持的隔离性有 4 种，隔离性从低到高分别为：读未提交、读提交、可重复读、可串行化。 读未提交（RU，Read Uncommitted）。它能读到其他一个事务的中间过程，违背了 ACID 特性，存在脏读的问题，所以基本不会用到，可以忽略。 读提交（RC，Read Committed）。它表示如果其他事务已经提交，那么我们就可以看到，这也是一种最普遍适用的级别。但由于一些历史原因，可能 RC 在生产环境中用的并不多。 可重复读（RR，Repeatable Read），是目前被使用得最多的一种级别。其特点是有 Gap 锁、目前还是默认的级别、在这种级别下会经常发生死锁、低并发等问题。 可串行化，这种实现方式，其实已经并不是多版本了，又回到了单版本的状态，因为它所有的实现都是通过锁来实现的。 并发事务控制单版本控制-锁锁用独占的方式来保证在只有一个版本的情况下事务之间相互隔离，所以锁可以理解为单版本控制。 在 MySQL 事务中，锁的实现与隔离级别有关系，在 RR（Repeatable Read）隔离级别下，MySQL 为了解决幻读的问题，以牺牲并行度为代价，通过 Gap 锁来防止数据的写入，而这种锁，因为其并行度不够，冲突很多，经常会引起死锁。 现在流行的 Row 模式可以避免很多冲突甚至死锁问题，所以推荐默认使用 Row + RC（Read Committed）模式的隔离级别，可以很大程度上提高数据库的读写并行度。 多版本控制-MVCC多版本控制也叫作 MVCC，是指在数据库中，为了实现高并发的数据访问，对数据进行多版本处理，并通过事务的可见性来保证事务能看到自己应该看到的数据版本。 那个多版本是如何生成的呢？每一次对数据库的修改，都会在 Undo 日志中记录当前修改记录的事务号及修改前数据状态的存储地址（即 ROLL_PTR），以便在必要的时候可以回滚到老的数据版本。例如，一个读事务查询到当前记录，而最新的事务还未提交，根据原子性，读事务看不到最新数据，但可以去回滚段中找到老版本的数据，这样就生成了多个版本。 多版本控制很巧妙地将稀缺资源的独占互斥转换为并发，大大提高了数据库的吞吐量及读写性能。 MVCC实现原理MySQL InnoDB 存储引擎，实现的是基于多版本的并发控制协议——MVCC，而不是基于锁的并发控制。 MVCC 最大的好处是读不加锁，读写不冲突。在读多写少的 OLTP（On-Line Transaction Processing）应用中，读写不冲突是非常重要的，极大的提高了系统的并发性能，这也是为什么现阶段几乎所有的 RDBMS（Relational Database Management System），都支持 MVCC 的原因。 快照读与当前读在 MVCC 并发控制中，读操作可以分为两类: 快照读（Snapshot Read）与当前读 （Current Read）。 快照读：读取的是记录的可见版本（有可能是历史版本），不用加锁。 当前读：读取的是记录的最新版本，并且当前读返回的记录，都会加锁，保证其他事务不会再并发修改这条记录。 ==注意：MVCC 只在 Read Commited 和 Repeatable Read 两种隔离级别下工作。== 如何区分快照读和当前读呢？ 可以简单的理解为： 快照读：简单的 select 操作，属于快照读，不需要加锁。 当前读：特殊的读操作，插入/更新/删除操作，属于当前读，需要加锁。 MVCC多版本实现为了让大家更直观地理解 MVCC 的实现原理，这里举一个“事务对某行记录更新的过程”的案例来讲解 MVCC 中多版本的实现。 假设 F1～F6 是表中字段的名字，1～6 是其对应的数据。后面三个隐含字段分别对应该行的隐含ID、事务号和回滚指针，如下图所示。 隐含 ID（DB_ROW_ID），6 个字节，当由 InnoDB 自动产生聚集索引时，聚集索引包括这个 DB_ROW_ID 的值。 事务号（DB_TRX_ID），6 个字节，标记了最新更新这条行记录的 Transaction ID，每处理一个事务，其值自动 +1。 回滚指针（DB_ROLL_PT），7 个字节，指向当前记录项的 Rollback Segment 的 Undo log记录，通过这个指针才能查找之前版本的数据。 具体的更新过程，简单描述如下。 首先，假如这条数据是刚 INSERT 的，可以认为 ID 为 1，其他两个字段为空。 然后，当事务 1 更改该行的数据值时，会进行如下操作，如下图所示。 ​ 用排他锁锁定该行；记录 Redo log； 把该行修改前的值复制到 Undo log，即图中下面的行； 修改当前行的值，填写事务编号，使回滚指针指向 Undo log 中修改前的行。 接下来，与事务 1 相同，此时 Undo log 中有两行记录，并且通过回滚指针连在一起。因此，如果 Undo log 一直不删除，则会通过当前记录的回滚指针回溯到该行创建时的初始内容，所幸的是在 InnoDB 中存在 purge 线程，它会查询那些比现在最老的活动事务还早的 Undo log，并删除它们，从而保证 Undo log 文件不会无限增长。 并发事务问题及解决方案并发事务问题脏读一个事务正在对一条记录做修改，在这个事务完成并提交前，这条记录的数据就处于不一致状态；这时，另一个事务也来读取同一条记录，如果不加控制，第二个事务读取了这些“脏”数据，并据此做进一步的处理，就会产生未提交的数据依赖关系。这种现象被形象的叫作”脏读”（Dirty Reads）。 不可重复读一个事务在读取某些数据后的某个时间，再次读取以前读过的数据，却发现其==读出的数据已经发生了改变、或某些记录已经被删除了==！这种现象就叫作“ 不可重复读”（Non-Repeatable Reads）。 幻读一个事务按相同的查询条件重新读取以前检索过的数据，却发现其他事务插入了满足其查询条件的新数据，这种现象就称为“幻读”（Phantom Reads）。 解决方案产生的这些问题，MySQL 数据库是通过事务隔离级别来解决的。要求大家能够记住这种关系的矩阵表；记住各种事务隔离级别及各自都解决了什么问题，如下图所示。 锁概述MySQL锁分类在 MySQL 中有三种级别的锁：页级锁、表级锁、行级锁。 表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低。 会发生在：MyISAM、memory、InnoDB、BDB 等存储引擎中。 行级锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度最高。会发生在：InnoDB 存储引擎。 页级锁：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。会发生在：BDB 存储引擎。 三种级别的锁分别对应存储引擎关系如下图所示。 注意：MySQL 中的表锁包括读锁和写锁。只需记住这个表锁模式兼容矩阵即可。 InnoDB中的锁在 MySQL InnoDB 存储引擎中，锁分为行锁和表锁。其中行锁包括两种锁。 共享锁（S）：多个事务可以一起读，共享锁之间不互斥，共享锁会阻塞排它锁。 排他锁（X）：允许获得排他锁的事务更新数据，阻止其他事务取得相同数据集的共享读锁和排他写锁。 另外，为了允许行锁和表锁共存，实现多粒度锁机制，InnoDB 还有两种内部使用的意向锁（Intention Locks），这两种意向锁都是表锁。表锁又分为三种。 意向共享锁（IS）：事务在给一个数据行加共享锁前必须先取得该表的 IS 锁。 意向排他锁（IX）：事务在给一个数据行加排他锁前必须先取得该表的 IX 锁。 自增锁（AUTO-INC Locks）：特殊表锁，自增长计数器通过该“锁”来获得子增长计数器最大的计数值。 在加行锁之前必须先获得表级意向锁，否则等待 innodb_lock_wait_timeout 超时后根据innodb_rollback_on_timeout 决定是否回滚事务。 InnoDB行锁InnoDB 行锁是通过对索引数据页上的记录（record）加锁实现的。主要实现算法有 3 种：Record Lock、Gap Lock 和 Next-key Lock。 Record Lock 锁：单个行记录的锁（锁数据，不锁 Gap）。 Gap Lock 锁：间隙锁，锁定一个范围，不包括记录本身（不锁数据，仅仅锁数据前面的Gap）。 Next-key Lock 锁：同时锁住数据，并且锁住数据前面的 Gap。 排查 InnoDB 锁问题排查 InnoDB 锁问题通常有 2 种方法。 打开 innodb_lock_monitor 表，注意使用后记得关闭，否则会影响性能。 在 MySQL 5.5 版本之后，可以通过查看 information_schema 库下面的 innodb_locks、innodb_lock_waits、innodb_trx 三个视图排查 InnoDB 的锁问题。 InnoDB加锁行为举一些例子分析 InnoDB 不同索引的加锁行为。分析锁时需要跟隔离级别联系起来，我们以 RR 为例，主要是从四个场景分析。 主键 + RR 唯一键 + RR 非唯一键 + RR 无索引 + RR 主键 + RR 假设条件是： update t1 set name=‘XX’ where id=10。 id 为主键索引。 加锁行为： 仅在 id=10 的主键索引记录上加 X锁。 唯一键 + RR 假设条件是： update t1 set name=‘XX’ where id=10。 id 为唯一索引。 加锁行为： 先在唯一索引 id 上加 id=10 的 X 锁。 再在 id=10 的主键索引记录上加 X 锁，若 id=10 记录不存在，那么加间隙锁。 非唯一键 + RR 假设条件是： update t1 set name=‘XX’ where id=10。 id 为非唯一索引。 加锁行为： 先通过 id=10 在 key(id) 上定位到第一个满足的记录，对该记录加 X 锁，而且要在 (6,c)~(10,b) 之间加上 Gap lock，为了防止幻读。然后在主键索引 name 上加对应记录的X 锁； 再通过 id=10 在 key(id) 上定位到第二个满足的记录，对该记录加 X 锁，而且要在(10,b)~(10,d)之间加上 Gap lock，为了防止幻读。然后在主键索引 name 上加对应记录的X 锁； 最后直到 id=11 发现没有满足的记录了，此时不需要加 X 锁，但要再加一个 Gap lock： (10,d)~(11,f)。 无索引 + RR 假设条件是： update t1 set name=‘XX’ where id=10。 id 列无索引。 加锁行为： 表里所有行和间隙均加 X 锁。","link":"/posts/12078.html"},{"title":"MySQL：索引","text":"索引概述索引是存储引擎用于快速找到记录的一种数据结构。举例说明：如果查找一本书中的某个特定主题，一般会先看书的目录（类似索引），找到对应页面。在MySQL，同样，对于数据库的表而言，索引其实就是它的“目录”。 索引的常见模型索引的出现是为了提高查询效率，但是实现索引的方式却有很多种，所以这里也就引入了索 引模型的概念。 哈希表哈希表是一种以键 - 值（key-value）存储数据的结构，只要输入待查找的值即 key， 就可以找到其对应的值即 Value。哈希的思路很简单，把值放在数组里，用一个哈希函数把 key 换算成一个确定的位置，然后把 value 放在数组的这个位置。 不可避免地，多个 key 值经过哈希函数的换算，会出现同一个值的情况。处理这种情况的 一种方法是，拉出一个链表。 假设，你现在维护着一个身份证信息和姓名的表，需要根据身份证号查找对应的名字，这时 对应的哈希索引的示意图如下所示： 如果你现在要找身份证号在 [ID_card_X, ID_card_Y] 这个区间的所有用 户，就必须全部扫描一遍了。 所以，哈希表这种结构适用于只有等值查询的场景，比如 Memcached 及其他一些 NoSQL 引擎。 有序数组有序数组在等值查询和范围查询场景中的性能就都非常优秀。还是上面这个根据身份证号查名字的例子，如果我们使用有序数组来实现的话，示意图如下所示： 这个数组就是按照身份证号递增的顺序保存的。这时候如 果你要查 ID_card_n2 对应的名字，用二分法就可以快速得到，这个时间复杂度是 O(log(N))。 同时很显然，这个索引结构支持范围查询。你要查身份证号在 [ID_card_X, ID_card_Y] 区 间的 User，可以先用二分法找到 ID_card_X（如果不存在 ID_card_X，就找到大于 ID_card_X 的第一个 User），然后向右遍历，直到查到第一个大于 ID_card_Y 的身份证 号，退出循环。 如果仅仅看查询效率，有序数组就是最好的数据结构了。但是，在需要更新数据的时候就麻烦了，你往中间插入一个记录就必须得挪动后面所有的记录，成本太高。 所以，有序数组索引只适用于静态存储引擎，比如你要保存的是 2017 年某个城市的所有人 口信息，这类不会再修改的数据。 二叉树二叉搜索树也是课本里的经典数据结构了。还是上面根据身份证号查名字的例子，如果我们 用二叉搜索树来实现的话，示意图如下所示： 二叉搜索树的特点是：每个节点的左儿子小于父节点，父节点又小于右儿子。这样如果你要 查 ID_card_n2 的话，按照图中的搜索顺序就是按照 UserA -&gt; UserC -&gt; UserF -&gt; User2 这个路径得到。这个时间复杂度是 O(log(N))。 当然为了维持 O(log(N)) 的查询复杂度，你就需要保持这棵树是平衡二叉树。为了做这个 保证，更新的时间复杂度也是 O(log(N))。 二叉树是搜索效率最高的，但是实际上大多数的数据库存储却并不使用二叉树。 其原因是，索引不止存在内存中，还要写到磁盘上。 N叉树你可以想象一下一棵 100 万节点的平衡二叉树，树高 20。一次查询可能需要访问 20 个数 据块。在机械硬盘时代，从磁盘随机读一个数据块需要 10 ms 左右的寻址时间。也就是 说，对于一个100万行的表，如果使用二叉树来存储，单独访问一个行可能需要 20个10 ms的时间，这个查询可真够慢的。 为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。那么，我们就不 应该使用二叉树，而是要使用“N 叉”树。这里，“N 叉”树中的“N”取决于数据块的大小。 以 InnoDB 的一个整数字段索引为例，这个N差不多是1200。这棵树高是4的时候，就 可以存 1200 的 3 次方个值，这已经 17 亿了。考虑到树根的数据块总是在内存中的，一个 10 亿行的表上一个整数字段的索引，查找一个值最多只需要访问 3 次磁盘。其实，树的第 二层也有很大概率在内存中，那么访问磁盘的平均次数就更少了。 N叉树由于在读写上的性能优点，以及适配磁盘的访问模式，已经被广泛应用在数据库引擎中了。 小结管是哈希还是有序数组，或者 N 叉树，它们都是不断迭代、不断优化的产物或者解决方案。数据库底层存储的核心就是基于这些数据模型的。每碰到一个新数据 库，我们需要先关注它的数据模型，这样才能从理论上分析出这个数据库的适用场景。 InnoDB的索引模型在 InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。InnoDB使用了B+ 树索引模型，所以数据都是存储在 B+ 树中的。 假设，我们有一个主键列为 ID 的表，表中有字段 k，并且在 k 上有索引。 123456create table T( id int primary key, k int not null, name varchar(16), index (k)) engine=InnoDB; 表中 R1~R5 的 (ID,k) 值分别为 (100,1)、(200,2)、(300,3)、(500,5) 和 (600,6)，两棵树 的示例示意图如下。 主键索引&amp;非主键索引根据叶子节点的内容，索引类型分为主键索引和非主键索引。 主键索引的叶子节点存的是整行数据。在 InnoDB 里，主键索引也被称为聚簇索引 （clustered index）。 非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引 （secondary index）。 基于主键索引和普通索引的查询有什么区别？ 如果语句是select * from T where ID=500，即主键查询方式，则只需要搜索ID这棵B+树； 如果语句是 select * from T where k=5，即普通索引查询方式，则需要先搜索k索引树，得到ID的值为 500，再到ID索引树搜索一次。这个过程称为回表。 也就是说，基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。 索引维护B+ 树为了维护索引有序性，在插入新值的时候需要做必要的维护。以上面这个图为例，如 果插入新的行 ID 值为 700，则只需要在 R5 的记录后面插入一个新记录。如果新插入的 ID 值为 400，就相对麻烦了，需要逻辑上挪动后面的数据，空出位置。 而更糟的情况是，如果 R5 所在的数据页已经满了，根据 B+ 树的算法，这时候需要申请一 个新的数据页，然后挪动部分数据过去。这个过程称为页分裂。在这种情况下，性能自然会 受影响。 除了性能外，页分裂操作还影响数据页的利用率。原本放在一个页的数据，现在分到两个页中，整体空间利用率降低大约 50%。 当然有分裂就有合并。当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合 并。合并的过程，可以认为是分裂过程的逆过程。 自增主键基于上面的索引维护过程说明，我们来讨论一个案例： 你可能在一些建表规范里面见到过类似的描述，要求建表语句里一定要有自增主键。当然事无绝对，我们来分析一下哪些场景下应该使用自增主键，而哪些场景下不应该。 使用自增主键插入新记录的时候可以不指定 ID 的值，系统会获取当前 ID 最大值加 1 作为下一条记录的 ID 值。 自增主键的插入数据模式，正符合了我们前面提到的递增插入的场景。每次插入一条新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂。 而有业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高。 除了考虑性能外，我们还可以从存储空间的角度来看。假设你的表中确实有一个唯一字段， 比如字符串类型的身份证号，那应该用身份证号做主键，还是用自增字段做主键呢？ 由于每个非主键索引的叶子节点上都是主键的值。如果用身份证号做主键，那么每个二级索 引的叶子节点占用约 20 个字节，而如果用整型做主键，则只要 4 个字节，如果是长整型 （bigint）则是 8 个字节。 显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。 业务字段主键有没有什么场景适合用业务字段直接做主键的呢？还是有的。比如，有些业务的场景需求是 这样的： 只有一个索引； 该索引必须是唯一索引。 这就是典型的 KV 场景。 索引优化回表查询在下面这个表T中，如果我执行 select * from T where k between 3 and 5，需要执行几次树的搜索操作，会扫描多少行？ 1 现在，我们一起来看看这条 SQL 查询语句的执行流程： 在 k 索引树上找到 k=3 的记录，取得 ID = 300； 再到 ID 索引树查到 ID=300 对应的 R3； 在 k 索引树取下一个值 k=5，取得 ID=500； 再回到 ID 索引树查到 ID=500 对应的 R4； 在 k 索引树取下一个值 k=6，不满足条件，循环结束 这个过程中，回到主键索引树搜索的过程，我们称为回表。可以看到，这个查询过程读了k 索引树的3条记录（步骤 1、3 和 5），回表了两次（步骤 2 和 4）。 覆盖索引在这个例子中，由于查询结果所需要的数据只在主键索引上有，所以不得不回表。那么，有 没有可能经过索引优化，避免回表过程呢？ 如果执行的语句是select ID from T where k between 3 and 5，这时只需要查 ID 的值， 而 ID 的值已经在 k 索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，在 这个查询里面，索引 k 已经“覆盖了”我们的查询需求，我们称为覆盖索引。 由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的 性能优化手段。 需要注意的是，在引擎内部使用覆盖索引在索引 k 上其实读了三个记录，R3~R5（对应的索引k上的记录项），但是对于 MySQL 的 Server 层来说，它就是找引擎拿到了两条记 录，因此 MySQL认为扫描行数是2。 讨论一个问题：在一个市民信息表上，是否有必要将身份证号和名字建立联合索引？ 12345678910CREATE TABLE `tuser` ( `id` int(11) NOT NULL, `id_card` varchar(32) DEFAULT NULL, `name` varchar(32) DEFAULT NULL, `age` int(11) DEFAULT NULL, `ismale` tinyint(1) DEFAULT NULL, PRIMARY KEY (`id`), KEY `id_card` (`id_card`), KEY `name_age` (`name`,`age`)) ENGINE=InnoDB 如果有根据身份证号查询市民信息的需求，我们只要在身份证号字段上建立索引就够了。而再建立一个（身份证号、姓名）的联合索引，是不是浪费空间？ 如果现在有一个高频请求，要根据市民的身份证号查询他的姓名，这个联合索引就有意义 了。它可以在这个高频请求上用到覆盖索引，不再需要回表查整行记录，减少语句的执行时间。 当然，索引字段的维护总是有代价的。因此，在建立冗余索引来支持覆盖索引时就需要权衡考虑了。这正是业务 DBA，或者称为业务数据架构师的工作。 最左前缀原理如果为每一种查询都设计一个索引，索引是不是太多了。如果我现在要按照市民的身份证号去查他的家庭地址呢？虽然这个查询需求在业务中出现的概率不高，但总不能让它走全表扫描吧？反过来说，单独为一个不频繁的请求创建一个（身份证 号，地址）的索引又感觉有点浪费。应该怎么做呢？ B+ 树这种索引结构，可以利用索引的“最左前缀”，来定位记录。 为了直观地说明这个概念，我们用（name，age）这个联合索引来分析。 可以看到，索引项是按照索引定义里面出现的字段顺序排序的。 当你的逻辑需求是查到所有名字是“张三”的人时，可以快速定位到 ID4，然后向后遍历得 到所有需要的结果。 只要满足最左前缀，就可以利用索引来加速检索。这个 最左前缀可以是联合索引的最左N个字段，也可以是字符串索引的最左M个字符。 在建立联合索引的时候，如何安排索引内的字段顺序？ 索引的复用能力。因为可以支持最左前缀，所以当已经有了 (a,b) 这个联合索引后，一般就不需要单独在 a 上建立索引了。因此，第一原则是，如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。 如果既有联合查询，又有基于 a、b 各自的查询呢？查询条件里面只有 b 的语句， 是无法使用 (a,b) 这个联合索引的，这时候你不得不维护另外一个索引，也就是说你需要同 时维护 (a,b)、(b) 这两个索引。这时候，考虑的原则就是空间了。 索引下推满足最左前缀原则的时候，最左前缀可以用于在索引中定位记录。这时，你可能要问，那些不符合最左前缀的部分，会怎么样呢？ 以市民表的联合索引（name, age）为例。如果现在有一个需求：检索出表中“名 字第一个字是张，而且年龄是 10 岁的所有男孩”。那么，SQL 语句是这么写的： 1select * from tuser where name like '张 %' and age=10 and ismale=1; 已经知道了前缀索引规则，所以这个语句在搜索索引树的时候，只能用 “张”，找到第 一个满足条件的记录 ID3。当然，这还不错，总比全表扫描要好。 然后呢？ 当然是判断其他条件是否满足。 在 MySQL 5.6 之前，只能从 ID3 开始一个个回表。到主键索引上找出数据行，再对比字段值。 而 MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过 程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。 这个过程 InnoDB 并不会去看 age 的值，只是按顺序把“name 第一个字是’张’”的记录一条条取出来回表。因此， 需要回表 4 次。 InnoDB 在 (name,age) 索引内部就判断了 age 是否等于 10，对于 不等于 10 的记录，直接判断并跳过。在我们的这个例子中，只需要对 ID4、ID5 这两条记 录回表取数据判断，就只需要回表2次。","link":"/posts/22384.html"},{"title":"Mybatis：缓存","text":"MyBatis提供了一级缓存和二级缓存，其中一级缓存基于SqlSession实现，而二级缓存基于Mapper实现。 MyBatis一级缓存概述Mybatis一级缓存默认是开启的，而且不能关闭。至于一级缓存为什么不能关闭，MyBatis核心开发人员做出了解释：MyBatis的一些关键特性（例如通过和建立级联映射、避免循环引用（circular references）、加速重复嵌套查询等）都是基于MyBatis一级缓存实现的。 MyBatis提供了一个配置参数localCacheScope，用于控制一级缓存的级别，该参数的取值为SESSION、STATEMENT。 当指定localCacheScope参数值为SESSION时，缓存对整个SqlSession有效，只有执行DML语句（更新语句）时，缓存才会被清除。 当localCacheScope值为STATEMENT时，缓存仅对当前执行的语句有效，当语句执行完毕后，缓存就会被清空。 实现原理SqlSession提供了面向用户的API，但是真正执行SQL操作的是Executor组件。Executor采用模板方法设计模式，BaseExecutor类用于处理一些通用的逻辑，其中一级缓存相关的逻辑就是在BaseExecutor类中完成的 在BaseExecutor类中维护了两个PerpetualCache属性，代码如下： 12345678910111213141516public abstract class BaseExecutor implements Executor { // Mybatis一级缓存对象 protected PerpetualCache localCache; // 存储过程输出参数缓存 protected PerpetualCache localOutputParameterCache; protected BaseExecutor(Configuration configuration, Transaction transaction) { this.transaction = transaction; this.deferredLoads = new ConcurrentLinkedQueue&lt;&gt;(); this.localCache = new PerpetualCache(&quot;LocalCache&quot;); this.localOutputParameterCache = new PerpetualCache(&quot;LocalOutputParameterCache&quot;); this.closed = false; this.configuration = configuration; this.wrapper = this; }} BaseExecutor的query()方法相关的执行逻辑，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142public &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler) throws SQLException { BoundSql boundSql = ms.getBoundSql(parameter); // 构建缓存cacheKey CacheKey key = createCacheKey(ms, parameter, rowBounds, boundSql); // 执行查询 return query(ms, parameter, rowBounds, resultHandler, key, boundSql);}public &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException { // 是否清空localCache，flushCache=&quot;true&quot; if (queryStack == 0 &amp;&amp; ms.isFlushCacheRequired()) { clearLocalCache(); } List&lt;E&gt; list; try { queryStack++; // 从缓存中换取查询的数据 list = resultHandler == null ? (List&lt;E&gt;) localCache.getObject(key) : null; if (list != null) { handleLocallyCachedOutputParameters(ms, key, parameter, boundSql); } else { // 缓存不存在从数据库中获取数据，然后回填到localCache； // 如果执行语句为存储过程，还需回填到localOutputParameterCache。 list = queryFromDatabase(ms, parameter, rowBounds, resultHandler, key, boundSql); } } finally { queryStack--; } if (queryStack == 0) { for (DeferredLoad deferredLoad : deferredLoads) { deferredLoad.load(); } // issue #601 deferredLoads.clear(); // 当localCacheScope = STATEMENT时，清空缓存。 if (configuration.getLocalCacheScope() == LocalCacheScope.STATEMENT) { // issue #482 clearLocalCache(); } } return list; } MyBatis一级缓存的“坑”在实际生产中务必将MyBatis的localCacheScope属性设置为STATEMENT，避免其他应用节点执行SQL更新语句后，本节点缓存得不到刷新而导致的数据一致性问题。 MyBatis二级缓存如何使用MyBatis二级缓存？MyBatis二级缓存的使用比较简单，只需要以下几步： 在MyBatis主配置文件中指定cacheEnabled属性值为true。 1234&lt;settings&gt; ... &lt;setting name=&quot;cacheEnabled&quot; value=&quot;true&quot;/&gt;&lt;/settings&gt; 在MyBatis Mapper配置文件中，配置缓存策略、缓存刷新频率、缓存的容量等属性。例如： 1234&lt;cache eviction=&quot;FIFO&quot; flushInterval=&quot;60000&quot; size=&quot;512&quot; readOnly=&quot;true&quot;/&gt; 在配置Mapper时，通过useCache属性指定Mapper执行时是否使用缓存。另外，还可以通过flushCache属性指定Mapper执行后是否刷新缓存，例如： 12345&lt;select id=&quot;list&quot; flushCache=&quot;false&quot; useCache=&quot;true&quot; resultType=&quot;User&quot;&gt; select * from user&lt;/select&gt; 通过上面的配置，MyBatis的二级缓存就可以生效了。执行查询操作时，查询结果会缓存到二级缓存中，执行更新操作后，二级缓存会被清空。 实现原理MyBatis二级缓存是通过CachingExecutor实现的。Configuration类提供了一个工厂方法newExecutor()，该方法返回一个Executor对象。代码如下： 1234567891011121314151617public Executor newExecutor(Transaction transaction, ExecutorType executorType) { executorType = executorType == null ? defaultExecutorType : executorType; executorType = executorType == null ? ExecutorType.SIMPLE : executorType; Executor executor; if (ExecutorType.BATCH == executorType) { executor = new BatchExecutor(this, transaction); } else if (ExecutorType.REUSE == executorType) { executor = new ReuseExecutor(this, transaction); } else { executor = new SimpleExecutor(this, transaction); } if (cacheEnabled) { executor = new CachingExecutor(executor); } executor = (Executor) interceptorChain.pluginAll(executor); return executor; } 如果cacheEnabled属性值为true（开启了二级缓存），则使用CachingExecutor对普通的Executor对象进行装饰，CachingExecutor在普通Executor的基础上增加了二级缓存功能。 CachingExecutor类中维护了一个TransactionalCacheManager实例，TransactionalCacheManager用于管理所有的二级缓存对象。CachingExecutor代码如下： 最后，回顾一下MappedStatement对象创建过程中二级缓存实例的创建。XMLMapperBuilder在解析Mapper配置时会调用cacheElement()方法解析标签，cacheElement()方法代码如下： 在调用MapperBuilderAssistant对象的addMappedStatement()方法创建MappedStatement对象时会将当前命名空间对应的二级缓存对象的引用添加到MappedStatement对象中。 MyBatis使用Redis缓存MyBatis官方提供了一个mybatis-redis模块，该模块用于整合Redis作为二级缓存。使用步骤如下： 首先需要引入该模块的依赖； 然后需要在Mapper的XML配置文件中添加缓存配置。如下所示： 1&lt;cache type = &quot;org.mybatis.caches.redis.redisCache&quot; /&gt; 最后，需要在classpath下新增redis.properties文件，配置Redis的连接信息。 参考资料 美团技术团队-聊聊MyBatis缓存机制 MyBatis 3源码深度解析","link":"/posts/21632.html"},{"title":"SLF4J详解","text":"SLF4J概述The Simple Logging Facade for Java (SLF4J) serves as a simple facade or abstraction for various logging frameworks, such as java.util.logging, logback and log4j. SLF4J allows the end-user to plug in the desired logging framework at deployment time. Simple Logging Facade for Java（SLF4J）可用作各种日志框架的简单外观或抽象，例如java.util.logging，logback和log4j。SLF4J允许最终用户在部署时插入所需的日志记录框架。 Binding with a logging framework at deployment timeThe SLF4J distribution ships with several jar files referred to as SLF4J bindings, with each binding corresponding to a supported framework. slf4j-log4j12-1.7.27.jar Binding for log4j version 1.2, a widely used logging framework. You also need to place log4j.jar on your class path. slf4j-jdk14-1.7.27.jar Binding for java.util.logging, also referred to as JDK 1.4 logging slf4j-nop-1.7.27.jar Binding for NOP, silently discarding all logging. slf4j-simple-1.7.27.jar Binding for Simple implementation, which outputs all events to System.err. Only messages of level INFO and higher are printed. This binding may be useful in the context of small applications. slf4j-jcl-1.7.27.jar Binding for Jakarta Commons Logging. This binding will delegate all SLF4J logging to JCL. logback-classic-1.2.3.jar (requires logback-core-1.2.3.jar) There are also SLF4J bindings external to the SLF4J project, Logback's ch.qos.logback.classic.Logger class is a direct implementation of SLF4J’s org.slf4j.Logger interface. To switch logging frameworks, just replace slf4j bindings on your class path. For example, to switch from java.util.logging to log4j, just replace slf4j-jdk14-1.7.27.jar with slf4j-log4j12-1.7.27.jar. 要切换日志框架，只需替换类路径上的slf4j绑定。例如，要从java.util.logging切换到log4j，只需将slf4j-jdk14-1.7.27.jar替换为slf4j-log4j12-1.7.27.jar即可。 SLF4J does not rely on any special class loader machinery. In fact, each SLF4J binding is hardwired at compile time to use one and only one specific logging framework. For example, the slf4j-log4j12-1.7.27.jar binding is bound at compile time to use log4j. In your code, in addition to slf4j-api-1.7.27.jar, you simply drop one and only one binding of your choice onto the appropriate class path location. Do not place more than one binding on your class path. Consolidate logging via SLF4J（通过SLF4J整合日志记录）SLF4J caters for this common use-case by providing bridging modules for JCL, java.util.logging and log4j. SLF4J通过为JCL，java.util.logging和log4j提供桥接模块来满足这种常见用例。 Mapped Diagnostic Context (MDC) supportMapped Diagnostic Context is essentially a map maintained by the logging framework where the application code provides key-value pairs which can then be inserted by the logging framework in log messages. MDC data can also be highly helpful in filtering messages or triggering certain actions. 映射诊断上下文本质上是由日志框架维护的映射，其中应用程序代码提供键值对，然后日志消息中的日志记录框架可以插入键值对。 MDC数据在过滤消息或触发某些操作方面也非常有用。 SLF4J supports MDC, or mapped diagnostic context. If the underlying logging framework offers MDC functionality, then SLF4J will delegate to the underlying framework’s MDC. Note that at this time, only log4j and logback offer MDC functionality. If the underlying framework does not offer MDC, for example java.util.logging, then SLF4J will still store MDC data but the information therein will need to be retrieved by custom user code. SLF4J支持MDC或映射诊断上下文。 如果底层日志记录框架提供了MDC功能，那么SLF4J将委托给底层框架的MDC。 请注意，目前只有log4j和logback提供MDC功能。 如果底层框架不提供MDC，例如java.util.logging，则SLF4J仍将存储MDC数据，但其中的信息需要由自定义用户代码检索。 原理分析 通过上图可知SLF4J能够抽象各种具体日志框架，是由StaticLoggerBinder类完成。接下来重点看看bind()方法是如何查找StaticLoggerBinder类，源码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859private final static void bind() { try { // 查找类路径下所有的StaticLoggerBinder类 Set&lt;URL&gt; staticLoggerBinderPathSet = findPossibleStaticLoggerBinderPathSet(); // 如果存在多个StaticLoggerBinder类，则打印日志 reportMultipleBindingAmbiguity(staticLoggerBinderPathSet); // 获取StaticLoggerBinder实例，如果不存在，则抛出NoClassDefFoundError异常 StaticLoggerBinder.getSingleton(); INITIALIZATION_STATE = SUCCESSFUL_INITIALIZATION; // 打印实际使用StaticLoggerBinder类 reportActualBinding(staticLoggerBinderPathSet); fixSubstitutedLoggers(); } catch (NoClassDefFoundError ncde) { String msg = ncde.getMessage(); if (messageContainsOrgSlf4jImplStaticLoggerBinder(msg)) { INITIALIZATION_STATE = NOP_FALLBACK_INITIALIZATION; Util.report(&quot;Failed to load class \\&quot;org.slf4j.impl.StaticLoggerBinder\\&quot;.&quot;); Util.report(&quot;Defaulting to no-operation (NOP) logger implementation&quot;); Util.report(&quot;See &quot; + NO_STATICLOGGERBINDER_URL + &quot; for further details.&quot;); } else { failedBinding(ncde); throw ncde; } } catch (java.lang.NoSuchMethodError nsme) { String msg = nsme.getMessage(); if (msg != null &amp;&amp; msg.indexOf(&quot;org.slf4j.impl.StaticLoggerBinder.getSingleton()&quot;) != -1) { INITIALIZATION_STATE = FAILED_INITIALIZATION; Util.report(&quot;slf4j-api 1.6.x (or later) is incompatible with this binding.&quot;); Util.report(&quot;Your binding is version 1.5.5 or earlier.&quot;); Util.report(&quot;Upgrade your binding to version 1.6.x.&quot;); } throw nsme; } catch (Exception e) { failedBinding(e); throw new IllegalStateException(&quot;Unexpected initialization failure&quot;, e); }}private static String STATIC_LOGGER_BINDER_PATH = &quot;org/slf4j/impl/StaticLoggerBinder.class&quot;;private static Set&lt;URL&gt; findPossibleStaticLoggerBinderPathSet() { Set&lt;URL&gt; staticLoggerBinderPathSet = new LinkedHashSet&lt;URL&gt;(); try { ClassLoader loggerFactoryClassLoader = LoggerFactory.class.getClassLoader(); Enumeration&lt;URL&gt; paths; if (loggerFactoryClassLoader == null) { paths = ClassLoader.getSystemResources(STATIC_LOGGER_BINDER_PATH); } else { paths = loggerFactoryClassLoader.getResources(STATIC_LOGGER_BINDER_PATH); } while (paths.hasMoreElements()) { URL path = (URL) paths.nextElement(); staticLoggerBinderPathSet.add(path); } } catch (IOException ioe) { Util.report(&quot;Error getting resources from path&quot;, ioe); } return staticLoggerBinderPathSet;} SLF4J与logback集成 maven依赖包如下： 123456789101112131415161718&lt;!-- slf4j-api --&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.12&lt;/version&gt;&lt;/dependency&gt;&lt;!-- logback --&gt;&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-core&lt;/artifactId&gt; &lt;version&gt;1.1.3&lt;/version&gt; &lt;/dependency&gt;&lt;!-- logback-classic（已含有对slf4j的集成包） --&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.1.3&lt;/version&gt; &lt;/dependency&gt; 编写logback的配置文件logback.xml，内容如下: 1234567891011&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;configuration&gt; &lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;encoder&gt; &lt;pattern&gt;%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;root level=&quot;DEBUG&quot;&gt; &lt;appender-ref ref=&quot;STDOUT&quot; /&gt; &lt;/root&gt; &lt;/configuration&gt; 使用方式 123456789101112private static final Logger logger=LoggerFactory.getLogger(LogbackTest.class);public static void main(String[] args){ if(logger.isDebugEnabled()){ logger.debug(&quot;slf4j-logback debug message&quot;); } if(logger.isInfoEnabled()){ logger.info(&quot;slf4j-logback info message&quot;); } if(logger.isTraceEnabled()){ logger.trace(&quot;slf4j-logback trace message&quot;); }} org.slf4j.impl.StaticLoggerBinder类实现 SLF4J与log4集成 maven依赖包如下： 1234567891011121314151617181920&lt;!-- slf4j --&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.12&lt;/version&gt;&lt;/dependency&gt;&lt;!-- slf4j-log4j --&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.12&lt;/version&gt;&lt;/dependency&gt;&lt;!-- log4j --&gt;&lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt;&lt;/dependency&gt; 编写log4j.properties配置文件,放到类路径下 1234log4j.rootLogger = debug, consolelog4j.appender.console = org.apache.log4j.ConsoleAppenderlog4j.appender.console.layout = org.apache.log4j.PatternLayoutlog4j.appender.console.layout.ConversionPattern = %-d{yyyy-MM-dd HH:mm:ss} %m%n 使用方式 123456789101112private static Logger logger=LoggerFactory.getLogger(Log4j2Slf4jTest.class);public static void main(String[] args){ if(logger.isTraceEnabled()){ logger.trace(&quot;slf4j-log4j2 trace message&quot;); } if(logger.isDebugEnabled()){ logger.debug(&quot;slf4j-log4j2 debug message&quot;); } if(logger.isInfoEnabled()){ logger.info(&quot;slf4j-log4j2 info message&quot;); }} org.slf4j.impl.StaticLoggerBinder类实现 参考资料slf4j官方文档 jcl与jul、log4j1、log4j2、logback的集成原理 ```","link":"/posts/11108.html"},{"title":"Netty：从0到1实现RPC","text":"RPC 框架架构设计RPC 又称远程过程调用（Remote Procedure Call），用于解决分布式系统中服务之间的调用问题。通俗地讲，就是开发者能够像调用本地方法一样调用远程的服务。RPC 框架的基本架构如下： RPC 框架包含三个最重要的组件，分别是客户端、服务端和注册中心。在一次 RPC 调用流程中，这三个组件是这样交互的： 服务端在启动后，会将它提供的服务列表发布到注册中心，客户端向注册中心订阅服务地址； 客户端会通过本地代理模块 Proxy 调用服务端，Proxy 模块负责将方法、参数等数据转化成网络字节流； 客户端从服务列表中选取其中一个的服务地址，并将数据通过网络发送给服务端； 服务端接收到数据后进行解码，得到请求信息； 服务端根据解码后的请求信息调用对应的服务，然后将调用结果返回给客户端。 虽然 RPC 调用流程很容易理解，但是实现一个完整的 RPC 框架设计到很多内容，例如服务注册与发现、通信协议与序列化、负载均衡、动态代理等 服务注册与发现在 RPC 框架中，主要是使用注册中心来实现服务注册和发现的功能。服务端节点上线后自行向注册中心注册服务列表，节点下线时需要从注册中心将节点元数据信息移除。客户端向服务端发起调用时，自己负责从注册中心获取服务端的服务列表，然后在通过负载均衡算法选择其中一个服务节点进行调用。 现在思考一个问题，服务在下线时需要从注册中心移除元数据，那么注册中心怎么才能感知到服务下线呢？我们最先想到的方法就是节点主动通知的实现方式，当节点需要下线时，向注册中心发送下线请求，让注册中心移除自己的元数据信息。但是如果节点异常退出，例如断网、进程崩溃等，那么注册中心将会一直残留异常节点的元数据，从而可能造成服务调用出现问题。 为了避免上述问题，实现服务优雅下线比较好的方式是采用主动通知 + 心跳检测的方案。除了主动通知注册中心下线外，还需要增加节点与注册中心的心跳检测功能，这个过程也叫作探活。心跳检测可以由节点或者注册中心负责，例如注册中心可以向服务节点每 60s 发送一次心跳包，如果 3 次心跳包都没有收到请求结果，可以任务该服务节点已经下线。 通信协议与序列化既然 RPC 是远程调用，必然离不开网络通信协议。客户端在向服务端发起调用之前，需要考虑采用何种方式将调用信息进行编码，并传输到服务端。 RPC 框架对性能有非常高的要求，所以通信协议应该越简单越好，这样可以减少编解码的性能损耗。RPC 框架可以基于不同的协议实现，大部分主流 RPC 框架会选择 TCP、HTTP 协议，出名的 gRPC 框架使用的则是 HTTP2。 TCP、HTTP、HTTP2 都是稳定可靠的，但其实使用 UDP 协议也是可以的，具体看业务使用的场景。成熟的 RCP 框架能够支持多种协议，例如阿里开源的 Dubbo 框架被很多互联网公司广泛使用，其中可插拔的协议支持是 Dubbo 的一大特色，这样不仅可以给开发者提供多种不同的选择，而且为接入异构系统提供了便利。 客户端和服务端在通信过程中需要传输哪些数据呢？这些数据又该如何编解码呢？ 如果采用 TCP 协议，你需要将调用的接口、方法、请求参数、调用属性等信息序列化成二进制字节流传递给服务提供方，服务端接收到数据后，再把二进制字节流反序列化得到调用信息，然后利用反射的原理调用对应方法，最后将返回结果、返回码、异常信息等编码后通过网络返回给客户端。 所谓序列化和反序列化就是将对象转换成二进制流以及将二进制流再转换成对象的过程。因为网络通信依赖于字节流，而且这些请求信息都是不确定的，所以一般会选用通用且高效的序列化算法。比较常用的序列化算法有 FastJson、Kryo、Hessian、Protobuf 等，这些第三方序列化算法都比 Java 原生的序列化操作都更加高效。Dubbo 支持多种序列化算法，并定义了 Serialization 接口规范，所有序列化算法扩展都必须实现该接口，其中默认使用的是 Hessian 序列化算法。 RPC调用方式成熟的 RPC 框架一般会提供四种调用方式，分别为同步 Sync、异步 Future、回调 Callback和单向 Oneway。RPC 框架的性能和吞吐量与合理使用调用方式是息息相关的，下面我们逐一介绍下四种调用方式的实现原理。 Sync 同步调用。客户端线程发起 RPC 调用后，当前线程会一直阻塞，直至服务端返回结果或者处理超时异常。Sync 同步调用一般是 RPC 框架默认的调用方式，为了保证系统可用性，客户端设置合理的超时时间是非常重要的。虽说 Sync 是同步调用，但是客户端线程和服务端线程并不是同一个线程，实际在 RPC 框架内部还是异步处理的。Sync 同步调用的过程如下图所示。 Future 异步调用。客户端发起调用后不会再阻塞等待，而是拿到 RPC 框架返回的 Future 对象，调用结果会被服务端缓存，客户端自行决定后续何时获取返回结果。当客户端主动获取结果时，该过程是阻塞等待的。Future 异步调用过程如下图所示。 Callback 回调调用。如下图所示，客户端发起调用时，将 Callback 对象传递给 RPC 框架，无须同步等待返回结果，直接返回。当获取到服务端响应结果或者超时异常后，再执行用户注册的 Callback 回调。所以 Callback 接口一般包含 onResponse 和 onException 两个方法，分别对应成功返回和异常返回两种情况。 Oneway 单向调用。客户端发起请求之后直接返回，忽略返回结果。Oneway 方式是最简单的，具体调用过程如下图所示。 四种调用方式都各有优缺点，很难说异步方式一定会比同步方式效果好，在不用的业务场景可以按需选取更合适的调用方式。 线程模型线程模型是 RPC 框架需要重点关注的部分，与Netty Reactor 线程模型有什么区别和联系吗？ 首先我们需要明确 I/O 线程和业务线程的区别，以 Dubbo 框架为例，Dubbo 使用 Netty 作为底层的网络通信框架，采用了我们熟悉的主从 Reactor 线程模型，其中 Boss 和 Worker 线程池就可以看作 I/O 线程。I/O 线程可以理解为主要负责处理网络数据，例如事件轮询、编解码、数据传输等。 如果业务逻辑能够立即完成，也可以使用 I/O 线程进行处理，这样可以省去线程上下文切换的开销。如果业务逻辑耗时较多，例如包含查询数据库、复杂规则计算等耗时逻辑，那么 I/O 必须将这些请求分发到业务线程池中进行处理，以免阻塞 I/O 线程。 负载均衡在分布式系统中，服务提供者和服务消费者都会有多台节点，如何保证服务提供者所有节点的负载均衡呢？客户端在发起调用之前，需要感知有多少服务端节点可用，然后从中选取一个进行调用。客户端需要拿到服务端节点的状态信息，并根据不同的策略实现负载均衡算法。负载均衡策略是影响 RPC 框架吞吐量很重要的一个因素，下面我们介绍几种最常用的负载均衡策略。 Round-Robin 轮询。Round-Robin 是最简单有效的负载均衡策略，并没有考虑服务端节点的实际负载水平，而是依次轮询服务端节点。 Weighted Round-Robin 权重轮询。对不同负载水平的服务端节点增加权重系数，这样可以通过权重系数降低性能较差或者配置较低的节点流量。权重系数可以根据服务端负载水平实时进行调整，使集群达到相对均衡的状态。 Least Connections 最少连接数。客户端根据服务端节点当前的连接数进行负载均衡，客户端会选择连接数最少的一台服务器进行调用。Least Connections 策略只是服务端其中一种维度，我们可以演化出最少请求数、CPU 利用率最低等其他维度的负载均衡方案。 Consistent Hash 一致性 Hash。目前主流推荐的负载均衡策略，Consistent Hash 是一种特殊的 Hash 算法，在服务端节点扩容或者下线时，尽可能保证客户端请求还是固定分配到同一台服务器节点。Consistent Hash 算法是采用哈希环来实现的，通过 Hash 函数将对象和服务器节点放置在哈希环上，一般来说服务器可以选择 IP + Port 进行 Hash，然后为对象选择对应的服务器节点，在哈希环中顺时针查找距离对象 Hash 值最近的服务器节点。 此外，负载均衡算法可以是多种多样的，客户端可以记录例如健康状态、连接数、内存、CPU、Load 等更加丰富的信息，根据综合因素进行更好地决策。 动态代理RPC 框架怎么做到像调用本地接口一样调用远端服务呢？ 这必须依赖动态代理来实现。需要创建一个代理对象，在代理对象中完成数据报文编码，然后发起调用发送数据给服务提供方，以此屏蔽 RPC 框架的调用细节。 因为代理类是在运行时生成的，所以代理类的生成速度、生成的字节码大小都会影响 RPC 框架整体的性能和资源消耗，所以需要慎重选择动态代理的实现方案。动态代理比较主流的实现方案有以下几种：JDK 动态代理、Cglib、Javassist、ASM、Byte Buddy，我们简单做一个对比和介绍。 JDK 动态代理。在运行时可以动态创建代理类，但是 JDK 动态代理的功能比较局限，代理对象必须实现一个接口，否则抛出异常。因为代理类会继承 Proxy 类，然而 Java 是不支持多重继承的，只能通过接口实现多态。JDK 动态代理所生成的代理类是接口的实现类，不能代理接口中不存在的方法。JDK 动态代理是通过反射调用的形式代理类中的方法，比直接调用肯定是性能要慢的。 Cglib 动态代理。Cglib 是基于 ASM 字节码生成框架实现的，通过字节码技术生成的代理类，所以代理类的类型是不受限制的。而且 Cglib 生成的代理类是继承于被代理类，所以可以提供更加灵活的功能。在代理方法方面，Cglib 是有优势的，它采用了 FastClass 机制，为代理类和被代理类各自创建一个 Class，这个 Class 会为代理类和被代理类的方法分配 index 索引，FastClass 就可以通过 index 直接定位要调用的方法，并直接调用，这是一种空间换时间的优化思路。 Javassist 和 ASM。二者都是 Java 字节码操作框架，使用起来难度较大，需要开发者对 Class 文件结构以及 JVM 都有所了解，但是它们都比反射的性能要高。 Byte Buddy 也是一个字节码生成和操作的类库，Byte Buddy 功能强大，相比于 Javassist 和 ASM，Byte Buddy 提供了更加便捷的 API，用于创建和修改 Java 类，无须理解字节码的格式，而且 Byte Buddy 更加轻量，性能更好。","link":"/posts/63400.html"},{"title":"SpringCloud：面试题","text":"SpringCloud Rpc调用过程SpringCloud Rpc调用过程，大致会经过如下几个组件配合： 1Feign -----&gt;Hystrix —&gt;Ribbon —&gt;Http Client（apache http components 或者 Okhttp） 具体交互流程上，如下图所示： 当调用被@FeignClient注解修饰的接口时，在框架内部，将请求转换成Feign的请求实例feign.Request，交由Feign框架处理。 根据@FeignClient(name = “service-vod”)找到服务名，并根据接口中的方法地址进行调用。 Hystrix代理拦截每一个Feign调用请求，Hystrix都会将其包装成HystrixCommand，参与Hystrix的流控和熔断规则。 如果请求判断需要熔断，则Hystrix直接熔断，抛出异常或者使用FallbackFactory返回熔断Fallback结果；如果通过，则将调用请求传递给Ribbon组件。 当请求传递到Ribbon之后，Ribbon会根据自身维护的服务列表，根据服务的服务质量，如平均响应时间，Load等，结合特定的规则，从列表中挑选合适的服务实例，选择好机器之后，然后将机器实例的信息请求传递给Http Client客户端，HttpClient客户端来执行真正的Http接口调用。 HttpClien根据上层Ribbon传递过来的请求，已经指定了服务地址，则HttpClient开始执行真正的Http请求。 Rpc超时时间和重试次数设定TODO Hystrix线程池隔离实现原理","link":"/posts/28354.html"},{"title":"Spring：XML配置扩展","text":"原文地址：https://www.jianshu.com/p/8639e5e9fba6 Step从Spring 2.0版本开始，Spring提供了XML Schema可扩展机制，用于定义和配置Bean。完成XML自定义扩展，需要下面几个步骤： 创建一个 XML Schema 文件，描述自定义的合法构建模块，也就是xsd文件。 自定义处理器类，并实现NamespaceHandler接口。 自定义一个或多个解析器，实现BeanDefinitionParser接口(最关键的部分)。 注册上面的组件到Spring IOC容器中。 Coding按照上面的步骤，实现如下可扩展XML元素： 1&lt;myns:dateformat id=&quot;dateFormat&quot; pattern=&quot;yyyy-MM-dd HH:mm&quot; lenient=&quot;true&quot;/&gt; 等价于 1234&lt;bean id=&quot;dateFormat&quot; class=&quot;java.text.SimpleDateFormat&quot;&gt; &lt;constructor-arg value=&quot;yyyy-HH-dd HH:mm&quot;/&gt; &lt;property name=&quot;lenient&quot; value=&quot;true&quot;/&gt;&lt;/bean&gt; 创建XML Schema 文件12345678910111213141516171819202122&lt;!-- myns.xsd (inside package org/springframework/samples/xml) --&gt;&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;xsd:schema xmlns=&quot;http://www.mycompany.com/schema/myns&quot; xmlns:xsd=&quot;http://www.w3.org/2001/XMLSchema&quot; xmlns:beans=&quot;http://www.springframework.org/schema/beans&quot; targetNamespace=&quot;http://www.mycompany.com/schema/myns&quot; elementFormDefault=&quot;qualified&quot; attributeFormDefault=&quot;unqualified&quot;&gt; &lt;xsd:import namespace=&quot;http://www.springframework.org/schema/beans&quot;/&gt; &lt;xsd:element name=&quot;dateformat&quot;&gt; &lt;xsd:complexType&gt; &lt;xsd:complexContent&gt; &lt;xsd:extension base=&quot;beans:identifiedType&quot;&gt; &lt;xsd:attribute name=&quot;lenient&quot; type=&quot;xsd:boolean&quot;/&gt; &lt;xsd:attribute name=&quot;pattern&quot; type=&quot;xsd:string&quot; use=&quot;required&quot;/&gt; &lt;/xsd:extension&gt; &lt;/xsd:complexContent&gt; &lt;/xsd:complexType&gt; &lt;/xsd:element&gt;&lt;/xsd:schema&gt; 自定义NamespaceHandlerNamespacespaceHandler用于解析配置文件时遇到的特定命名空间的所有元素。在我们的例子中，NamespaceHandler应该处理myns：dateformat元素的解析。 NamespaceHandler提供如下三个方法: init(): NamespaceHandler被使用之前调用，完成NamespaceHandler的初始化。 BeanDefinition parse(Element, ParserContext): 当遇到顶层元素时被调用。 BeanDefinition decorate(Node,BeanDefinitionHandler,ParserContext): 当遇到一个属性或者嵌套元素的时候调用。 Spring提供了一个默认的实现类NamespaceHandlerSupport，只需要在init的时候注册每个元素的解析器即可。 123456public class DateformatNamespaceHandler extends NamespaceHandlerSupport { public void init() { registerBeanDefinitionParser(&quot;dateformat&quot;, new DeteformatDefinitionParser()); }} 这里实际用到了代理委托的概念，NamespaceHandlerSupport可以注册任意个BeanDefinitionParser。NamespaceHandlerSupport负责所有自定义元素的编排，而解析XML的工作委托给各个BeanDefinitionParser负责。 自定义BeanDefinitionParser如果NamespapceHandler遇到元素类型（如：dateformat）已经有对应注册的parser，则DateformatDefinitionParser会被调用，解析相应的属性设置到Bean中。BeanDefinitionParser负责解析一个顶级元素。 Spring提供了AbstractSingleBeanDefinitionParser来处理繁重的解析工作，只需要实现两个方法: Class&lt;?&gt; getBeanClass(Element)：返回元素的Class类型。 void doParse(Element element,BeanDefinitionBuilder builder)：添加元素的属性或者构造参数等等。 12345678910111213141516171819202122232425262728ppackage org.springframework.samples.xml;import org.springframework.beans.factory.support.BeanDefinitionBuilder;import org.springframework.beans.factory.xml.AbstractSingleBeanDefinitionParser;import org.springframework.util.StringUtils;import org.w3c.dom.Element;import java.text.SimpleDateFormat;public class SimpleDateFormatBeanDefinitionParser extends AbstractSingleBeanDefinitionParser { protected Class getBeanClass(Element element) { return SimpleDateFormat.class; } protected void doParse(Element element, BeanDefinitionBuilder bean) { // this will never be null since the schema explicitly requires that a value be supplied String pattern = element.getAttribute(&quot;pattern&quot;); bean.addConstructorArg(pattern); // this however is an optional property String lenient = element.getAttribute(&quot;lenient&quot;); if (StringUtils.hasText(lenient)) { bean.addPropertyValue(&quot;lenient&quot;, Boolean.valueOf(lenient)); } }} 注册handler和schema为了让Spring在解析xml的时候能够感知到我们的自定义元素，我们需要把NamespaceHandler和xsd文件放到2个指定的配置文件中，这2个文件都位于META-INF目录中 'META-INF/spring.handlers'：包含XML Schema URI到命名空间处理程序类的映射。因此，对于我们的示例，我们需要编写以下内容： 1http\\://www.mycompany.com/schema/myns=org.springframework.samples.xml.DateformatNamespaceHandler META-INF/spring.schemas：包含XML Schema xsd到类路径资源的映射。 1http\\://www.mycompany.com/schema/myns/myns.xsd=org/springframework/samples/xml/myns.xsd Spring XML配置中使用自定义扩展12345678910111213141516171819&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:myns=&quot;http://www.mycompany.com/schema/myns&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.mycompany.com/schema/myns http://www.mycompany.com/schema/myns/myns.xsd&quot;&gt; &lt;!-- as a top-level bean --&gt; &lt;myns:dateformat id=&quot;defaultDateFormat&quot; pattern=&quot;yyyy-MM-dd HH:mm&quot; lenient=&quot;true&quot;/&gt; &lt;bean id=&quot;jobDetailTemplate&quot; abstract=&quot;true&quot;&gt; &lt;property name=&quot;dateFormat&quot;&gt; &lt;!-- as an inner bean --&gt; &lt;myns:dateformat pattern=&quot;HH:mm MM-dd-yyyy&quot;/&gt; &lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt;","link":"/posts/45222.html"},{"title":"分布式缓存：基本原理","text":"缓存概述缓存的定义 狭义缓存：是指用于加速 CPU 数据交换的 RAM，即随机存取存储器，通常这种存储器使用更昂贵但快速的静态 RAM（SRAM）技术，用以对 DRAM进 行加速。 广义缓存：任何可以用于数据高速交换的存储介质都是缓存，可以是硬件也可以是软件。 缓存存在的意义就是通过开辟一个新的数据交换缓冲区，来解决原始数据获取代价太大的问题，让数据得到更快的访问。 缓存的基本思想 缓存构建的基本思想是利用时间局限性原理，通过空间换时间来达到加速数据获取的目的，同时由于缓存空间的成本较高，在实际设计架构中还要考虑访问延迟和成本的权衡问题。这里面有 3 个关键点。 时间局限性原理，即被获取过一次的数据在未来会被多次引用，比如一条微博被一个人感兴趣并阅读后，它大概率还会被更多人阅读，当然如果变成热门微博后，会被数以百万/千万计算的更多用户查看。 以空间换时间，因为原始数据获取太慢，所以我们开辟一块高速独立空间，提供高效访问，来达到数据获取加速的目的。 性能成本Tradeoff，构建系统时希望系统的访问性能越高越好，访问延迟越低小越好。但维持相同数据规模的存储及访问，性能越高延迟越小，成本也会越高，所以在系统架构设计时，你需要在系统性能和开发运行成本之间做取舍。比如左边这张图，相同成本的容量，SSD 硬盘容量会比内存大 10～30 倍以上，但读写延迟却高 50～100 倍。 缓存的优势缓存的优势主要有以下几点： 提升访问性能 降低网络拥堵 减轻服务负载 增强可扩展性 在实际业务场景中，缓存中存储的往往是需要频繁访问的中间数据甚至最终结果，这些数据相比 DB 中的原始数据小很多，这样就可以减少网络流量，降低网络拥堵，同时由于减少了解析和计算，调用方和存储服务的负载也可以大幅降低。 缓存的读写性能很高，预热快，在数据访问存在性能瓶颈或遇到突发流量，系统读写压力大增时，可以快速部署上线，同时在流量稳定后，也可以随时下线，从而使系统的可扩展性大大增强。 缓存的代价任何事情都有两面性，缓存也不例外，我们在享受缓存带来一系列好处的同时，也注定需要付出一定的代价。 首先，服务系统中引入缓存，会增加系统的复杂度。 其次，由于缓存相比原始 DB 存储的成本更高，所以系统部署及运行的费用也会更高。 最后，由于一份数据同时存在缓存和 DB 中，甚至缓存内部也会有多个数据副本，多份数据就会存在一致性问题，同时缓存体系本身也会存在可用性问题和分区的问题。这就需要我们加强对缓存原理、缓存组件以及优秀缓存体系实践的理解，从系统架构之初就对缓存进行良好设计，降低缓存引入的副作用，让缓存体系成为服务系统高效稳定运行的强力基石。 一般来讲，服务系统的全量原始数据存储在 DB 中（如 MySQL、HBase 等），所有数据的读写都可以通过 DB 操作来获取。但 DB 读写性能低、延迟高，如 MySQL 单实例的读写 QPS 通常只有千级别（3000～6000），读写平均耗时 10～100ms 级别，如果一个用户请求需要查 20 个不同的数据来聚合，仅仅 DB 请求就需要数百毫秒甚至数秒。而 cache 的读写性能正好可以弥补 DB 的不足，比如 Memcached 的读写 QPS 可以达到 10～100万 级别，读写平均耗时在 1ms 以下，结合并发访问技术，单个请求即便查上百条数据，也可以轻松应对。 但 cache 容量小，只能存储部分访问频繁的热数据，同时，同一份数据可能同时存在 cache 和 DB，如果处理不当，就会出现数据不一致的问题。所以服务系统在处理业务请求时，需要对 cache 的读写方式进行适当设计，既要保证数据高效返回，又要尽量避免数据不一致等各种问题。 缓存的读写模式Cache Aside 在Cache Aside 模式中，业务应用方对于写，是更新 DB 后，直接将 key 从 cache 中删除，然后由 DB 驱动缓存数据的更新；而对于读，是先读 cache，如果 cache 没有，则读 DB，同时将从 DB 中读取的数据回写到 cache。 这种模式的特点是，业务端处理所有数据访问细节，同时利用 Lazy 计算的思想，更新 DB 后，直接删除 cache 并通过 DB 更新，确保数据以 DB 结果为准，则可以大幅降低 cache 和 DB 中数据不一致的概率。 如果没有专门的存储服务，同时是对数据一致性要求比较高的业务，或者是缓存数据更新比较复杂的业务，这些情况都比较适合使用 Cache Aside 模式。如微博发展初期，不少业务采用这种模式，这些缓存数据需要通过多个原始数据进行计算后设置。在部分数据变更后，直接删除缓存。同时，使用一个 Trigger 组件，实时读取 DB 的变更日志，然后重新计算并更新缓存。如果读缓存的时候，Trigger 还没写入 cache，则由调用方自行到 DB 加载计算并写入cache。 Read/Write Through 对于Cache Aside 模式，业务应用需要同时维护cache和DB两个数据存储方，过于繁琐，于是就有了Read/Write Through模式。在这种模式下，业务应用只关注一个存储服务即可，业务方的读写cache 和 DB 的操作，都由存储服务代理。存储服务收到业务应用的写请求时，会首先查 cache，如果数据在 cache 中不存在，则只更新 DB，如果数据在cache中存在，则先更新 cache，然后更新 DB。而存储服务收到读请求时，如果命中cache直接返回，否则先从DB加载，回种到cache后返回响应。 这种模式的特点是，存储服务封装了所有的数据处理细节，业务应用端代码只用关注业务逻辑本身，系统的隔离性更佳。另外，进行写操作时，如果 cache 中没有数据则不更新，有缓存数据才更新，内存效率更高。 微博 Feed 的 Outbox Vector（即用户最新微博列表）就采用这种模式。一些粉丝较少且不活跃的用户发表微博后，Vector服务会首先查询 Vector Cache，如果cache 中没有该用户的Outbox记录，则不写该用户的cache数据，直接更新DB后就返回，只有cache中存在才会通过 CAS 指令进行更新。 Write Behind Caching Write Behind Caching 模式与Read/Write Through模式类似，也由数据存储服务来管理cache和DB的读写。不同点是，数据更新时，Read/write Through是同步更新cache和DB，而 Write Behind Caching则是只更新缓存，不直接更新DB，而是改为异步批量的方式来更新DB。 该模式的特点是，数据存储的写性能最高，非常适合一些变更特别频繁的业务，特别是可以合并写请求的业务，比如对一些计数业务，一条Feed被点赞1万次，如果更新1万次DB代价很大，而合并成一次请求直接加 1万，则是一个非常轻量的操作。但这种模型有个显著的缺点，即数据的一致性变差，甚至在一些极端场景下可能会丢失数据。比如系统 Crash、机器宕机时，如果有数据还没保存到DB，则会存在丢失的风险。 所以这种读写模式适合变更频率特别高，但对一致性要求不太高的业务，这样写操作可以异步批量写入DB，减小DB压力。 缓存分类按宿主层次分类按宿主层次分类的话，缓存一般可以分为本地 Cache、进程间Cache和远程Cache。 本地 Cache 是指业务进程内的缓存，这类缓存由于在业务系统进程内，所以读写性能超高且无任何网络开销，但不足是会随着业务系统重启而丢失。 进程间 Cache 是本机独立运行的缓存，这类缓存读写性能较高，不会随着业务系统重启丢数据，并且可以大幅减少网络开销，但不足是业务系统和缓存都在相同宿主机，运维复杂，且存在资源竞争。 远程 Cache 是指跨机器部署的缓存，这类缓存因为独立设备部署，容量大且易扩展，在互联网企业使用最广泛。不过远程缓存需要跨机访问，在高读写压力下，带宽容易成为瓶颈。 本地 Cache 的缓存组件有 Ehcache、Guava Cache 等，开发者自己也可以用 Map、Set 等轻松构建一个自己专用的本地 Cache。进程间 Cache 和远程 Cache 的缓存组件相同，只是部署位置的差异罢了，这类缓存组件有 Memcached、Redis、Pika 等。 按存储介质分类还有一种常见的分类方式是按存储介质来分，这样可以分为内存型缓存和持久化型缓存。 内存型缓存将数据存储在内存，读写性能很高，但缓存系统重启或 Crash 后，内存数据会丢失。 持久化型缓存将数据存储到 SSD/Fusion-IO 硬盘中，相同成本下，这种缓存的容量会比内存型缓存大 1 个数量级以上，而且数据会持久化落地，重启不丢失，但读写性能相对低 1～2 个数量级。Memcached 是典型的内存型缓存，而 Pika 以及其他基于 RocksDB 开发的缓存组件等则属于持久化型缓存。","link":"/posts/38132.html"},{"title":"分布式缓存：设计缓存架构需考量那些因素？","text":"设计思路缓存组件选择在设计架构缓存时，你首先要选定缓存组件，比如要用 Local-Cache，还是 Redis、Memcached、Pika 等开源缓存组件，如果业务缓存需求比较特殊，你还要考虑是直接定制开发一个新的缓存组件，还是对开源缓存进行二次开发，来满足业务需要。 缓存数据结构设计确定好缓存组件后，你还要根据业务访问的特点，进行缓存数据结构的设计。对于直接简单 KV 读写的业务，你可以将这些业务数据封装为 String、Json、Protocol Buffer 等格式，序列化成字节序列，然后直接写入缓存中。读取时，先从缓存组件获取到数据的字节序列，再进行反序列化操作即可。 对于只需要存取部分字段或需要在缓存端进行计算的业务，你可以把数据设计为 Hash、Set、List、Geo 等结构，存储到支持复杂集合数据类型的缓存中，如 Redis、Pika 等。 缓存分布设计确定了缓存组件，设计好了缓存数据结构，接下来就要设计缓存的分布。可以从 3 个维度来进行缓存分布设计。 首先，要选择分布式算法，是采用取模还是一致性 Hash 进行分布。 取模分布的方案简单，每个 key 只会存在确定的缓存节点，一致性 Hash 分布的方案相对复杂，一个 key 对应的缓存节点不确定。但一致性 Hash 分布，可以在部分缓存节点异常时，将失效节点的数据访问均衡分散到其他正常存活的节点，从而更好地保证了缓存系统的稳定性。 其次，分布读写访问如何进行实施，是由缓存 Client 直接进行 Hash 分布定位读写，还是通过 Proxy 代理来进行读写路由？ Client 直接读写，读写性能最佳，但需要 Client 感知分布策略。在缓存部署发生在线变化时，也需要及时通知所有缓存 Client，避免读写异常，另外，Client 实现也较复杂。 而通过 Proxy 路由，Client 只需直接访问 Proxy，分布逻辑及部署变更都由 Proxy 来处理，对业务应用开发最友好，但业务访问多一跳，访问性能会有一定的损失。 最后，缓存系统运行过程中，如果待缓存的数据量增长过快，会导致大量缓存数据被剔除，缓存命中率会下降，数据访问性能会随之降低，这样就需要将数据从缓存节点进行动态拆分，把部分数据水平迁移到其他缓存节点。这个迁移过程需要考虑，是由 Proxy 进行迁移还是缓存 Server 自身进行迁移，甚至根本就不支持迁移。对于 Memcached，一般不支持迁移，对 Redis，社区版本是依靠缓存 Server 进行迁移，而对 Codis 则是通过 Admin、Proxy 配合后端缓存组件进行迁移。 缓存架构部署及运维管理设计完毕缓存的分布策略后，接下来就要考虑缓存的架构部署及运维管理了。架构部署主要考虑如何对缓存进行分池、分层、分 IDC，以及是否需要进行异构处理。 核心的、高并发访问的不同数据，需要分别分拆到独立的缓存池中，进行分别访问，避免相互影响；访问量较小、非核心的业务数据，则可以混存。 对海量数据、访问超过 10～100万 级的业务数据，要考虑分层访问，并且要分摊访问量，避免缓存过载。 如果业务系统需要多 IDC 部署甚至异地多活，则需要对缓存体系也进行多 IDC 部署，要考虑如何跨 IDC 对缓存数据进行更新，可以采用直接跨 IDC 读写，也可以采用 DataBus 配合队列机进行不同 IDC 的消息同步，然后由消息处理机进行缓存更新，还可以由各个 IDC 的 DB Trigger 进行缓存更新。 某些极端场景下，还需要把多种缓存组件进行组合使用，通过缓存异构达到最佳读写性能。 站在系统层面，要想更好得管理缓存，还要考虑缓存的服务化，考虑缓存体系如何更好得进行集群管理、监控运维等。 缓存设计架构的常见考量点读写方式首先是 value 的读写方式。是全部整体读写，还是只部分读写及变更？是否需要内部计算？比如，用户粉丝数，很多普通用户的粉丝有几千到几万，而大 V 的粉丝更是高达几千万甚至过亿，因此，获取粉丝列表肯定不能采用整体读写的方式，只能部分获取。另外在判断某用户是否关注了另外一个用户时，也不需要拉取该用户的全部关注列表，直接在关注列表上进行检查判断，然后返回 True/False 或 0/1 的方式更为高效。 KV size然后是不同业务数据缓存 KV 的 size。如果单个业务的 KV size 过大，需要分拆成多个 KV 来缓存。但是，不同缓存数据的 KV size 如果差异过大，也不能缓存在一起，避免缓存效率的低下和相互影响。 key 的数量key 的数量也是一个重要考虑因素。如果 key 数量不大，可以在缓存中存下全量数据，把缓存当 DB 存储来用，如果缓存读取 miss，则表明数据不存在，根本不需要再去 DB 查询。如果数据量巨大，则在缓存中尽可能只保留频繁访问的热数据，对于冷数据直接访问 DB。 读写峰值另外，对缓存数据的读写峰值，如果小于 10万 级别，简单分拆到独立 Cache 池即可。而一旦数据的读写峰值超过 10万 甚至到达 100万 级的QPS，则需要对 Cache 进行分层处理，可以同时使用 Local-Cache 配合远程 cache，甚至远程缓存内部继续分层叠加分池进行处理。微博业务中，大多数核心业务的 Memcached 访问都采用的这种处理方式。 命中率缓存的命中率对整个服务体系的性能影响甚大。对于核心高并发访问的业务，需要预留足够的容量，确保核心业务缓存维持较高的命中率。比如微博中的 Feed Vector Cache，常年的命中率高达 99.5% 以上。为了持续保持缓存的命中率，缓存体系需要持续监控，及时进行故障处理或故障转移。同时在部分缓存节点异常、命中率下降时，故障转移方案，需要考虑是采用一致性 Hash 分布的访问漂移策略，还是采用数据多层备份策略。 过期策略 可以设置较短的过期时间，让冷 key 自动过期； 也可以让 key 带上时间戳，同时设置较长的过期时间，比如很多业务系统内部有这样一些 key：key_20190801。 平均缓存穿透加载时间平均缓存穿透加载时间在某些业务场景下也很重要，对于一些缓存穿透后，加载时间特别长或者需要复杂计算的数据，而且访问量还比较大的业务数据，要配置更多容量，维持更高的命中率，从而减少穿透到 DB 的概率，来确保整个系统的访问性能。 缓存可运维性对于缓存的可运维性考虑，则需要考虑缓存体系的集群管理，如何进行一键扩缩容，如何进行缓存组件的升级和变更，如何快速发现并定位问题，如何持续监控报警，最好有一个完善的运维平台，将各种运维工具进行集成。 缓存安全性对于缓存的安全性考虑，一方面可以限制来源 IP，只允许内网访问，同时对于一些关键性指令，需要增加访问权限，避免被攻击或误操作时，导致重大后果。","link":"/posts/50198.html"},{"title":"如何动态调整线上日志级别？","text":"作为开发人员，定位问题是我们的日常工作，而日志是我们定位问题非常重要的依据。传统方式定位问题时，往往是如下步骤： 将日志级别设低，例如 DEBUG ； 重启应用； 复现问题，观察日志； 实际上是可以动态修改日志级别，无需重启应用，立即生效。本文收集了3种动态修改日志级别的文章，分别是 Spring Boot 2动态修改日志级别 阿里在线诊断工具Arthas调整日志等级记录 美团日志级别动态调整——小工具解决大问题 Spirng Boot动态修改日志级别从 Spring Boot 1.5 开始，Spring Boot Actuator 组件就已提供动态修改日志级别的能力。 示例1.引入spring-boot-starter-actuator依赖，内容如下： 12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 2.编写测试代码，如下： 1234567891011121314151617181920@SpringBootApplicationpublic class DemoApplication { public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); }}@RestControllerpublic class DemoController { private static Logger logger = LoggerFactory.getLogger(DemoController.class); @GetMapping(&quot;/helloworld&quot;) public String helloworld(){ logger.debug(&quot;welcome to learn spring boot&quot;); return &quot;welcome to learn spring boot&quot;; }} 3.配置文件 12345management: endpoints: web: exposure: include: 'loggers' Spring Boot 2.x默认只暴露 /health 以及 /info 端点，而日志控制需要用到 /loggers 端点，故而需要设置将其暴露。 测试/loggers端点提供了查看以及修改日志级别的能力。 查看当前应用各包/类的日志级别：访问 http://localhost:8080/actuator/loggers ，可看到类似如下的结果： 查看指定包/类日志详情：访问 http://localhost:8080/actuator/loggers/com.blockmao.springboot.demo.DemoController ，可看到类似如下的结果： 修改日志级别：默认的日志级别是INFO，所以DemoController的debug日志不会打印。下面来尝试将该类的日志级别设为DEBUG，如下 此时，访问 http://localhost:8080/helloworld 会看到类似如下的日志： 并且，此时再访问 http://localhost:8080/actuator/loggers/com.itmuch.logging.TestController ，可看到类似如下的结果： 原理Actuator有约定， /actuator/xxx端点的定义代码在 xxxEndpoint中。找到类org.springframework.boot.actuate.logging.LoggersEndpoint ，代码如下： 123456789101112@Endpoint(id = &quot;loggers&quot;)public class LoggersEndpoint { private final LoggingSystem loggingSystem; @WriteOperation public void configureLogLevel(@Selector String name, @Nullable LogLevel configuredLevel) { Assert.notNull(name, &quot;Name must not be empty&quot;); this.loggingSystem.setLogLevel(name, configuredLevel); } // ...其他省略} 其中， Endpoint 、WriteOperation 、@Selector 都是Spring Boot 2.0开始提供的新注解。 @Endpoint(id = &quot;loggers&quot;) 用来描述Spring Boot Actuator 的端点，这样就会产生一个/actuator/loggers 的路径，它类似于Spring MVC的@RequestMapping(&quot;loggers&quot;)。 @WriteOperation 表示这是一个写操作，它类似于Spring MVC的 @PostMapping 。Spring Boot Actuator还提供了其他操作，如下表： Operation HTTP method @ReadOperation GET @WriteOperation POST @DeleteOperation DELETE @Selector用于筛选@Endpoint注解返回值的子集，它类似于Spring MVC的@PathVariable 。 这样，上面的代码就很好理解了— configureLogLevel 方法：送POST请求后，name就是我们传的包名或者类名，configuredLevel就是我们传的消息体。 org.springframework.boot.logging.LoggingSystem#setLogLevel是抽象方法，具体实现由子类完成。LoggingSystem类结构如下图所示： LoggingSystem有这么多实现类，Spring Boot怎么知道什么情况下用什么LoggingSystem呢？可在 org.springframework.boot.logging.LoggingSystem 找到类似如下代码： 1234567891011121314151617181920212223242526272829303132333435public abstract class LoggingSystem { private static final Map&lt;String, String&gt; SYSTEMS; static { Map&lt;String, String&gt; systems = new LinkedHashMap&lt;&gt;(); systems.put(&quot;ch.qos.logback.core.Appender&quot;, &quot;org.springframework.boot.logging.logback.LogbackLoggingSystem&quot;); systems.put(&quot;org.apache.logging.log4j.core.impl.Log4jContextFactory&quot;, &quot;org.springframework.boot.logging.log4j2.Log4J2LoggingSystem&quot;); systems.put(&quot;java.util.logging.LogManager&quot;, &quot;org.springframework.boot.logging.java.JavaLoggingSystem&quot;); SYSTEMS = Collections.unmodifiableMap(systems); } /** * Detect and return the logging system in use. Supports Logback and Java Logging. * @param classLoader the classloader * @return the logging system */ public static LoggingSystem get(ClassLoader classLoader) { String loggingSystem = System.getProperty(SYSTEM_PROPERTY); if (StringUtils.hasLength(loggingSystem)) { if (NONE.equals(loggingSystem)) { return new NoOpLoggingSystem(); } return get(classLoader, loggingSystem); } return SYSTEMS.entrySet().stream() .filter((entry) -&gt; ClassUtils.isPresent(entry.getKey(), classLoader)) .map((entry) -&gt; get(classLoader, entry.getValue())).findFirst() .orElseThrow(() -&gt; new IllegalStateException( &quot;No suitable logging system located&quot;)); } // 省略不相关内容...} 由代码不难发现，其实就是构建了一个名为 SYSTEMS 的map，作为各种日志系统的字典；然后在 get 方法中，看应用是否加载了map中的类；如果加载了，就通过反射，初始化LoggingSystem 。例如：Spring Boot发现当前应用加载了ch.qos.logback.core.Appender ，就去实例化 org.springframework.boot.logging.logback.LogbackLoggingSystem 。 Arthas ognl命令动态修改日志级别使用ognl命令可以动态修改日志级别，步骤如下： 查找当前类的classLoaderHash 用OGNL获取logger 可以发现日志使用的是Logback框架。 单独设置DemoController的logger level 全局设置logger level 如果使用的日志框架是log4j，则使用上述ognl命令则会报错。至于为什么？请阅读Java日志：SLF4J详解 美团日志级别动态调整小工具 初始化：确定所使用的日志框架，获取配置文件中所有的Logger内存实例，并将它们的引用缓存到Map容器中。 12345678910111213141516171819202122232425262728293031323334353637String type = StaticLoggerBinder.getSingleton().getLoggerFactoryClassStr(); if (LogConstant.LOG4J_LOGGER_FACTORY.equals(type)) { logFrameworkType = LogFrameworkType.LOG4J; Enumeration enumeration = org.apache.log4j.LogManager.getCurrentLoggers(); while (enumeration.hasMoreElements()) { org.apache.log4j.Logger logger = (org.apache.log4j.Logger) enumeration.nextElement(); if (logger.getLevel() != null) { loggerMap.put(logger.getName(), logger); } } org.apache.log4j.Logger rootLogger = org.apache.log4j.LogManager.getRootLogger(); loggerMap.put(rootLogger.getName(), rootLogger); } else if (LogConstant.LOGBACK_LOGGER_FACTORY.equals(type)) { logFrameworkType = LogFrameworkType.LOGBACK; ch.qos.logback.classic.LoggerContext loggerContext = (ch.qos.logback.classic.LoggerContext) LoggerFactory.getILoggerFactory(); for (ch.qos.logback.classic.Logger logger : loggerContext.getLoggerList()) { if (logger.getLevel() != null) { loggerMap.put(logger.getName(), logger); } } ch.qos.logback.classic.Logger rootLogger = (ch.qos.logback.classic.Logger) LoggerFactory.getLogger(Logger.ROOT_LOGGER_NAME); loggerMap.put(rootLogger.getName(), rootLogger); } else if (LogConstant.LOG4J2_LOGGER_FACTORY.equals(type)) { logFrameworkType = LogFrameworkType.LOG4J2; org.apache.logging.log4j.core.LoggerContext loggerContext = (org.apache.logging.log4j.core.LoggerContext) org.apache.logging.log4j.LogManager.getContext(false); Map&lt;String, org.apache.logging.log4j.core.config.LoggerConfig&gt; map = loggerContext.getConfiguration().getLoggers(); for (org.apache.logging.log4j.core.config.LoggerConfig loggerConfig : map.values()) { String key = loggerConfig.getName(); if (StringUtils.isBlank(key)) { key = &quot;root&quot;; } loggerMap.put(key, loggerConfig); } } else { logFrameworkType = LogFrameworkType.UNKNOWN; LOG.error(&quot;Log框架无法识别: type={}&quot;, type); } 获取Logger列表：从本地Map容器取出 12345678910111213141516171819202122232425private String getLoggerList() { JSONObject result = new JSONObject(); result.put(&quot;logFramework&quot;, logFrameworkType); JSONArray loggerList = new JSONArray(); for (ConcurrentMap.Entry&lt;String, Object&gt; entry : loggerMap.entrySet()) { JSONObject loggerJSON = new JSONObject(); loggerJSON.put(&quot;loggerName&quot;, entry.getKey()); if (logFrameworkType == LogFrameworkType.LOG4J) { org.apache.log4j.Logger targetLogger = (org.apache.log4j.Logger) entry.getValue(); loggerJSON.put(&quot;logLevel&quot;, targetLogger.getLevel().toString()); } else if (logFrameworkType == LogFrameworkType.LOGBACK) { ch.qos.logback.classic.Logger targetLogger = (ch.qos.logback.classic.Logger) entry.getValue(); loggerJSON.put(&quot;logLevel&quot;, targetLogger.getLevel().toString()); } else if (logFrameworkType == LogFrameworkType.LOG4J2) { org.apache.logging.log4j.core.config.LoggerConfig targetLogger = (org.apache.logging.log4j.core.config.LoggerConfig) entry.getValue(); loggerJSON.put(&quot;logLevel&quot;, targetLogger.getLevel().toString()); } else { loggerJSON.put(&quot;logLevel&quot;, &quot;Logger的类型未知,无法处理!&quot;); } loggerList.add(loggerJSON); } result.put(&quot;loggerList&quot;, loggerList); LOG.info(&quot;getLoggerList: result={}&quot;, result.toString()); return result.toString();} 修改Logger的级别 12345678910111213141516171819202122232425262728293031private String setLogLevel(JSONArray data) { LOG.info(&quot;setLogLevel: data={}&quot;, data); List&lt;LoggerBean&gt; loggerList = parseJsonData(data); if (CollectionUtils.isEmpty(loggerList)) { return &quot;&quot;; } for (LoggerBean loggerbean : loggerList) { Object logger = loggerMap.get(loggerbean.getName()); if (logger == null) { throw new RuntimeException(&quot;需要修改日志级别的Logger不存在&quot;); } if (logFrameworkType == LogFrameworkType.LOG4J) { org.apache.log4j.Logger targetLogger = (org.apache.log4j.Logger) logger; org.apache.log4j.Level targetLevel = org.apache.log4j.Level.toLevel(loggerbean.getLevel()); targetLogger.setLevel(targetLevel); } else if (logFrameworkType == LogFrameworkType.LOGBACK) { ch.qos.logback.classic.Logger targetLogger = (ch.qos.logback.classic.Logger) logger; ch.qos.logback.classic.Level targetLevel = ch.qos.logback.classic.Level.toLevel(loggerbean.getLevel()); targetLogger.setLevel(targetLevel); } else if (logFrameworkType == LogFrameworkType.LOG4J2) { org.apache.logging.log4j.core.config.LoggerConfig loggerConfig = (org.apache.logging.log4j.core.config.LoggerConfig) logger; org.apache.logging.log4j.Level targetLevel = org.apache.logging.log4j.Level.toLevel(loggerbean.getLevel()); loggerConfig.setLevel(targetLevel); org.apache.logging.log4j.core.LoggerContext ctx = (org.apache.logging.log4j.core.LoggerContext) org.apache.logging.log4j.LogManager.getContext(false); ctx.updateLoggers(); // This causes all Loggers to refetch information from their LoggerConfig. } else { throw new RuntimeException(&quot;Logger的类型未知,无法处理!&quot;); } } return &quot;success&quot;; }","link":"/posts/9693.html"},{"title":"微服务安全架构：基于OAuth 2.0&#x2F;JWT的微服务参考架构","text":"[]: https://time.geekbang.org/column/article/257837?utm_source=related_read&amp;utm_medium=article&amp;utm_term=related_read “架构案例：基于OAuth 2.0/JWT的微服务参考架构”","link":"/posts/7199.html"},{"title":"彻底搞懂Git-Rebase","text":"原文地址：http://jartto.wang/2018/12/11/git-rebase/# 原文作者：Jartto’s blog 使用 Git 已经好几年了，却始终只是熟悉一些常用的操作。对于 Git Rebase 却很少用到，直到这一次，不得不用。 一、起因上线构建的过程中扫了一眼代码变更，突然发现，commit 提交竟然多达 62 次。我们来看看都提交了什么东西： 这里我们先不说 git 提交规范，就单纯这么多次无用的 commit 就很让人不舒服。可能很多人觉得无所谓，无非是多了一些提交纪录。 然而，并非如此，你可能听过破窗效应，编程也是如此！ 二、导致问题 不利于代码 review 设想一下，你要做 code review ，结果一个很小的功能，提交了 60 多次，会不会有一些崩溃？ 会造成分支污染 你的项目充满了无用的 commit 纪录，如果有一天线上出现了紧急问题，你需要回滚代码，却发现海量的 commit 需要一条条来看。 遵循项目规范才能提高团队协作效率，而不是随心所欲。 三、Rebase 场景一：如何合并多次提交纪录？基于上面所说问题，我们不难想到：每一次功能开发， 对多个 commit 进行合并处理。 这时候就需要用到 git rebase 了。这个命令没有太难，不常用可能源于不熟悉，所以我们来通过示例学习一下。 我们来合并最近的 4 次提交纪录，执行： 1git rebase -i HEAD~4 这时候，会自动进入 vi 编辑模式： 12345678910111213141516171819202122s cacc52da add: qrcodes f072ef48 update: indexeddb hacks 4e84901a feat: add indexedDB floders 8f33126c feat: add test2.js# Rebase 5f2452b2..8f33126c onto 5f2452b2 (4 commands)## Commands:# p, pick = use commit# r, reword = use commit, but edit the commit message# e, edit = use commit, but stop for amending# s, squash = use commit, but meld into previous commit# f, fixup = like &quot;squash&quot;, but discard this commit's log message# x, exec = run command (the rest of the line) using shell# d, drop = remove commit## These lines can be re-ordered; they are executed from top to bottom.## If you remove a line here THAT COMMIT WILL BE LOST.## However, if you remove everything, the rebase will be aborted.# 有几个命令需要注意一下： p, pick = use commit r, reword = use commit, but edit the commit message e, edit = use commit, but stop for amending s, squash = use commit, but meld into previous commit f, fixup = like “squash”, but discard this commit’s log message x, exec = run command (the rest of the line) using shell d, drop = remove commit pick：保留该commit（缩写:p） reword：保留该commit，但我需要修改该commit的注释（缩写:r） edit：保留该commit, 但我要停下来修改该提交(不仅仅修改注释)（缩写:e） squash：将该commit和前一个commit合并（缩写:s） fixup：将该commit和前一个commit合并，但我不要保留该提交的注释信息（缩写:f） exec：执行shell命令（缩写:x） drop：我要丢弃该commit（缩写:d） 按照如上命令来修改你的提交纪录： 1234s cacc52da add: qrcodes f072ef48 update: indexeddb hacks 4e84901a feat: add indexedDB floderp 8f33126c feat: add test2.js 如果保存的时候，你碰到了这个错误： 1error: cannot 'squash' without a previous commit 注意不要合并先前提交的东西，也就是已经提交远程分支的纪录。 如果你异常退出了 vi 窗口，不要紧张： 1git rebase --edit-todo 这时候会一直处在这个编辑的模式里，我们可以回去继续编辑，修改完保存一下： 1git rebase --continue 查看结果 1git log 三次提交合并成了一次，减少了无用的提交信息。 四、Rebase 场景二：分支合并 我们先从 master 分支切出一个 dev 分支，进行开发： 1git:(master) git checkout -b feature1 2. 这时候，你的同事完成了一次 hotfix，并合并入了 master 分支，此时 master 已经领先于你的 feature1 分支了：3. 恰巧，我们想要同步 master 分支的改动，首先想到了 merge，执行： 1git:(feature1) git merge master 图中绿色的点就是我们合并之后的结果，执行： 1git:(feature1) git log 就会在记录里发现一些 merge 的信息，但是我们觉得这样污染了 commit 记录，想要保持一份干净的 commit，怎么办呢？这时候，git rebase 就派上用场了。 让我们来试试 git rebase ，先回退到同事 hotfix 后合并 master 的步骤： 使用 rebase 后来看看结果： 1git:(feature1) git rebase master 这里补充一点：rebase 做了什么操作呢？ 首先，git 会把 feature1 分支里面的每个 commit 取消掉；其次，把上面的操作临时保存成 patch 文件，存在 .git/rebase 目录下；然后，把 feature1 分支更新到最新的 master 分支；最后，把上面保存的 patch 文件应用到 feature1 分支上； 从 commit 记录我们可以看出来，feature1 分支是基于 hotfix 合并后的 master ，自然而然的成为了最领先的分支，而且没有 merge 的 commit 记录，是不是感觉很舒服了。 在 rebase 的过程中，也许会出现冲突 conflict。在这种情况，git 会停止 rebase 并会让你去解决冲突。在解决完冲突后，用 git add 命令去更新这些内容。 注意，你无需执行 git-commit，只要执行 continue 1git rebase --continue 这样 git 会继续应用余下的 patch 补丁文件。 在任何时候，我们都可以用 --abort 参数来终止 rebase 的行动，并且分支会回到 rebase 开始前的状态。 1git rebase —abort 五、更多 Rebase 的使用场景git-rebase 存在的价值是：对一个分支做「变基」操作。 当我们在一个过时的分支上面开发的时候，执行 rebase 以此同步 master 分支最新变动； 假如我们要启动一个放置了很久的并行工作，现在有时间来继续这件事情，很显然这个分支已经落后了。这时候需要在最新的基准上面开始工作，所以 rebase 是最合适的选择。 六、为什么会是危险操作？根据上文来看，git-rebase 很完美，解决了我们的两个问题： 合并 commit 记录，保持分支整洁； 相比 merge 来说会减少分支合并的记录； 如果你提交了代码到远程，提交前是这样的： 提交后远程分支变成了这样： 而此时你的同事也在 feature1 上开发，他的分支依然还是： 那么当他 pull 远程 master 的时候，就会有丢失提交纪录。这就是为什么我们经常听到有人说 git rebase 是一个危险命令，因为它改变了历史，我们应该谨慎使用。 除非你可以肯定该 feature1 分支只有你自己使用，否则请谨慎操作。 结论：只要你的分支上需要 rebase 的所有 commits 历史还没有被 push 过，就可以安全地使用 git-rebase来操作。 七、参考： rebase git-rebase 使用总结 git 中的 rebase 操作 git-rebase vs git-merge 详解","link":"/posts/27252.html"},{"title":"数据结构与算法：二分搜索","text":"原理二分搜索（折半搜索）的 Wikipedia 定义： 是一种在有序数组中查找某一特定元素的搜索算法。从定义可知，运用二分搜索的前提是数组必须是排好序的。另外，输入并不一定是数组，也有可能是给定一个区间的起始和终止的位置。 优点：时间复杂度是 O(lgn)，非常高效。 缺点：要求待查找的数组或者区间是排好序的。 二分搜索一般化的解题思路如下： 从已经排好序的数组或区间中取出中间位置的元素，判断该元素是否满足要搜索的条件，如果满足，停止搜索，程序结束。 如果正中间的元素不满足条件，则从它两边的区域进行搜索。由于数组是排好序的，可以利用排除法，确定接下来应该从这两个区间中的哪一个去搜索。 通过判断，如果发现真正要找的元素在左半区间的话，就继续在左半区间里进行二分搜索。反之，就在右半区间里进行二分搜索。 实现假设我们要从一个排好序的数组里 {1, 3, 4, 6, 7, 8, 10, 13, 14} 查看一下数字 7 是否在里面，如果在，返回它的下标，否则返回 -1。 递归解法优点：简洁；缺点：执行消耗大。代码模板如下： 1234567891011121314151617181920212223// 二分搜索函数的定义里，除了要指定数组 nums 和目标查找数 target 之外，// 还要指定查找区间的起点和终点位置，分别用 low 和 high 去表示。int binarySearch(int[] nums, int target, int low, int high) { // 为了避免无限循环，先判断，如果起点位置大于终点位置，表明这是一个非法的区间， // 已经尝试了所有的搜索区间还是没能找到结果，返回 -1。 if (low &gt; high) { return -1; } // 取正中间那个数的下标 middle。 int middle = low + (high - low) / 2; // 判断一下正中间的那个数是不是要找的目标数 target，是，就返回下标 middle。 if (nums[middle] == target) { return middle; } // 如果发现目标数在左边，就递归地从左半边进行二分搜索。 if (target &lt; nums[middle]) { return binarySearch(nums, target, low, middle - 1); } else { //否则从右半边递归地进行二分搜索。 return binarySearch(nums, target, middle + 1, high); }} 非递归解法非递归写法的代码模板如下： 12345678910111213141516171819202122int binarySearch(int[] nums, int target, int low, int high) { // 在 while 循环里，判断搜索的区间范围是否有效 while (low &lt;= high) { // 计算正中间的数的下标 int middle = low + (high - low) / 2; // 判断正中间的那个数是不是要找的目标数 target。如果是，就返回下标 middle if (nums[middle] == target) { return middle; } // 如果发现目标数在左边，调整搜索区间的终点为 middle - 1； // 否则，调整搜索区间的起点为 middle + 1 if (target &lt; nums[middle]) { high = middle - 1; } else { low = middle + 1; } } // 如果超出了搜索区间，表明无法找到目标数，返回 -1 return -1;} 场景例题分析一：找确定的边界边界分上边界和下边界，有时候也被成为右边界和左边界。确定的边界指边界的数值等于要找的目标数。 LeetCode 第 34 题： 解题思路 在二分搜索里，比较难的是判断逻辑，对这道题来说，什么时候知道这个位置是不是 8 第一次以及最后出现的地方呢？ 把第一次出现的地方叫下边界（lower bound），把最后一次出现的地方叫上边界（upper bound）。 那么成为 8 的下边界的条件应该有两个。 该数必须是 8； 该数的左边一个数必须不是 8： 8 的左边有数，那么该数必须小于 8； 8 的左边没有数，即 8 是数组的第一个数。 而成为 8 的上边界的条件也应该有两个。 该数必须是 8； 该数的右边一个数必须不是 8： 8 的右边有数，那么该数必须大于8； 8 的右边没有数，即 8 是数组的最后一个数。 用递归的方法来寻找下边界，代码如下。 123456789101112131415161718int searchLowerBound(int[] nums, int target, int low, int high) { if (low &gt; high) { return -1; } int middle = low + (high - low) / 2; // 判断是否是下边界时，先看看 middle 的数是否为 target，并判断该数是否已为数组的第一个数， // 或者，它左边的一个数是不是已经比它小，如果都满足，即为下边界。 if (nums[middle] == target &amp;&amp; (middle == 0 || nums[middle - 1] &lt; target)) { return middle; } if (target &lt;= nums[middle]) { return searchLowerBound(nums, target, low, middle - 1); } else { return searchLowerBound(nums, target, middle + 1, high); } //不满足，如果这个数等于 target，那么就得往左边继续查找。} 查找上边界的代码如下 12345678910111213141516171819int searchUpperBound(int[] nums, int target, int low, int high) { if (low &gt; high) { return -1; } int middle = low + (high - low) / 2; // 判断是否是上边界时，先看看 middle 的数是否为 target，并判断该数是否已为数组的最后一个数 // ，或者，它右边的数是不是比它大，如果都满足，即为上边界。 if (nums[middle] == target &amp;&amp; (middle == nums.length - 1 || nums[middle + 1] &gt; target)) { return middle; } if (target &lt; nums[middle]) { return searchUpperBound(nums, target, low, middle - 1); } else { return searchUpperBound(nums, target, middle + 1, high); } //不满足时，需判断搜索方向。} 例题分析二：找模糊的边界二分搜索可以用来查找一些模糊的边界。模糊的边界指，边界的值并不等于目标的值，而是大于或者小于目标的值。 例题：从数组 {-2, 0, 1, 4, 7, 9, 10} 中找到第一个大于 6 的数。 解题思路 在一个排好序的数组里，判断一个数是不是第一个大于 6 的数，只要它满足如下的条件： 该数要大于 6； 该数有可能是数组里的第一个数，或者它之前的一个数比 6 小。 只要满足了上面的条件就是第一个大于 6 的数。 1234567891011121314151617181920Integer firstGreaterThan(int[] nums, int target, int low, int high) { if (low &gt; high) { return null; } int middle = low + (high - low) / 2; // 判断 middle 指向的数是否为第一个比 target 大的数时，须同时满足两个条件：middle 这个数必须大于 // target；middle 要么是第一个数，要么它之前的数小于或者等于 target。 if (nums[middle] &gt; target &amp;&amp; (middle == 0 || nums[middle - 1] &lt;= target)) { return middle; } if (target &lt; nums[middle]) { return firstGreaterThan(nums, target, low, middle - 1); } else { return firstGreaterThan(nums, target, middle + 1, high); }} 例题分析三：旋转过的排序数组LeetCode 第 33 题 解题思路 对于这道题，输入数组不是完整排好序，还能运用二分搜索吗？思路如下。 一开始，中位数是 7，并不是我们要找的 0，如何判断往左边还是右边搜索呢？ 这个数组是经过旋转的，即，从数组中的某个位置开始划分，左边和右边都是排好序的。如何判断左边是不是排好序的那个部分呢？ 只要比较 nums[low] 和 nums[middle]。nums[low] &lt;= nums[middle] 时，能判定左边这部分一定是排好序的，否则，右边部分一定是排好序的。 为什么要判断 nums[low] = nums[middle] 的情况呢？因为计算 middle 的公式是 int middle = low + (high - low) / 2。 当只有一个数的时候，low=high，middle=ow，同样认为这一边是排好序的。 判定某一边是排好序的，有什么用处呢？ 能准确地判断目标值是否在这个区间里。如果 nums[low] &lt;= target &amp;&amp; target &lt; nums[middle]，则应该在这个区间里搜索目标值。反之，目标值肯定在另外一边。 123456789101112131415161718192021222324int binarySearch(int[] nums, int target, int low, int high) { if (low &gt; high) { return -1; } //判断是否已超出了搜索范围，是则返回-1。 int middle = low + (high - low) / 2; //取中位数。 if (nums[middle] == target) { return middle; } //判断中位数是否为要找的数 if (nums[low] &lt;= nums[middle]) { //判断左半边是不是排好序的。 if (nums[low] &lt;= target &amp;&amp; target &lt; nums[middle]) { //是，则判断目标值是否在左半边。 return binarySearch(nums, target, low, middle - 1); //是，则在左半边继续进行二分搜索。 } return binarySearch(nums, target, middle + 1, high); //否，在右半边进行二分搜索。 } else { if (nums[middle] &lt; target &amp;&amp; target &lt;= nums[high]) { //若右半边是排好序的那一半，判断目标值是否在右边。 return binarySearch(nums, target, middle + 1, high); //是，则在右半边继续进行二分搜索。 } return binarySearch(nums, target, low, middle - 1); //否，在左半边进行二分搜索。 }} 例题分析四：不定长的边界那么对于没有给定明确区间的问题能不能运用二分搜索呢？ 例题：有一段不知道具体长度的日志文件，里面记录了每次登录的时间戳，已知日志是按顺序从头到尾记录的，没有记录日志的地方为空，要求当前日志的长度。 解题思路 可以把这个问题看成是不知道长度的数组，数组从头开始记录都是时间戳，到了某个位置就成为了空：{2019-01-14, 2019-01-17, … , 2019-08-04, …. , null, null, null …}。 思路 1：顺序遍历该数组，一直遍历下去，当发现第一个 null 的时候，就知道了日志的总数量。很显然，这是很低效的办法。 思路 2：借用二分搜索的思想，反着进行搜索。 一开始设置 low = 0，high = 1 只要 logs[high] 不为 null，high *= 2 当 logs[high] 为 null 的时候，可以在区间 [0, high] 进行普通的二分搜索 1234567891011121314151617181920212223242526// 先通过getUpperBound函数不断地去试探在什么位置会出现空的日志。int getUpperBound(String[] logs, int high) { if (logs[high] == null) { return high; } return getUpperBound(logs, high * 2);}// 再运用二分搜索的方法去寻找日志的长度。int binarySearch(String[] logs, int low, int high) { if (low &gt; high) { return -1; } int middle = low + (high - low) / 2; if (logs[middle] == null &amp;&amp; logs[middle - 1] != null) { return middle; } if (logs[middle] == null) { return binarySearch(logs, low, middle - 1); } else { return binarySearch(logs, middle + 1, high); }}","link":"/posts/54287.html"},{"title":"数据结构与算法：排序","text":"概述排序算法分类 排序算法总结 冒泡排序（Bubble Sort）思路嵌套循环，每次查看相邻的元素如果逆序，则交换。 动图演示 代码实现12345678910111213141516171819202122232425public int[] bubbleSort(int[] nums) { // 标记每轮遍历中是否发生了交换 boolean hasChange = true; int length = nums.length; for (int i = 0; i &lt; length - 1; i++) { for (int j = 0; j &lt; length - 1 - i; j++) { // 如果当前元素比下一个数大，则交换元素位置 if (nums[j] &gt; nums[j + 1]) { swap(nums, j, j + 1); hasChange = false; } } if (hasChange) { break; } } return nums;}private void swap(int[] nums, int i, int j) { int tmp = nums[i]; nums[i] = nums[j]; nums[j] = tmp;} 选择排序思路每次找最小值，然后放到待排序数组的起始位置。 动图演示 代码实现123456789101112131415161718public int[] selectionSort(int[] nums) { int length = nums.length; for (int i = 0; i &lt; length - 1; i++) { int min = i; for (int j = i + 1; j &lt; length; j++) { if (nums[j] &lt; nums[min]) { min = j; } } if (i != min) { int tmp = nums[i]; nums[min] = nums[i]; nums[i] = tmp; } } return nums;} 插入排序（Insertion Sort）思路从前到后逐步构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置插入。 动图演示 代码实现123456789101112131415161718public int[] insertionSort(int[] nums) { int length = nums.length; // 从下标为1的元素开始选择合适的位置插入，因为下标为0的只有一个元素，默认是有序的 for (int i = 1; i &lt; length; i++) { // 记录要插入数据 int temp = nums[i]; // 从已经排序的序列最右边的开始比较，找到比其小的数 int j = i; while (j &gt; 0 &amp;&amp; temp &lt; nums[j - 1]) { nums[j] = nums[j - 1]; } // 存在比其小的数，插入 if (j != i) { nums[j] = temp; } } return nums;} 快速排序（Quick Sort）思路数组取基准pivot，将小元素放pivot左边，大元素放右边，然后依次对右边和左边的子数组继续快排，以达到整个序列有序。 动图演示 代码实现1234567891011121314151617181920212223242526272829303132public void quickSort(int[] nums, int begin, int end) { if (begin &gt;= end) { return; } // 基准元素位置索引 int pivot = partition(nums, begin, end); quickSort(nums, left, pivot - 1); quickSort(nums, pivot + 1, right);}private int partition(int[] nums, int begin, int end) { // select pivot index int pivot = end; // 记录小于pivot元素的位置 int counter = begin; // 从左到右用每个数和基准值比较，若比基准值小，则放到指针counter所指向的位置。循环完毕后，counter指针之前的数都比基准值小 for (int i = begin; i &lt; end; i++) { if (nums[i] &lt;= nums[pivot]) { swap(nums, counter++, i); } } // 末尾的基准值放置到指针counter的位置，counter指针之后的数都比基准值大 swap(nums, pivot, counter); return i;}private void swap(int[] nums, int i, int j) { int temp = nums[i]; nums[i] = nums[j]; nums[j] = temp;} 归并排序（Merge Sort）思路 将长度为n的输入序列分成两个长度为n/2的子序列； 分别对两个子序列采用归并排序； 合并两个排序好的子序列； 动图演示 代码实现12345678910111213141516171819202122232425262728293031323334public void mergeSort(int[] nums, int low, int high) { // 分治 if (low &gt;= high) {return;} int mid = low + (high - low) &gt;&gt; 1; mergeSort(nums, low, mid); mergeSort(nums, mid + 1, high); mergeTwoArrays(nums, low, mid, high);}private void mergeTwoArrays(int[] nums, int low, int mid, int high) { // 临时数组，存储结果数据 int[] temp = new int[high - low + 1]; int index = 0; int i = low, j = mid + 1; while (i &lt;= mid &amp;&amp; j &lt;= high) { if (nums[i] &lt; nums[j]) { temp[index++] = nums[i++]; } else { temp[index++] = nums[j++]; } } while (i &lt;= mid) { temp[index++] = nums[i++]; } while (j &lt;= high) { temp[index++] = nums[j++]; } for (int k = 0; k &lt; index; k++) { nums[low + k] = temp[k]; }} 堆排序（Heap Sort）思路 数组元素一次建立小顶堆； 依次取堆顶元素，并删除。 动画演示 代码实现123456789101112131415161718192021222324252627282930313233343536public void heapSort(int[] nums) { if (nums.length == 0) return; int length = nums.length; // 构建堆 for (int i = length / 2 - 1; i &gt;= 0; i--) { heapify(nums, length, i); } // 依次取堆顶元素，并删除 for (int i = length - 1; i &gt;= 0; i--) { int temp = nums[0]; nums[0] = nums[i]; nums[i] = temp; heapify(nums, i, 0); }}private void heapify(int[] nums, int length, int i) { int left = 2 * i + 1, right = 2 * i + 2; int largest = i; if (left &lt; length &amp;&amp; nums[left] &gt; nums[largest]) { largest = left; } if (right &lt; length &amp;&amp; nums[right] &gt; nums[largest]) { largest = right; } if (largest != i) { int temp = nums[i]; nums[i] = nums[largest]; nums[largest] = temp; heapify(nums, length, largest); }}","link":"/posts/61276.html"},{"title":"理解OAuth 2.0","text":"OAuth 2解决什么问题有一个”云冲印”的网站，可以将用户储存在Google的照片，冲印出来。用户为了使用该服务，必须让”云冲印”读取自己储存在Google上的照片。 问题是只有得到用户的授权，Google才会同意”云冲印”读取这些照片。那么，”云冲印”怎样获得用户的授权呢？ 传统方法是，用户将自己的Google用户名和密码，告诉”云冲印”，后者就可以读取用户的照片了。这样的做法有以下几个严重的缺点。 “云冲印”为了后续的服务，会保存用户的密码，这样很不安全。 Google不得不部署密码登录，而我们知道，单纯的密码登录并不安全。 “云冲印”拥有了获取用户储存在Google所有资料的权力，用户没法限制”云冲印”获得授权的范围和有效期。 用户只有修改密码，才能收回赋予”云冲印”的权力。但是这样做，会使得其他所有获得用户授权的第三方应用程序全部失效。 只要有一个第三方应用程序被破解，就会导致用户密码泄漏，以及所有被密码保护的数据泄漏。 OAuth 2.0就是为了解决上面这些问题而诞生的。 术语 Resource Owner：资源所有者，本文中又称”用户”（user）。 Third-party application：第三方应用程序，本文中又称”客户端”（client）。 Authorization server：认证服务器，即服务提供商专门用来处理认证的服务器。 Resource server：资源服务器，即服务提供商存放用户生成的资源的服务器。它与认证服务器，可以是同一台服务器，也可以是不同的服务器。 OAuth 2的定义 OAuth 2应用场景 OAuth 2优势 OAuth 2运行流程OAuth 2.0的运行流程如下图，摘自RFC 6749。 （A）用户打开客户端以后，客户端要求用户给予授权。 （B）用户同意给予客户端授权。 （C）客户端使用上一步获得的授权，向认证服务器申请令牌。 （D）认证服务器对客户端进行认证以后，确认无误，同意发放令牌。 （E）客户端使用令牌，向资源服务器申请获取资源。 （F）资源服务器确认令牌无误，同意向客户端开放资源。 不难看出来，上面六个步骤之中，B是关键，即用户怎样才能给于客户端授权。有了这个授权以后，客户端就可以获取令牌，进而凭令牌获取资源。 客户端的授权模式客户端必须得到用户的授权（authorization grant），才能获得令牌（access token）。OAuth 2.0定义了四种授权方式。 授权码模式（authorization code） 简化模式（implicit） 密码模式（resource owner password credentials） 客户端模式（client credentials） 授权码模式授权码模式（authorization code）是功能最完整、流程最严密的授权模式。它的特点就是通过客户端的后台服务器，与”服务提供商”的认证服务器进行互动。 （A）用户访问客户端，后者将前者导向认证服务器。 （B）用户选择是否给予客户端授权。 （C）假设用户给予授权，认证服务器将用户导向客户端事先指定的”重定向URI”（redirection URI），同时附上一个授权码。 （D）客户端收到授权码，附上早先的”重定向URI”，向认证服务器申请令牌。这一步是在客户端的后台的服务器上完成的，对用户不可见。 （E）认证服务器核对了授权码和重定向URI，确认无误后，向客户端发送访问令牌（access token）和更新令牌（refresh token）。 简化模式简化模式（implicit grant type）不通过第三方应用程序的服务器，直接在浏览器中向认证服务器申请令牌，跳过了”授权码”这个步骤，因此得名。所有步骤在浏览器中完成，令牌对访问者是可见的，且客户端不需要认证。 （A）客户端将用户导向认证服务器。 （B）用户决定是否给于客户端授权。 （C）假设用户给予授权，认证服务器将用户导向客户端指定的”重定向URI”，并在URI的Hash部分包含了访问令牌。 （D）浏览器向资源服务器发出请求，其中不包括上一步收到的Hash值。 （E）资源服务器返回一个网页，其中包含的代码可以获取Hash值中的令牌。 （F）浏览器执行上一步获得的脚本，提取出令牌。 （G）浏览器将令牌发给客户端。 密码模式密码模式（Resource Owner Password Credentials Grant）中，用户向客户端提供自己的用户名和密码。客户端使用这些信息，向”服务商提供商”索要授权。 （A）用户向客户端提供用户名和密码。 （B）客户端将用户名和密码发给认证服务器，向后者请求令牌。 （C）认证服务器确认无误后，向客户端提供访问令牌。 客户端模式客户端模式（Client Credentials Grant）指客户端以自己的名义，而不是以用户的名义，向”服务提供商”进行认证。严格地说，客户端模式并不属于OAuth框架所要解决的问题。在这种模式中，用户直接向客户端注册，客户端以自己的名义要求”服务提供商”提供服务，其实不存在授权问题。 （A）客户端向认证服务器进行身份认证，并要求一个访问令牌。 （B）认证服务器确认无误后，向客户端提供访问令牌。 如何选择授权模式 ？ 刷新令牌刷新令牌由授权服务器颁发给客户端，用于当前访问令牌变得无效或过期时获取新的访问令牌，简化重新授权流程。","link":"/posts/51457.html"},{"title":"计算机网络：HTTPS协议详解","text":"为什么需要HTTPS？HTTP 由于是明文传输，所谓的明文，就是说客户端与服务端通信的信息都是肉眼可见的，随意使用一个抓包工具都可以截获通信的内容。 所以安全上存在以下三个风险： 窃听风险，比如通信链路上可以获取通信内容。 篡改风险，比如强制植入垃圾广告，视觉污染。 冒充风险，比如冒充淘宝网站。 HTTPS 在 HTTP 与 TCP 层之间加入了 TLS 协议，来解决上述的风险。如下图： TLS 协议是如何解决 HTTP 的风险的呢？ 信息加密：HTTP 交互信息是被加密的，第三方就无法被窃取； 校验机制：校验信息传输过程中是否有被第三方篡改过，如果被篡改过，则会有警告提示； 身份证书：证明淘宝是真的淘宝网； 可见，有了 TLS 协议，能保证 HTTP 通信是安全的了，那么在进行 HTTP 通信前，需要先进行 TLS 握手。 TLS 协议的组成TLS 包含几个子协议，也可以理解为它是由几个不同职责的模块组成，比较常用的有记录协议、警报协议、握手协议、变更密码规范协议等。 记录协议（Record Protocol）规定了 TLS 收发数据的基本单位：记录（record）。它有点像是 TCP 里的 segment，所有的其他子协议都需要通过记录协议发出。但多个记录数据可以在一个 TCP 包里一次性发出，也并不需要像 TCP 那样返回 ACK。 警报协议（Alert Protocol）的职责是向对方发出警报信息，有点像是 HTTP 协议里的状态码。比如，protocol_version 就是不支持旧版本，bad_certificate 就是证书有问题，收到警报后另一方可以选择继续，也可以立即终止连接。 握手协议（Handshake Protocol）是 TLS 里最复杂的子协议，要比 TCP 的 SYN/ACK 复杂的多，浏览器和服务器会在握手过程中协商 TLS 版本号、随机数、密码套件等信息，然后交换证书和密钥参数，最终双方协商得到会话密钥，用于后续的混合加密系统。 最后一个是变更密码规范协议（Change Cipher Spec Protocol），它非常简单，就是一个“通知”，告诉对方，后续的数据都将使用加密保护。那么反过来，在它之前，数据都是明文的。 TLS 的简要握手过程，如下图： 其中每一个“框”都是一个记录，多个记录组合成一个 TCP 包发送。所以，最多经过两次消息往返（4 个消息）就可以完成握手，然后就可以在安全的通信环境里发送 HTTP 报文，实现 HTTPS 协议。 ECDHE 握手过程 在 TCP 建立连接之后，浏览器会首先发一个“Client Hello”消息，也就是跟服务器“打招呼”。里面有客户端的版本号、支持的密码套件，还有一个随机数（Client Random），用于后续生成会话密钥。 123456Handshake Protocol: Client Hello Version: TLS 1.2 (0x0303) Random: 1cbf803321fd2623408dfe… Cipher Suites (17 suites) Cipher Suite: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 (0xc02f) Cipher Suite: TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 (0xc030) 服务器收到“Client Hello”后，会返回一个“Server Hello”消息。把版本号对一下，也给出一个随机数（Server Random），然后从客户端的列表里选一个作为本次通信使用的密码套件。 1234Handshake Protocol: Server Hello Version: TLS 1.2 (0x0303) Random: 0e6320f21bae50842e96… Cipher Suite: TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 (0xc030) 服务器为了证明自己的身份，就把证书也发给了客户端（Server Certificate）。 因为服务器选择了 ECDHE 算法，所以它会在证书后发送“Server Key Exchange”消息，里面是椭圆曲线的公钥（Server Params），用来实现密钥交换算法，再加上自己的私钥签名认证。 1234567Handshake Protocol: Server Key Exchange EC Diffie-Hellman Server Params Curve Type: named_curve (0x03) Named Curve: x25519 (0x001d) Pubkey: 3b39deaf00217894e... Signature Algorithm: rsa_pkcs1_sha512 (0x0601) Signature: 37141adac38ea4... 服务器发送“Server Hello Done”消息。服务器说：“我的信息就是这些，打招呼完毕。 客户端验证服务端的证书是否有效； 客户端按照密码套件的要求，也生成一个椭圆曲线的公钥（Client Params），用“Client Key Exchange”消息发给服务器。 客户端和服务器手里都拿到了密钥交换算法的两个参数（Client Params、Server Params），就用 ECDHE 算法一阵算，算出了一个新的东西，叫“Pre-Master”，其实也是一个随机数。然后用这三个Client Random、Server Random 和 Pre-Master，就可以生成用于加密会 话的主密钥，叫“Master Secret”。 客户端发一个“Change Cipher Spec”，然后再发一个“Finished”消息，把之前所有发送的数据做个摘要，再加密一下，让服务器做个验证。 服务器也是同样的操作，发“Change Cipher Spec”和“Finished”消息，双方都验证加密解密 OK，握手正式结束，后面就收发被加密的 HTTP 请求和响应了。 RSA 握手过程ECDHE是如今主流的 TLS 握手过程，这与传统的RSA握手有两点不同。 第一个，使用 ECDHE 实现密钥交换，而不是 RSA，所以会在服务器端发出“Server Key Exchange”消息。 第二个，因为使用了 ECDHE，客户端可以不用等到服务器发回“Finished”确认握手完毕，立即就发出 HTTP 报文，省去了一个消息往返的时间浪费。这个叫“TLS False Start”，意思就是“抢跑”，和“TCP Fast Open”有点像，都是不等连接完全建立就提前发应用数据，提高传输的效率。 大体的流程没有变，只是“Pre-Master”不再需要用算法生成，而是客户端直接生成随机数，然后用服务器的公钥加密，通过“Client Key Exchange”消息发给服务器。服务器再用私钥解密，这样双方也实现了共享三个随机数，就可以生成主密钥。 双向认证上述描述的是“单向认证”握手过程，只认证了服务器的身份，而没有认证客户端的身份。这是因为通常单向认证通过后已经建立了安全通信，用账号、密码等简单的手段就能够确认用户的真实身份。 但为了防止账号、密码被盗，有的时候（比如网上银行）还会使用 U 盾给用户颁发客户端证书，实现“双向认证”，这样会更加安全。 双向认证的流程也没有太多变化，只是在“Server Hello Done”之后，“Client Key Exchange”之前，客户端要发送“Client Certificate”消息，服务器收到后也把证书链走一遍，验证客户端的身份。","link":"/posts/44020.html"},{"title":"设计模式：中介模式","text":"中介模式的英文翻译是 Mediator Design Pattern。在 GoF 中的《设计模式》一书中，它是这样定义的： Mediator pattern defines a separate (mediator) object that encapsulates the interaction between a set of objects and the objects delegate their interaction to a mediator object instead of interacting with each other directly. 翻译成中文就是：中介模式定义了一个单独的（中介）对象，来封装一组对象之间的交互。将这组对象之间的交互委派给与中介对象交互，来避免对象之间的直接交互。 实际上，中介模式的设计思想跟中间层很像，通过引入中介这个中间层，将一组对象之间的交互关系（或者说依赖关系）从多对多（网状关系）转换为一对多（星状关系）。原来一个对象要跟 n 个对象交互，现在只需要跟一个中介对象交互，从而最小化对象之间的交互关系，降低了代码的复杂度，提高了代码的可读性和可维护性。 右边的交互图是利用中介模式对左边交互关系优化之后的结果，从图中我们可以很直观地看出，右边的交互关系更加清晰、简洁。 中介者模式的一个比较经典的例子，就是航空管制。 为了让飞机在飞行的时候互不干扰，每架飞机都需要知道其他飞机每时每刻的位置，这就需要时刻跟其他飞机通信。飞机通信形成的通信网络就会无比复杂。这个时候，我们通过引入“塔台”这样一个中介，让每架飞机只跟塔台来通信，发送自己的位置给塔台，由塔台来负责每架飞机的航线调度。这样就大大简化了通信网络。 原本业务逻辑会分散在各个控件中，现在都集中到了中介类中。实际上，这样做既有好处，也有坏处。好处是简化了控件之间的交互，坏处是中介类有可能会变成大而复杂的“上帝类”（God Class）。所以，在使用中介模式的时候，我们要根据实际的情况，平衡对象之间交互的复杂度和中介类本身的复杂度。 中介者模式 VS 门面模式门面模式是以封装和隔离为主要任务，而中介者模式则是以调和同事类之间的关系为主，因为要调和，所以具有了部分的业务逻辑控制。两者的主要区别如下： 功能区别 门面模式只是增加了一个门面，他对子系统来说没有增加任何的功能，子系统若脱离门面模式是完全可以独立运行的。而中介者模式则增加了业务功能，他把各个同事类中的原有耦合关系移植到了中介者，同事类不可能脱离中介者而独立存在，除非想增加系统的复杂性和降低扩展性。 知晓状态不同 对门面模式来说，子系统不知道有门面存在，而对中介者来说，每个同事类都知道中介者存在，因为要依靠中介者调和同事之间的关系，他们对中介者非常了解。 封装程度不同 门面模式是一种简单的封装，所有的请求处理都委托给子系统完成，而中介者模式则需要有一个中心，由中心协调同事类完成，并且中心本身也完成部分业务，他属于更进一步的业务功能封装。","link":"/posts/50524.html"},{"title":"设计模式：享元模式","text":"享元模式（Flyweight Pattern）又叫作轻量级模式，是对象池的一种实现。其定义如下： Use sharing to support large numbers of fine-grainedobjects efficiently. 享元模式的意图是复用对象，节省内存，前提是享元对象是不可变对象。 “不可变对象”指的是，一旦通过构造函数初始化完成之后，它的状态（对象的成员变量或者属性）就不会再被修改了。所以，不可变对象不能暴露任何 set() 等修改内部状态的方法。之所以要求享元是不可变对象，那是因为它会被多处代码共享使用，避免一处代码对享元进行了修改，影响到其他使用它的代码。 应用举例当一个系统中存在大量重复对象的时候，如果这些重复的对象是不可变对象，我们就可以利用享元模式将对象设计成享元，在内存中只保留一份实例，供多处代码引用。这样可以减少内存中对象的数量，起到节省内存的目的。实际上，不仅仅相同对象可以设计成享元，对于相似对象，我们也可以将这些对象中相同的部分（字段）提取出来，设计成享元，让这些大量相似对象引用这些享元。 假设我们在开发一个棋牌游戏（比如象棋）。一个游戏厅中有成千上万个“房间”，每个房间对应一个棋局。棋局要保存每个棋子的数据，比如：棋子类型（将、相、士、炮等）、棋子颜色（红方、黑方）、棋子在棋局中的位置。利用这些数据，我们就能显示一个完整的棋盘给玩家。 为了记录每个房间当前的棋局情况，我们需要给每个房间都创建一个ChessBoard 棋局对象。因为游戏大厅中有成千上万的房间（实际上，百万人同时在线的游戏大厅也有很多），那保存这么多棋局对象就会消耗大量的内存。有没有什么办法来节省内存呢？ 这个时候，享元模式就可以派上用场了。这些相似对象的 id、text、color 都是相同的，唯独 positionX、positionY 不同。实际上，我们可以将棋子的 id、text、color 属性拆分出来，设计成独立的类，并且作为享元供多个棋盘复用。这样，棋盘只需要记录每个棋子的位置信息就可以了。 具体的代码实现如下所示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// 享元类public class ChessPieceUnit { private int id; private String text; private Color color; public ChessPieceUnit(int id, String text, Color color) { this.id = id; this.text = text; this.color = color; } public static enum Color { RED, BLACK } // ...省略其他属性和getter方法...}public class ChessPieceUnitFactory { private static final Map&lt;Integer, ChessPieceUnit&gt; pieces = new HashMap&lt;&gt;(); static { pieces.put(1, new ChessPieceUnit(1, &quot;車&quot;, ChessPieceUnit.Color.BLACK)); pieces.put(2, new ChessPieceUnit(2,&quot;馬&quot;, ChessPieceUnit.Color.BLACK)); //...省略摆放其他棋子的代码... } public static ChessPieceUnit getChessPiece(int chessPieceId) { return pieces.get(chessPieceId); }}public class ChessPiece { private ChessPieceUnit chessPieceUnit; private int positionX; private int positionY; public ChessPiece(ChessPieceUnit unit, int positionX, int positionY) { this.chessPieceUnit = unit; this.positionX = positionX; this.positionY = positionY; } // 省略getter、setter方法}public class ChessBoard { private Map&lt;Integer, ChessPiece&gt; chessPieces = new HashMap&lt;&gt;(); public ChessBoard() { init(); } private void init() { chessPieces.put(1, new ChessPiece( ChessPieceUnitFactory.getChessPiece(1), 0,0)); chessPieces.put(1, new ChessPiece( ChessPieceUnitFactory.getChessPiece(2), 1,0)); //...省略摆放其他棋子的代码... } public void move(int chessPieceId, int toPositionX, int toPositionY) { //...省略... }} 享元模式 vs 单例、缓存、对象池享元模式跟单例的区别 在单例模式中，一个类只能创建一个对象，而在享元模式中，一个类可以创建多个对象，每个对象被多处代码引用共享。实际上，享元模式有点类似于之前讲到的单例的变体：多例。 应用享元模式是为了对象复用，节省内存，而应用多例模式是为了限制对象的个数。 享元模式跟缓存的区别 在享元模式的实现中，我们通过工厂类来“缓存”已经创建好的对象。这里的“缓存”实际上是“存储”的意思，跟我们平时所说的“数据库缓存”“CPU 缓存”“MemCache 缓存”是两回事。我们平时所讲的缓存，主要是为了提高访问效率，而非复用。 享元模式跟对象池的区别 对象池、连接池（比如数据库连接池）、线程池等也是为了复用，那它们跟享元模式有什么区别呢？ 你可能对连接池、线程池比较熟悉，对对象池比较陌生，所以，这里我简单解释一下对象池。像 C++ 这样的编程语言，内存的管理是由程序员负责的。为了避免频繁地进行对象创建和释放导致内存碎片，我们可以预先申请一片连续的内存空间，也就是这里说的对象池。每次创建对象时，我们从对象池中直接取出一个空闲对象来使用，对象使用完成之后，再放回到对象池中以供后续复用，而非直接释放掉。 虽然对象池、连接池、线程池、享元模式都是为了复用，但是，如果我们再细致地抠一抠“复用”这个字眼的话，对象池、连接池、线程池等池化技术中的“复用”和享元模式中的“复用”实际上是不同的概念。 池化技术中的“复用”可以理解为“重复使用”，主要目的是节省时间（比如从数据库池中取一个连接，不需要重新创建）。在任意时刻，每一个对象、连接、线程，并不会被多处使用，而是被一个使用者独占，当使用完成之后，放回到池中，再由其他使用者重复利用。享元模式中的“复用”可以理解为“共享使用”，在整个生命周期中，都是被所有使用者共享的，主要目的是节省空间。","link":"/posts/43697.html"},{"title":"设计模式：代理模式","text":"概述定义代理模式（Proxy Pattern）其定义如下： Provide a surrogate or placeholder for another object tocontrol access to it. 翻译成中文就是：代理模式为其他对象提供一种代理，以控制对这个对象的访问。 在某些情况下，一个对象不适合或者不能直接引用另一个对象，而代理对象可以在客户端与目标对象之间起到中介的作用。 当无法或不想直接引用某个对象或访问某个对象存在困难时，可以通过代理对象来间接访问。使用代理模式主要有两个目的： 保护目标对象， 增强目标对象。 UML类图 抽象主题角色（ISubject）：抽象主题类的主要职责是声明真实主题与代理的共同接口方法，该类可以是接口，也可以是抽象类。 真实主题角色（RealSubject）：该类也被称为被代理类，该类定义了代理所表示的真实对象，是负责执行系统的真正的逻辑业务对象。 代理主题角色（Proxy）：也被称为代理类，其内部持有RealSubject的引用，因此具备完全的对RealSubject的代理权。客户端调用代理对象的方法，也调用被代理对象的方法，但是会在代理对象前后增加一些处理代码。 代码实现业务需求：统计UserController类中的每个方法的耗时。UserController类的代码如下： 12345678910111213141516171819public interface IUserController { UserVo login(String telephone, String password); UserVo register(String telephone, String password);}public class UserController implements IUserController { //...省略其他属性和方法... @Override public UserVo login(String telephone, String password) { //...省略login逻辑... //...返回UserVo数据... } @Override public UserVo register(String telephone, String password) { //...省略register逻辑... //...返回UserVo数据... }} 静态代理12345678910111213141516171819202122232425262728293031323334353637383940// 代理类public class UserControllerProxy implements IUserController { // 收集数据 private MetricsCollector metricsCollector; // 被代理类 private UserController userController; public UserControllerProxy(UserController userController) { this.userController = userController; this.metricsCollector = new MetricsCollector(); } @Override public UserVo login(String telephone, String password) { long startTimestamp = System.currentTimeMillis(); // 委托 UserVo userVo = userController.login(telephone, password); long endTimeStamp = System.currentTimeMillis(); long responseTime = endTimeStamp - startTimestamp; RequestInfo requestInfo = new RequestInfo(&quot;login&quot;, responseTime, startTimestamp); metricsCollector.recordRequest(requestInfo); return userVo; } @Override public UserVo register(String telephone, String password) { long startTimestamp = System.currentTimeMillis(); UserVo userVo = userController.register(telephone, password); long endTimeStamp = System.currentTimeMillis(); long responseTime = endTimeStamp - startTimestamp; RequestInfo requestInfo = new RequestInfo(&quot;register&quot;, responseTime, startTimestamp); metricsCollector.recordRequest(requestInfo); return userVo; }}//UserControllerProxy使用举例//因为原始类和代理类实现相同的接口，是基于接口而非实现编程//将UserController类对象替换为UserControllerProxy类对象，不需要改动太多代码IUserController userController = new UserControllerProxy(new UserController()); 如果原始类并没有定义接口，并且原始类代码并不是我们开发维护的（比如它来自一个第三方的类库），我们也没办法直接修改原始类，给它重新定义一个接口。在这种情况下，我们该如何实现代理模式呢？ 对于这种外部类的扩展，我们一般都是采用继承的方式。让代理类继承原始类，然后扩展附加功能。具体代码如下： 1234567891011121314151617181920212223242526272829public class UserControllerProxy extends UserController { private MetricsCollector metricsCollector; public UserControllerProxy() { this.metricsCollector = new MetricsCollector(); } public UserVo login(String telephone, String password) { long startTimestamp = System.currentTimeMillis(); UserVo userVo = super.login(telephone, password); long endTimeStamp = System.currentTimeMillis(); long responseTime = endTimeStamp - startTimestamp; RequestInfo requestInfo = new RequestInfo(&quot;login&quot;, responseTime, startTimestamp); metricsCollector.recordRequest(requestInfo); return userVo; } public UserVo register(String telephone, String password) { long startTimestamp = System.currentTimeMillis(); UserVo userVo = super.register(telephone, password); long endTimeStamp = System.currentTimeMillis(); long responseTime = endTimeStamp - startTimestamp; RequestInfo requestInfo = new RequestInfo(&quot;register&quot;, responseTime, startTimestamp); metricsCollector.recordRequest(requestInfo); return userVo; }}//UserControllerProxy使用举例UserController userController = new UserControllerProxy(); 动态代理如果有 50 个要添加附加功能的原始类，那我们就要创建 50 个对应的代理类。这会导致项目中类的个数成倍增加，增加了代码维护成本。并且，每个代理类中的代码都有点像模板式的“重复”代码，也增加了不必要的开发成本。那这个问题怎么解决呢？ 可以使用动态代理来解决这个问题。所谓动态代理（Dynamic Proxy），就是不事先为每个原始类编写代理类，而是在运行的时候，动态地创建原始类对应的代理类，然后在系统中用代理类替换掉原始类。 Java 语言本身就已经提供了动态代理的语法（实际上，动态代理底层依赖的就是 Java 的反射语法），如何用 Java 的动态代理来实现上述的功能。具体的代码如下所示： 1234567891011121314151617181920212223242526272829303132333435363738public class MetricsCollectorProxy { private MetricsCollector metricsCollector; public MetricsCollectorProxy() { this.metricsCollector = new MetricsCollector(); } public Object createProxy(Object proxiedObject) { Class&lt;?&gt;[] interfaces = proxiedObject.getClass().getInterfaces(); DynamicProxyHandler handler = new DynamicProxyHandler(proxiedObject); return Proxy.newProxyInstance(proxiedObject.getClass().getClassLoader(), interfaces, handler); } private class DynamicProxyHandler implements InvocationHandler { private Object proxiedObject; public DynamicProxyHandler(Object proxiedObject) { this.proxiedObject = proxiedObject; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { long startTimestamp = System.currentTimeMillis(); // 执行业务逻辑 Object result = method.invoke(proxiedObject, args); long endTimeStamp = System.currentTimeMillis(); long responseTime = endTimeStamp - startTimestamp; String apiName = proxiedObject.getClass().getName() + &quot;:&quot; + method.getName(); RequestInfo requestInfo = new RequestInfo(apiName, responseTime, startTimestamp); metricsCollector.recordRequest(requestInfo); return result; } }}//MetricsCollectorProxy使用举例MetricsCollectorProxy proxy = new MetricsCollectorProxy();IUserController userController = (IUserController) proxy.createProxy(new UserController());","link":"/posts/61369.html"},{"title":"设计模式：命令模式","text":"概述命令模式的英文翻译是 Command Design Pattern。在 GoF 的《设计模式》一书中，它是这么定义的： The command pattern encapsulates a request as an object, thereby letting us parameterize other objects with different requests, queue or log requests, and support undoable operations. 命令模式将请求（命令）封装为一个对象，这样可以使用不同的请求参数化其他对象（将不同请求依赖注入到其他对象），并且能够支持请求（命令）的排队执行、记录日志、撤销等（附加控制）功能。 落实到编码实现，命令模式用的最核心的实现手段，是将函数封装成对象。C 语言支持函数指针，可以把函数当作变量传递来传递去。但是，在大部分编程语言中，函数没法儿作为参数传递给其他函数，也没法儿赋值给变量。借助命令模式，我们可以将函数封装成对象。具体来说就是，设计一个包含这个函数的类，实例化一个对象传来传去，这样就可以实现把函数像对象一样使用。从实现的角度来说，它类似我们之前讲过的回调。 命令模式的主要作用和应用场景，是用来控制命令的执行，比如，异步、延迟、排队执行命令、撤销重做命令、存储命令、给命令记录日志等等，这才是命令模式能发挥独一无二作用的地方。 实战讲解假设我们正在开发一个类似《天天酷跑》或者《QQ 卡丁车》这样的手游。这种游戏本身的复杂度集中在客户端。后端基本上只负责数据（比如积分、生命值、装备）的更新和查询，所以，后端逻辑相对于客户端来说，要简单很多。 一般来说，游戏客户端和服务器之间的数据交互是比较频繁的，所以，为了节省网络连接建立的开销，客户端和服务器之间一般采用长连接的方式来通信。通信的格式有多种，比如 Protocol Buffer、JSON、XML，甚至可以自定义格式。不管是什么格式，客户端发送给服务器的请求，一般都包括两部分内容：指令和数据。其中，指令我们也可以叫作事件，数据是执行这个指令所需的数据。 整个手游后端服务器轮询获取客户端发来的请求，获取到请求之后，借助命令模式，把请求包含的数据和处理逻辑封装为命令对象，并存储在内存队列中。然后，再从队列中取出一定数量的命令来执行。执行完成之后，再重新开始新的一轮轮询。具体的示例代码如下所示，你可以结合着一块看下。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public abstract class Command { public abstract void execute();}public class GotDiamondCommand implements Command { // 省略成员变量 public GotDiamondCommand(/*数据*/) { // ... } @Override public void execute() { // 执行相应的逻辑 }}// GotStartCommand/HitObstacleCommand/ArchiveCommand类省略public class GameApplication { private static final MAX_HANDLED_REQ_COUNT_PER_LOOP =100; private Queue&lt;Command&gt; queue = new LinkedList&lt;Command&gt;(); public void mainloop() { while (true) { List&lt;Request&gt; requests = new ArrayList&lt;Request&gt;(); for (Request request : requests) { Event event = request.getEvent(); Command command = null; if (event.equals(Event.GOT_DIAMOND)) { command = new GotDiamondCommand(/*数据*/); } else if (event.equals(Event.GOT_START)) { command = new GotStartCommand(/*数据*/); } else if (event.equals(Event.HIT_OBSTACLE)) { command = new HitObstacleCommand(/*数据*/); } // ... queue.add(command); } int handedCount = 0; while (handedCount &lt; MAX_HANDLED_REQ_COUNT_PER_LOOP) { if (queue.isEmpty()) { break; } Command command = queue.poll(); command.execute(); } } }} 命令模式 vs 策略模式实际上，每个设计模式都应该由两部分组成：第一部分是应用场景，即这个模式可以解决哪类问题；第二部分是解决方案，即这个模式的设计思路和具体的代码实现。不过，代码实现并不是模式必须包含的。如果你单纯地只关注解决方案这一部分，甚至只关注代码实现，就会产生大部分模式看起来都很相似的错觉。 设计模式之间的主要区别还是在于设计意图，也就是应用场景。单纯地看设计思路或者代码实现，有些模式确实很相似，比如策略模式和工厂模式。 在策略模式中，不同的策略具有相同的目的、不同的实现、互相之间可以替换。比如，BubbleSort、SelectionSort 都是为了实现排序的，只不过一个是用冒泡排序算法来实现的，另一个是用选择排序算法来实现的。而在命令模式中，不同的命令具有不同的目的，对应不同的处理逻辑，并且互相之间不可替换。","link":"/posts/3579.html"},{"title":"设计模式：备忘录模式","text":"原理与实现备忘录模式，也叫快照（Snapshot）模式，英文翻译是 Memento Design Pattern。在 GoF 的《设计模式》一书中，备忘录模式是这么定义的： Captures and externalizes an object’s internal state so that it can be restored later, all without violating encapsulation. 翻译成中文就是：在不违背封装原则的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态，以便之后恢复对象为先前的状态。 备忘录模式主要表达了两部分内容： 存储副本以便后期恢复。 在不违背封装原则的前提下，进行对象的备份和恢复。 如何理解“在不违背封装原则的前提下，进行对象的备份和恢复”？需要搞清楚以下两个问题： 为什么存储和恢复副本会违背封装原则？ 备忘录模式是如何做到不违背封装原则的？ 假设有这样一道面试题，希望你编写一个小程序，可以接收命令行的输入。用户输入文本时，程序将其追加存储在内存文本中；用户输入“:list”，程序在命令行中输出内存文本的内容；用户输入“:undo”，程序会撤销上一次输入的文本，也就是从内存文本中将上次输入的文本删除掉。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class InputText { private final StringBuilder text = new StringBuilder(); public String getText() { return text.toString(); } public void append(String input) { text.append(input); } public void setText(String text) { this.text.replace(0, this.text.length(), text); }}public class SnapshotHolder { private Stack&lt;InputText&gt; snapshots = new Stack&lt;&gt;(); public InputText popSnapshot(){ return snapshots.pop(); } public void pushSnapshots(InputText inputText) { InputText deepClonedInputText = new InputText(); deepClonedInputText.setText(inputText.getText()); snapshots.push(deepClonedInputText); }}public class ApplicationMain { public static void main(String[] args) { InputText inputText = new InputText(); SnapshotHolder snapshotHolder = new SnapshotHolder(); Scanner scanner = new Scanner(System.in); while (scanner.hasNext()){ String input = scanner.next(); if(input.equals(&quot;:list&quot;)){ System.out.println(inputText.getText()); }else if(input.equals(&quot;:undo&quot;)){ InputText snapshot = snapshotHolder.popSnapshot(); inputText.setText(snapshot.getText()); }else { snapshotHolder.pushSnapshots(inputText); inputText.append(input); } } }} 上面的代码基本上实现了最基本的备忘录的功能，但是不满足备忘录设计模式的第二点要求。体现在下面两方面： 第一，为了能用快照恢复 InputText 对象，我们在 InputText 类中定义了 setText() 函数，但这个函数有可能会被其他业务使用，所以，暴露不应该暴露的函数违背了封装原则； 第二，快照本身是不可变的，理论上讲，不应该包含任何 set() 等修改内部状态的函数，但在上面的代码实现中，“快照“这个业务模型复用了 InputText 类的定义，而 InputText 类本身有一系列修改内部状态的函数，所以，用 InputText 类来表示快照违背了封装原则。 针对以上问题，我们对代码做两点修改。其一，定义一个独立的类（Snapshot 类）来表示快照，而不是复用 InputText 类。这个类只暴露 get() 方法，没有 set() 等任何修改内部状态的方法。其二，在 InputText 类中，我们把 setText() 方法重命名为 restoreSnapshot() 方法，用意更加明确，只用来恢复对象。重构后的代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public class Snapshot { private String text; public Snapshot(String text){ this.text = text; } public String getText(){ return this.text; }}public class InputText { private final StringBuilder text = new StringBuilder(); public String getText() { return text.toString(); } public void append(String input) { text.append(input); } public Snapshot createSnapshot(){ return new Snapshot(text.toString()); } public void restoreSnapshot(Snapshot snapshot){ this.text.replace(0,this.text.length(),snapshot.getText()); }}public class SnapshotHolder { private Stack&lt;Snapshot&gt; snapshots = new Stack&lt;&gt;(); public Snapshot popSnapshot(){ return snapshots.pop(); } public void pushSnapshots(Snapshot snapshot) { snapshots.push(snapshot); }}public class ApplicationMain { public static void main(String[] args) { InputText inputText = new InputText(); SnapshotHolder snapshotHolder = new SnapshotHolder(); Scanner scanner = new Scanner(System.in); while (scanner.hasNext()){ String input = scanner.next(); if(input.equals(&quot;:list&quot;)){ System.out.println(inputText.getText()); }else if(input.equals(&quot;:undo&quot;)){ Snapshot snapshot = snapshotHolder.popSnapshot(); inputText.restoreSnapshot(snapshot); }else { snapshotHolder.pushSnapshots(inputText.createSnapshot()); inputText.append(input); } } }} 备忘录模式的UML类图 备忘录模式主要包含3个角色，如下： 发起人角色（Originator）：负责创建一个备忘录，记录自身需要保存的状态；具备状态回滚功能。 备忘录角色（Memento）：用于存储Originator的内部状态，且可以防止Originator以外的对象进行访问。 备忘录管理员角色（Caretaker）：负责存储、提供管理Memento，无法对Memento的内容进行操作和访问。 除了备忘录模式，还有一个跟它很类似的概念，“备份”，它在我们平时的开发中更常听到。那备忘录模式跟“备份”有什么区别和联系呢？实际上，这两者的应用场景很类似，都应用在防丢失、恢复、撤销等场景中。它们的区别在于，备忘录模式更侧重于代码的设计和实现，备份更侧重架构设计或产品设计。这个不难理解，这里我就不多说了。","link":"/posts/5768.html"},{"title":"设计模式：原型模式","text":"原型模式概述如果对象的创建成本比较大，而同一个类的不同对象之间差别不大（大部分字段都相同），在这种情况下，我们可以利用对已有对象（原型）进行复制（或者叫拷贝）的方式来创建新对象，以达到节省创建时间的目的。这种基于原型来创建对象的方式就叫作原型设计模式（Prototype Design Pattern），简称原型模式。 实际上，创建对象包含的申请内存、给成员变量赋值这一过程，本身并不会花费太多时间，或者说对于大部分业务系统来说，这点时间完全是可以忽略的。应用一个复杂的模式，只得到一点点的性能提升，这就是所谓的过度设计，得不偿失。 如果对象中的数据需要经过复杂的计算才能得到（比如排序、计算哈希值），或者需要从 RPC、网络、数据库、文件系统等非常慢速的 IO 中读取，这种情况下，我们就可以利用原型模式，从其他已有对象中直接拷贝得到，而不用每次在创建新对象的时候，都重复执行这些耗时的操作。 举例说明，描述如下： 假设数据库中存储了大约 10 万条“搜索关键词”信息，每条信息包含关键词、关键词被搜索的次数、信息最近被更新的时间等。系统 A 在启动的时候会加载这份数据到内存中，用于处理某些其他的业务需求。为了方便快速地查找某个关键词对应的信息，我们给关键词建立一个散列表索引。 还有另外一个系统 B，专门用来分析搜索日志，定期（比如间隔 10 分钟）批量地更新数据库中的数据，并且标记为新的数据版本。比如，在下面的示例图中，我们对 v2 版本的数据进行更新，得到 v3 版本的数据。这里我们假设只有更新和新添关键词，没有删除关键词的行为。 为了保证系统 A 中数据的实时性（不一定非常实时，但数据也不能太旧），系统 A 需要定期根据数据库中的数据，更新内存中的索引和数据。 实现这个需求实际上，也不难。我们只需要在系统 A 中，记录当前数据的版本 Va 对应的更新时间 Ta，从数据库中捞出更新时间大于 Ta 的所有搜索关键词，也就是找出 Va 版本与最新版本数据的“差集”，然后针对差集中的每个关键词进行处理。如果它已经在散列表中存在了，我们就更新相应的搜索次数、更新时间等信息；如果它在散列表中不存在，我们就将它插入到散列表中。 12345678910111213141516171819202122232425262728public class Demo { private ConcurrentHashMap&lt;String, SearchWord&gt; currentKeywords = new ConcurrentHashMap&lt;&gt;(); private long lastUpdateTime = -1; public void refresh() { // 从数据库中取出更新时间&gt;lastUpdateTime的数据，放入到currentKeywords中 List&lt;SearchWord&gt; toBeUpdatedSearchWords = getSearchWords(lastUpdateTime); long maxNewUpdatedTime = lastUpdateTime; for (SearchWord searchWord : toBeUpdatedSearchWords) { if (searchWord.getLastUpdateTime() &gt; maxNewUpdatedTime) { maxNewUpdatedTime = searchWord.getLastUpdateTime(); } if (currentKeywords.containsKey(searchWord.getKeyword())) { currentKeywords.replace(searchWord.getKeyword(), searchWord); } else { currentKeywords.put(searchWord.getKeyword(), searchWord); } } lastUpdateTime = maxNewUpdatedTime; } private List&lt;SearchWord&gt; getSearchWords(long lastUpdateTime) { // TODO: 从数据库中取出更新时间&gt;lastUpdateTime的数据 return null; }} 现在，我们有一个特殊的要求：任何时刻，系统 A 中的所有数据都必须是同一个版本的，要么都是版本 a，要么都是版本 b，不能有的是版本 a，有的是版本 b。那刚刚的更新方式就不能满足这个要求了。除此之外，我们还要求：在更新内存数据的时候，系统 A 不能处于不可用状态，也就是不能停机更新数据。 那我们该如何实现现在这个需求呢？ 可以把正在使用的数据的版本定义为“服务版本”，当我们要更新内存中的数据的时候，我们并不是直接在服务版本（假设是版本 a 数据）上更新，而是重新创建另一个版本数据（假设是版本 b 数据），等新的版本数据建好之后，再一次性地将服务版本从版本 a 切换到版本 b。这样既保证了数据一直可用，又避免了中间状态的存在。 12345678910111213141516public class Demo { private HashMap&lt;String, SearchWord&gt; currentKeywords=new HashMap&lt;&gt;(); public void refresh() { HashMap&lt;String, SearchWord&gt; newKeywords = new LinkedHashMap&lt;&gt;(); // 从数据库中取出所有的数据，放入到newKeywords中 List&lt;SearchWord&gt; toBeUpdatedSearchWords = getSearchWords(); for (SearchWord searchWord : toBeUpdatedSearchWords) { newKeywords.put(searchWord.getKeyword(), searchWord); } currentKeywords = newKeywords; } private List&lt;SearchWord&gt; getSearchWords() { // TODO: 从数据库中取出所有的数据 return null; }} 不过，在上面的代码实现中，newKeywords 构建的成本比较高。我们需要将这 10 万条数据从数据库中读出，然后计算哈希值，构建 newKeywords。这个过程显然是比较耗时。为了提高效率，原型模式就派上用场了。 按照这个设计思路，代码如下： 1234567891011121314151617181920212223242526272829public class Demo { private HashMap&lt;String, SearchWord&gt; currentKeywords=new HashMap&lt;&gt;(); private long lastUpdateTime = -1; public void refresh() { // 原型模式就这么简单，拷贝已有对象的数据，更新少量差值 HashMap&lt;String, SearchWord&gt; newKeywords = (HashMap&lt;String, SearchWord&gt;) currentKeywords.clone(); // 从数据库中取出更新时间&gt;lastUpdateTime的数据，放入到newKeywords中 List&lt;SearchWord&gt; toBeUpdatedSearchWords = getSearchWords(lastUpdateTime); long maxNewUpdatedTime = lastUpdateTime; for (SearchWord searchWord : toBeUpdatedSearchWords) { if (searchWord.getLastUpdateTime() &gt; maxNewUpdatedTime) { maxNewUpdatedTime = searchWord.getLastUpdateTime(); } if (newKeywords.containsKey(searchWord.getKeyword())) { SearchWord oldSearchWord = newKeywords.get(searchWord.getKeyword()); oldSearchWord.setCount(searchWord.getCount()); oldSearchWord.setLastUpdateTime(searchWord.getLastUpdateTime()); } else { newKeywords.put(searchWord.getKeyword(), searchWord); } } lastUpdateTime = maxNewUpdatedTime; currentKeywords = newKeywords; } private List&lt;SearchWord&gt; getSearchWords(long lastUpdateTime) { // TODO: 从数据库中取出更新时间&gt;lastUpdateTime的数据 return null; }} 利用了 Java 中的 clone() 语法来复制一个对象。如果你熟悉的语言没有这个语法，那把数据从 currentKeywords 中一个个取出来，然后再重新计算哈希值，放入到 newKeywords 中也是可以接受的。毕竟，最耗时的还是从数据库中取数据的操作。相对于数据库的 IO 操作来说，内存操作和 CPU 计算的耗时都是可以忽略的。 深拷贝和浅拷贝在散列表索引中，每个结点存储的key是搜索关键词，value是SearchWord对象的内存地址。SearchWord对象本身存储在散列表之外的内存空间中，如下图所示： 浅拷贝和深拷贝的区别在于，浅拷贝只会复制图中的索引（散列表），不会复制数据（SearchWord 对象）本身。相反，深拷贝不仅仅会复制索引，还会复制数据本身。浅拷贝得到的对象（newKeywords）跟原始对象（currentKeywords）共享数据（SearchWord 对象），而深拷贝得到的是一份完完全全独立的对象。 在 Java 语言中，Object 类的 clone() 方法执行的就是我们刚刚说的浅拷贝。它只会拷贝对象中的基本数据类型的数据（比如，int、long），以及引用对象（SearchWord）的内存地址，不会递归地拷贝引用对象本身。 那如何实现深拷贝呢？总结一下的话，有下面两种方法： 递归拷贝对象、对象的引用对象以及引用对象的引用对象……直到要拷贝的对象只包含基本数据类型数据，没有引用对象为止。 先将对象序列化，然后再反序列化成新的对象。 深拷贝都要比浅拷贝耗时、耗内存空间。针对我们这个应用场景，有没有更快、更省内存的实现方式呢？ 可以先采用浅拷贝的方式创建 newKeywords。对于需要更新的SearchWord对象，我们再使用深度拷贝的方式创建一份新的对象，替换newKeywords中的老对象。毕竟需要更新的数据是很少的。这种方式即利用了浅拷贝节省时间、空间的优点，又能保证currentKeywords中的中数据都是老版本的数据。具体的代码实现如下所示： 12345678910111213141516171819202122232425262728public class Demo { private HashMap&lt;String, SearchWord&gt; currentKeywords=new HashMap&lt;&gt;(); private long lastUpdateTime = -1; public void refresh() { // Shallow copy HashMap&lt;String, SearchWord&gt; newKeywords = (HashMap&lt;String, SearchWord&gt;) currentKeywords.clone(); // 从数据库中取出更新时间&gt;lastUpdateTime的数据，放入到newKeywords中 List&lt;SearchWord&gt; toBeUpdatedSearchWords = getSearchWords(lastUpdateTime); long maxNewUpdatedTime = lastUpdateTime; for (SearchWord searchWord : toBeUpdatedSearchWords) { if (searchWord.getLastUpdateTime() &gt; maxNewUpdatedTime) { maxNewUpdatedTime = searchWord.getLastUpdateTime(); } if (newKeywords.containsKey(searchWord.getKeyword())) { newKeywords.remove(searchWord.getKeyword()); } newKeywords.put(searchWord.getKeyword(), searchWord); } lastUpdateTime = maxNewUpdatedTime; currentKeywords = newKeywords; } private List&lt;SearchWord&gt; getSearchWords(long lastUpdateTime) { // TODO: 从数据库中取出更新时间&gt;lastUpdateTime的数据 return null; }}","link":"/posts/3118.html"},{"title":"设计模式：构建者模式","text":"为什么还需要建造者模式来创建呢？直接使用构造函数或者配合 set 方法就能创建对象，为什么还需要建造者模式来创建呢？ 假设有这样一道设计面试题：我们需要定义一个资源池配置类 ResourcePoolConfig。这里的资源池，你可以简单理解为线程池、连接池、对象池等。在这个资源池配置类中，有以下几个成员变量，也就是可配置项。现在，请你编写代码实现这个 ResourcePoolConfig 类。 最常见、最容易想到的实现思路如下代码所示。 123456789101112131415161718192021222324252627282930313233343536public class ResourcePoolConfig { private static final int DEFAULT_MAX_TOTAL = 8; private static final int DEFAULT_MAX_IDLE = 8; private static final int DEFAULT_MIN_IDLE = 0; private String name; private int maxTotal = DEFAULT_MAX_TOTAL; private int maxIdle = DEFAULT_MAX_IDLE; private int minIdle = DEFAULT_MIN_IDLE; public ResourcePoolConfig(String name, Integer maxTotal, Integer maxIdle, Integer minIdle) { if (StringUtils.isBlank(name)) { throw new IllegalArgumentException(&quot;name should not be empty.&quot;); } this.name = name; if (maxTotal != null) { if (maxTotal &lt;= 0) { throw new IllegalArgumentException(&quot;maxTotal should be positive.&quot;); } this.maxTotal = maxTotal; } if (maxIdle != null) { if (maxIdle &lt; 0) { throw new IllegalArgumentException(&quot;maxIdle should not be negative.&quot;); } this.maxIdle = maxIdle; } if (minIdle != null) { if (minIdle &lt; 0) { throw new IllegalArgumentException(&quot;minIdle should not be negative.&quot;); } this.minIdle = minIdle; } } //...省略getter方法...} 因为 maxTotal、maxIdle、minIdle 不是必填变量，所以在创建 ResourcePoolConfig 对象的时候，我们通过往构造函数中，给这几个参数传递 null 值，来表示使用默认值。 现在，ResourcePoolConfig 只有 4 个可配置项，对应到构造函数中，也只有 4 个参数，参数的个数不多。但是，如果可配置项逐渐增多，变成了 8 个、10 个，甚至更多，那继续沿用现在的设计思路，构造函数的参数列表会变得很长，代码在可读性和易用性上都会变差。 在使用构造函数的时候，我们就容易搞错各参数的顺序，传递进错误的参数值，导致非常隐蔽的 bug。解决这个问题的办法就是用 set() 函数来给成员变量赋值，以替代冗长的构造函数。代码如下： 1234567891011121314151617181920212223242526272829303132333435363738public class ResourcePoolConfig { private static final int DEFAULT_MAX_TOTAL = 8; private static final int DEFAULT_MAX_IDLE = 8; private static final int DEFAULT_MIN_IDLE = 0; private String name; private int maxTotal = DEFAULT_MAX_TOTAL; private int maxIdle = DEFAULT_MAX_IDLE; private int minIdle = DEFAULT_MIN_IDLE; public ResourcePoolConfig(String name) { if (StringUtils.isBlank(name)) { throw new IllegalArgumentException(&quot;name should not be empty.&quot;); } this.name = name; } public void setMaxTotal(int maxTotal) { if (maxTotal &lt;= 0) { throw new IllegalArgumentException(&quot;maxTotal should be positive.&quot;); } this.maxTotal = maxTotal; } public void setMaxIdle(int maxIdle) { if (maxIdle &lt; 0) { throw new IllegalArgumentException(&quot;maxIdle should not be negative.&quot;); } this.maxIdle = maxIdle; } public void setMinIdle(int minIdle) { if (minIdle &lt; 0) { throw new IllegalArgumentException(&quot;minIdle should not be negative.&quot;); } this.minIdle = minIdle; } //...省略getter方法...} 如果我们把问题的难度再加大点，比如，还需要解决下面这三个问题，那现在的设计思路就不能满足了。 name 是必填的，所以，我们把它放到构造函数中，强制创建对象的时候就设置。如果必填的配置项有很多，把这些必填配置项都放到构造函数中设置，那构造函数就又会出现参数列表很长的问题。如果我们把必填项也通过 set() 方法设置，那校验这些必填项是否已经填写的逻辑就无处安放了。 除此之外，假设配置项之间有一定的依赖关系，比如，如果用户设置了 maxTotal、maxIdle、minIdle 其中一个，就必须显式地设置另外两个；或者配置项之间有一定的约束条件，比如，maxIdle 和 minIdle 要小于等于 maxTotal。如果我们继续使用现在的设计思路，那这些配置项之间的依赖关系或者约束条件的校验逻辑就无处安放了。 如果我们希望 ResourcePoolConfig 类对象是不可变对象，也就是说，对象在创建好之后，就不能再修改内部的属性值。要实现这个功能，我们就不能在 ResourcePoolConfig 类中暴露 set() 方法。 为了解决这些问题，建造者模式就派上用场了。如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182public class ResourcePoolConfig { private String name; private int maxTotal; private int maxIdle; private int minIdle; private ResourcePoolConfig(Builder builder) { this.name = builder.name; this.maxTotal = builder.maxTotal; this.maxIdle = builder.maxIdle; this.minIdle = builder.minIdle; } //...省略getter方法... public static class Builder { private static final int DEFAULT_MAX_TOTAL = 8; private static final int DEFAULT_MAX_IDLE = 8; private static final int DEFAULT_MIN_IDLE = 0; private String name; private int maxTotal = DEFAULT_MAX_TOTAL; private int maxIdle = DEFAULT_MAX_IDLE; private int minIdle = DEFAULT_MIN_IDLE; public ResourcePoolConfig build() { // 校验逻辑放到这里来做，包括必填项校验、依赖关系校验、约束条件校验等 if (StringUtils.isBlank(name)) { throw new IllegalArgumentException(&quot;...&quot;); } if (maxIdle &gt; maxTotal) { throw new IllegalArgumentException(&quot;...&quot;); } if (minIdle &gt; maxTotal || minIdle &gt; maxIdle) { throw new IllegalArgumentException(&quot;...&quot;); } return new ResourcePoolConfig(this); } public Builder setName(String name) { if (StringUtils.isBlank(name)) { throw new IllegalArgumentException(&quot;...&quot;); } this.name = name; return this; } public Builder setMaxTotal(int maxTotal) { if (maxTotal &lt;= 0) { throw new IllegalArgumentException(&quot;...&quot;); } this.maxTotal = maxTotal; return this; } public Builder setMaxIdle(int maxIdle) { if (maxIdle &lt; 0) { throw new IllegalArgumentException(&quot;...&quot;); } this.maxIdle = maxIdle; return this; } public Builder setMinIdle(int minIdle) { if (minIdle &lt; 0) { throw new IllegalArgumentException(&quot;...&quot;); } this.minIdle = minIdle; return this; } }}// 这段代码会抛出IllegalArgumentException，因为minIdle&gt;maxIdleResourcePoolConfig config = new ResourcePoolConfig.Builder() .setName(&quot;dbconnectionpool&quot;) .setMaxTotal(16) .setMaxIdle(10) .setMinIdle(12) .build(); 使用建造者模式来构建对象，代码实际上是有点重复的，ResourcePoolConfig 类中的成员变量，要在 Builder 类中重新再定义一遍。 与工厂模式的区别？工厂模式是用来创建不同但是相关类型的对象（继承同一父类或者接口的一组子类），由给定的参数来决定创建哪种类型的对象。建造者模式是用来创建一种类型的复杂对象，通过设置不同的可选参数，“定制化”地创建不同的对象。 网上有一个经典的例子很好地解释了两者的区别。 顾客走进一家餐馆点餐，我们利用工厂模式，根据用户不同的选择，来制作不同的食物，比如披萨、汉堡、沙拉。对于披萨来说，用户又有各种配料可以定制，比如奶酪、西红柿、起司，我们通过建造者模式根据用户选择的不同配料来制作披萨。 使用场景如果存在下面情况中的任意一种，我们就要考虑使用建造者模式了。如下： 把类的必填属性放到构造函数中，强制创建对象的时候就设置。如果必填的属性有很多，把这些必填属性都放到构造函数中设置，那构造函数就又会出现参数列表很长的问题。如果我们把必填属性通过 set() 方法设置，那校验这些必填属性是否已经填写的逻辑就无处安放了。 如果类的属性之间有一定的依赖关系或者约束条件，我们继续使用构造函数配合 set() 方法的设计思路，那这些依赖关系或约束条件的校验逻辑就无处安放了。 如果我们希望创建不可变对象，也就是说，对象在创建好之后，就不能再修改内部的属性值，要实现这个功能，就不能在类中暴露 set() 方法。构造函数配合 set() 方法来设置属性值的方式就不适用了。","link":"/posts/41674.html"},{"title":"设计模式：桥接模式","text":"概述桥接模式（Bridge Pattern）又叫作桥梁模式，其定义如下： Decouple an abstraction from its implementation so that thetwo can vary independently. 翻译成中文就是：将抽象和实现解耦，让它们可以独立变化。 此处的抽象并不是指抽象类或接口这种高层概念，实现也不是指继承或接口实现。抽象与实现其实指的是两种独立变化的维度。其中，抽象包含实现，因此，一个抽象类的变化可能涉及多种维度的变化。 桥接模式的UML类图如下： 抽象（Abstraction）：该类持有一个对实现角色的引用，抽象角色中的方法需要实现角色来实现。抽象角色一般为抽象类（构造函数规定子类要传入一个实现对象）。 修正抽象（RefinedAbstraction）：Abstraction的具体实现，对Abstraction的方法进行完善和扩展。 实现（IImplementor）：确定实现维度的基本操作，提供给Abstraction使用。该类一般为接口或抽象类。 具体实现（ConcreteImplementor）：Implementor的具体实现。 案例分析JDBC 驱动是桥接模式的经典应用，如何利用 JDBC 驱动来查询数据库？具体的代码如下所示： 12345678910Class.forName(&quot;com.mysql.jdbc.Driver&quot;);//加载及注册JDBC驱动程序String url = &quot;jdbc:mysql://localhost:3306/sample_db?user=root&amp;password=your_password&quot;;Connection con = DriverManager.getConnection(url);Statement stmt = con.createStatement()；String query = &quot;select * from test&quot;;ResultSet rs=stmt.executeQuery(query);while(rs.next()) { rs.getString(1); rs.getInt(2);} 如果我们想要把 MySQL 数据库换成 Oracle 数据库，只需要把第一行代码中的 com.mysql.jdbc.Driver 换成 oracle.jdbc.driver.OracleDriver 就可以了。 com.mysql.jdbc.Driver 的源码如下： 123456789101112131415161718package com.mysql.jdbc;import java.sql.SQLException;public class Driver extends NonRegisteringDriver implements java.sql.Driver { static { try { java.sql.DriverManager.registerDriver(new Driver()); } catch (SQLException E) { throw new RuntimeException(&quot;Can't register driver!&quot;); } } /** * Construct a new driver and register it with DriverManager * @throws SQLException if a database error occurs. */ public Driver() throws SQLException { // Required for Class.forName().newInstance() }} 执行Class.forName(“com.mysql.jdbc.Driver”) 这条语句的时候，实际上是做了两件事情。第一件事情是要求 JVM 查找并加载指定的 Driver 类，第二件事情是执行该类的静态代码，也就是将 MySQL Driver 注册到 DriverManager 类中。 DriverManager 类是干什么用的？具体的代码如下所示： 12345678910111213141516171819202122232425262728public class DriverManager { private final static CopyOnWriteArrayList&lt;DriverInfo&gt; registeredDrivers = new CopyOnWriteArrayList&lt;DriverInfo&gt;(); //... static { loadInitialDrivers(); println(&quot;JDBC DriverManager initialized&quot;); } //... public static synchronized void registerDriver(java.sql.Driver driver) throws SQLException { if (driver != null) { registeredDrivers.addIfAbsent(new DriverInfo(driver)); } else { throw new NullPointerException(); } } public static Connection getConnection(String url, String user, String password) throws SQLException { java.util.Properties info = new java.util.Properties(); if (user != null) { info.put(&quot;user&quot;, user); } if (password != null) { info.put(&quot;password&quot;, password); } return (getConnection(url, info, Reflection.getCallerClass())); } //...} 当我们把具体的 Driver 实现类（比如，com.mysql.jdbc.Driver）注册到 DriverManager 之后，后续所有对 JDBC 接口的调用，都会委派到对具体的 Driver 实现类来执行。而 Driver 实现类都实现了相同的接口（java.sql.Driver ），这也是可以灵活切换 Driver 的原因。 桥接模式的定义是“将抽象和实现解耦，让它们可以独立变化”。那弄懂定义中“抽象”和“实现”两个概念，就是理解桥接模式的关键。那在 JDBC 这个例子中，什么是“抽象”？什么是“实现”呢？ 实际上，JDBC 本身就相当于“抽象”。注意，这里所说的“抽象”，指的并非“抽象类”或“接口”，而是跟具体的数据库无关的、被抽象出来的一套“类库”。具体的 Driver（比如，com.mysql.jdbc.Driver）就相当于“实现”。注意，这里所说的“实现”，也并非指“接口的实现类”，而是跟具体数据库相关的一套“类库”。JDBC 和 Driver 独立开发，通过对象之间的组合关系，组装在一起。JDBC 的所有逻辑操作，最终都委托给 Driver 来执行。 应用举例一个 API 接口监控告警的例子：根据不同的告警规则，触发不同类型的告警。告警支持多种通知渠道，包括：邮件、短信、微信、自动语音电话。通知的紧急程度有多种类型，包括：SEVERE（严重）、URGENCY（紧急）、NORMAL（普通）、TRIVIAL（无关紧要）。不同的紧急程度对应不同的通知渠道。比如，SERVE（严重）级别的消息会通过“自动语音电话”告知相关人员。 显然，监控告警接口可以划分两个维度：通知渠道和紧急程度。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public interface MsgSender { void send(String message);}public class TelephoneMsgSender implements MsgSender { private List&lt;String&gt; telephones; public TelephoneMsgSender(List&lt;String&gt; telephones) { this.telephones = telephones; } @Override public void send(String message) { //... }}public class EmailMsgSender implements MsgSender { // 与TelephoneMsgSender代码结构类似，所以省略...}public class WechatMsgSender implements MsgSender { // 与TelephoneMsgSender代码结构类似，所以省略...}public abstract class Notification { protected MsgSender msgSender; public Notification(MsgSender msgSender) { this.msgSender = msgSender; } public abstract void notify(String message);}public class SevereNotification extends Notification { public SevereNotification(MsgSender msgSender) { super(msgSender); } @Override public void notify(String message) { msgSender.send(message); }}public class UrgencyNotification extends Notification { // 与SevereNotification代码结构类似，所以省略...}public class NormalNotification extends Notification { // 与SevereNotification代码结构类似，所以省略...}public class TrivialNotification extends Notification { // 与SevereNotification代码结构类似，所以省略...}","link":"/posts/64842.html"},{"title":"设计模式：模板模式","text":"原理与实现模板模式，全称是模板方法设计模式，英文是 Template Method Design Pattern。在 GoF 的《设计模式》一书中，它是这么定义的： Define the skeleton of an algorithm in an operation, deferring some steps to subclasses. Template Method lets subclasses redefine certain steps of an algorithm without changing the algorithm’s structure. 翻译成中文就是：模板方法模式在一个方法中定义一个算法骨架，并将某些步骤推迟到子类中实现。模板方法模式可以让子类在不改变算法整体结构的情况下，重新定义算法中的某些步骤。 这里的“算法”，可以理解为广义上的“业务逻辑”，并不特指数据结构和算法中的“算法”。 原理很简单，代码实现就更加简单。示例代码如下所示： 1234567891011121314151617181920212223242526272829303132333435363738public abstract class AbstractClass { public final void templateMethod() { //... method1(); //... method2(); //... } protected abstract void method1(); protected abstract void method2();}public class ConcreteClass1 extends AbstractClass { @Override protected void method1() { //... } @Override protected void method2() { //... }}public class ConcreteClass2 extends AbstractClass { @Override protected void method1() { //... } @Override protected void method2() { //... }}AbstractClass demo = ConcreteClass1();demo.templateMethod(); templateMethod() 函数定义为 final，是为了避免子类重写它。method1() 和 method2() 定义为 abstract，是为了强迫子类去实现。不过，这些都不是必须的，在实际的项目开发中，模板模式的代码实现比较灵活。 模板模式作用作用一：复用模板模式把一个算法中不变的流程抽象到父类的模板方法 templateMethod() 中，将可变的部分 method1()、method2() 留给子类 ContreteClass1 和 ContreteClass2 来实现。所有的子类都可以复用父类中模板方法定义的流程代码。 Java IO 类库中，有很多类的设计用到了模板模式，比如 InputStream、OutputStream、Reader、Writer。以InputStream 来举例说明，read() 函数是一个模板方法，定义了读取数据的整个流程，并且暴露了一个可以由子类来定制的抽象方法。 1234567891011121314151617181920212223242526272829303132333435363738394041public abstract class InputStream implements Closeable { //...省略其他代码... public int read(byte b[], int off, int len) throws IOException { if (b == null) { throw new NullPointerException(); } else if (off &lt; 0 || len &lt; 0 || len &gt; b.length - off) { throw new IndexOutOfBoundsException(); } else if (len == 0) { return 0; } int c = read(); if (c == -1) { return -1; } b[off] = (byte)c; int i = 1; try { for (; i &lt; len ; i++) { c = read(); if (c == -1) { break; } b[off + i] = (byte)c; } } catch (IOException ee) { } return i; } public abstract int read() throws IOException;}public class ByteArrayInputStream extends InputStream { //...省略其他代码... @Override public synchronized int read() { return (pos &lt; count) ? (buf[pos++] &amp; 0xff) : -1; }} 作用二：扩展这里所说的扩展，并不是指代码的扩展性，而是指框架的扩展性。常在框架的开发中，使用模板模式让框架用户可以在不修改框架源码的情况下，定制化框架的功能。 在使用Servlet 来开发 Web 项目时，只需要定义一个继承 HttpServlet 的类，并且重写其中的 doGet() 或 doPost() 方法，来分别处理 get 和 post 请求。具体的代码示例如下所示： 1234567891011public class HelloServlet extends HttpServlet { @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { this.doPost(req, resp); } @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { resp.getWriter().write(&quot;Hello World.&quot;); }} 除此之外，我们还需要在配置文件 web.xml 中做如下配置。Tomcat、Jetty 等 Servlet 容器在启动的时候，会自动加载这个配置文件中的 URL 和 Servlet 之间的映射关系。 12345678&lt;servlet&gt; &lt;servlet-name&gt;HelloServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;com.xzg.cd.HelloServlet&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;HelloServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/hello&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 当我们在浏览器中输入网址（比如，http://127.0.0.1:8080/hello ）的时候，Servlet 容器会接收到相应的请求，并且根据 URL 和 Servlet 之间的映射关系，找到相应的 Servlet（HelloServlet），然后执行它的 service() 方法。service() 方法定义在父类 HttpServlet 中，它会调用 doGet() 或 doPost() 方法，然后输出数据（“Hello world”）到网页。 HttpServlet#service() 源码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public void service(ServletRequest req, ServletResponse res) throws ServletException, IOException{ HttpServletRequest request; HttpServletResponse response; if (!(req instanceof HttpServletRequest &amp;&amp; res instanceof HttpServletResponse)) { throw new ServletException(&quot;non-HTTP request or response&quot;); } request = (HttpServletRequest) req; response = (HttpServletResponse) res; service(request, response);}protected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException{ String method = req.getMethod(); if (method.equals(METHOD_GET)) { long lastModified = getLastModified(req); if (lastModified == -1) { // servlet doesn't support if-modified-since, no reason // to go through further expensive logic doGet(req, resp); } else { long ifModifiedSince = req.getDateHeader(HEADER_IFMODSINCE); if (ifModifiedSince &lt; lastModified) { // If the servlet mod time is later, call doGet() // Round down to the nearest second for a proper compare // A ifModifiedSince of -1 will always be less maybeSetLastModified(resp, lastModified); doGet(req, resp); } else { resp.setStatus(HttpServletResponse.SC_NOT_MODIFIED); } } } else if (method.equals(METHOD_HEAD)) { long lastModified = getLastModified(req); maybeSetLastModified(resp, lastModified); doHead(req, resp); } else if (method.equals(METHOD_POST)) { doPost(req, resp); } else if (method.equals(METHOD_PUT)) { doPut(req, resp); } else if (method.equals(METHOD_DELETE)) { doDelete(req, resp); } else if (method.equals(METHOD_OPTIONS)) { doOptions(req,resp); } else if (method.equals(METHOD_TRACE)) { doTrace(req,resp); } else { String errMsg = lStrings.getString(&quot;http.method_not_implemented&quot;); Object[] errArgs = new Object[1]; errArgs[0] = method; errMsg = MessageFormat.format(errMsg, errArgs); resp.sendError(HttpServletResponse.SC_NOT_IMPLEMENTED, errMsg); }} HttpServlet 的 service() 方法就是一个模板方法，它实现了整个 HTTP 请求的执行流程，doGet()、doPost() 是模板中可以由子类来定制的部分。实际上，这就相当于 Servlet 框架提供了一个扩展点（doGet()、doPost() 方法），让框架用户在不用修改 Servlet 框架源码的情况下，将业务代码通过扩展点镶嵌到框架中执行。 回调函数复用和扩展是模板模式的两大作用，实际上，还有另外一个技术概念，也能起到跟模板模式相同的作用，那就是回调（Callback）。 原理相对于普通的函数调用来说，回调是一种双向调用关系。A 类事先注册某个函数 F 到 B 类，A 类在调用 B 类的 P 函数的时候，B 类反过来调用 A 类注册给它的 F 函数（回调函数）。A 调用 B，B 反过来又调用 A，这种调用机制就叫作“回调”。 123456789101112131415161718192021public interface ICallback { void methodToCallback();}public class BClass { public void process(ICallback callback) { //... callback.methodToCallback(); //... }}public class AClass { public static void main(String[] args) { BClass b = new BClass(); b.process(new ICallback() { //回调对象 @Override public void methodToCallback() { System.out.println(&quot;Call back me.&quot;); } }); }} 从代码实现可以看出，回调跟模板模式一样，也具有复用和扩展的功能。除了回调函数之外，BClass 类的 process() 函数中的逻辑都可以复用。如果 ICallback、BClass 类是框架代码，AClass 是使用框架的客户端代码，可以通过 ICallback 定制 process() 函数，也就是说，框架因此具有了扩展的能力。 回调可以分为同步回调和异步回调（或者延迟回调）。同步回调指在函数返回之前执行回调函数；异步回调指的是在函数返回之后执行回调函数。从应用场景上来看，同步回调看起来更像模板模式，异步回调看起来更像观察者模式。 应用举例：JdbcTemplateSpring 提供了很多 Template 类，比如，JdbcTemplate、RedisTemplate、RestTemplate。尽管都叫作 xxxTemplate，但它们并非基于模板模式来实现的，而是基于回调来实现的，确切地说应该是同步回调。而同步回调从应用场景上很像模板模式，所以，在命名上，这些类使用 Template（模板）这个单词作为后缀。 当使用 JdbcTemplate 实现查询用户信息，我们只需要编写跟这个业务有关的代码，其中包括，查询用户的 SQL 语句、查询结果与 User 对象之间的映射关系。其他流程性质的代码（底层JDBC）都封装在了 JdbcTemplate 类中，不需要我们每次都重新编写。 12345678910111213141516public class JdbcTemplateDemo { private JdbcTemplate jdbcTemplate; public User queryUser(long id) { String sql = &quot;select * from user where id=&quot;+id; return jdbcTemplate.query(sql, new UserRowMapper()).get(0); } class UserRowMapper implements RowMapper&lt;User&gt; { public User mapRow(ResultSet rs, int rowNum) throws SQLException { User user = new User(); user.setId(rs.getLong(&quot;id&quot;)); user.setName(rs.getString(&quot;name&quot;)); user.setTelephone(rs.getString(&quot;telephone&quot;)); return user; } }} 那 JdbcTemplate 底层具体是如何实现的呢？ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public &lt;T&gt; T query(final String sql, final ResultSetExtractor&lt;T&gt; rse) throws DataAccessException { /** * Callback to execute the query. */ class QueryStatementCallback implements StatementCallback&lt;T&gt;, SqlProvider { @Override @Nullable public T doInStatement(Statement stmt) throws SQLException { ResultSet rs = null; try { rs = stmt.executeQuery(sql); return rse.extractData(rs); } finally { JdbcUtils.closeResultSet(rs); } } @Override public String getSql() { return sql; } } return execute(new QueryStatementCallback());}public &lt;T&gt; T execute(StatementCallback&lt;T&gt; action) throws DataAccessException { Assert.notNull(action, &quot;Callback object must not be null&quot;); Connection con = DataSourceUtils.getConnection(obtainDataSource()); Statement stmt = null; try { stmt = con.createStatement(); applyStatementSettings(stmt); T result = action.doInStatement(stmt); handleWarnings(stmt); return result; } catch (SQLException ex) { // Release Connection early, to avoid potential connection pool deadlock // in the case when the exception translator hasn't been initialized yet. String sql = getSql(action); JdbcUtils.closeStatement(stmt); stmt = null; DataSourceUtils.releaseConnection(con, getDataSource()); con = null; throw translateException(&quot;StatementCallback&quot;, sql, ex); } finally { JdbcUtils.closeStatement(stmt); DataSourceUtils.releaseConnection(con, getDataSource()); }} JdbcTemplate 通过回调的机制，将不变的执行流程抽离出来，放到模板方法 execute() 中，将可变的部分设计成回调 StatementCallback，由用户来定制。query() 函数是对 execute() 函数的二次封装，让接口用起来更加方便。 模板模式 VS 回调从应用场景上来看\b，同步回调跟模板模式几乎一致。它们都是在一个大的算法骨架中，自由替换其中的某个步骤，起到代码复用和扩展的目的。而异步回调跟模板模式有较大差别，更像是观察者模式。 从代码实现上来看，回调和模板模式完全不同。回调基于组合关系来实现，把一个对象传递给另一个对象，是一种对象之间的关系；模板模式基于继承关系来实现，子类重写父类的抽象方法，是一种类之间的关系。 在代码实现上，回调相对于模板模式会更加灵活，主要体现在下面几点： 像 Java 这种只支持单继承的语言，基于模板模式编写的子类，已经继承了一个父类，不再具有继承的能力。 回调可以使用匿名类来创建回调对象，可以不用事先定义类；而模板模式针对不同的实现都要定义不同的子类。 如果某个类中定义了多个模板方法，每个方法都有对应的抽象方法，那即便我们只用到其中的一个模板方法，子类也必须实现所有的抽象方法。而回调就更加灵活，我们只需要往用到的模板方法中注入回调对象即可。","link":"/posts/27871.html"},{"title":"设计模式：状态模式","text":"状态模式一般用来实现状态机，而状态机常用在游戏、工作流引擎等系统开发中。不过，状态机的实现方式有多种，除了状态模式，比较常用的还有分支逻辑法和查表法。 什么是有限状态机？有限状态机，英文翻译是 Finite State Machine，缩写为 FSM，简称为状态机。状态机有 3 个组成部分：状态（State）、事件（Event）、动作（Action）。其中，事件也称为转移条件（Transition Condition）。事件触发状态的转移及动作的执行。不过，动作不是必须的，也可能只转移状态，不执行任何动作。 “超级马里奥”游戏不知道你玩过没有？在游戏中，马里奥可以变身为多种形态，比如小马里奥（Small Mario）、超级马里奥（Super Mario）、火焰马里奥（Fire Mario）、斗篷马里奥（Cape Mario）等等。在不同的游戏情节下，各个形态会互相转化，并相应的增减积分。比如，初始形态是小马里奥，吃了蘑菇之后就会变成超级马里奥，并且增加 100 积分。 实际上，马里奥形态的转变就是一个状态机。其中，马里奥的不同形态就是状态机中的“状态”，游戏情节（比如吃了蘑菇）就是状态机中的“事件”，加减积分就是状态机中的“动作”。 我们如何编程来实现上面的状态机呢？换句话说，如何将上面的状态转移图翻译成代码呢？ 首先写出骨架代码，如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public enum State { SMALL(0), SUPER(1), FIRE(2), CAPE(3); private int value; private State(int value) { this.value = value; } public int getValue() { return this.value; }}public class MarioStateMachine { private int score; private State currentState; public MarioStateMachine() { this.score = 0; this.currentState = State.SMALL; } public void obtainMushRoom() { //TODO } public void obtainCape() { //TODO } public void obtainFireFlower() { //TODO } public void meetMonster() { //TODO } public int getScore() { return this.score; } public State getCurrentState() { return this.currentState; }}public class ApplicationDemo { public static void main(String[] args) { MarioStateMachine mario = new MarioStateMachine(); mario.obtainMushRoom(); int score = mario.getScore(); State state = mario.getCurrentState(); System.out.println(&quot;mario score: &quot; + score + &quot;; state: &quot; + state); }} 状态机实现方式一：分支逻辑法简单直接的实现方式是，参照状态转移图，将每一个状态转移，原模原样地直译成代码。这样编写的代码会包含大量的 if-else 或 switch-case 分支判断逻辑，甚至是嵌套的分支判断逻辑，所以，暂且命名为分支逻辑法。 按照这个实现思路，上面的骨架代码补全一下。补全之后的代码如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class MarioStateMachine { private int score; private State currentState; public MarioStateMachine() { this.score = 0; this.currentState = State.SMALL; } public void obtainMushRoom() { if (currentState.equals(State.SMALL)) { this.currentState = State.SUPER; this.score += 100; } } public void obtainCape() { if (currentState.equals(State.SMALL) || currentState.equals(State.SUPER) ) { this.currentState = State.CAPE; this.score += 200; } } public void obtainFireFlower() { if (currentState.equals(State.SMALL) || currentState.equals(State.SUPER) ) { this.currentState = State.FIRE; this.score += 300; } } public void meetMonster() { if (currentState.equals(State.SUPER)) { this.currentState = State.SMALL; this.score -= 100; return; } if (currentState.equals(State.CAPE)) { this.currentState = State.SMALL; this.score -= 200; return; } if (currentState.equals(State.FIRE)) { this.currentState = State.SMALL; this.score -= 300; return; } } public int getScore() { return this.score; } public State getCurrentState() { return this.currentState; }} 对于简单的状态机来说，分支逻辑这种实现方式是可以接受的。但是，对于复杂的状态机来说，这种实现方式极易漏写或者错写某个状态转移。除此之外，代码中充斥着大量的 if-else 或者 switch-case 分支判断逻辑，可读性和可维护性都很差。 状态机实现方式二：查表法实际上，除了用状态转移图来表示之外，状态机还可以用二维表来表示，如下所示。在这个二维表中，第一维表示当前状态，第二维表示事件，值表示当前状态经过事件之后，转移到的新状态及其执行的动作。 相对于分支逻辑的实现方式，查表法的代码实现更加清晰，可读性和可维护性更好。具体代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public enum Event { GOT_MUSHROOM(0), GOT_CAPE(1), GOT_FIRE(2), MET_MONSTER(3); private int value; private Event(int value) { this.value = value; } public int getValue() { return this.value; }}public class MarioStateMachine { private int score; private State currentState; private static final State[][] transitionTable = { {SUPER, CAPE, FIRE, SMALL}, {SUPER, CAPE, FIRE, SMALL}, {CAPE, CAPE, CAPE, SMALL}, {FIRE, FIRE, FIRE, SMALL} }; private static final int[][] actionTable = { {+100, +200, +300, +0}, {+0, +200, +300, -100}, {+0, +0, +0, -200}, {+0, +0, +0, -300} }; public MarioStateMachine() { this.score = 0; this.currentState = State.SMALL; } public void obtainMushRoom() { executeEvent(Event.GOT_MUSHROOM); } public void obtainCape() { executeEvent(Event.GOT_CAPE); } public void obtainFireFlower() { executeEvent(Event.GOT_FIRE); } public void meetMonster() { executeEvent(Event.MET_MONSTER); } private void executeEvent(Event event) { int stateValue = currentState.getValue(); int eventValue = event.getValue(); this.currentState = transitionTable[stateValue][eventValue]; this.score = actionTable[stateValue][eventValue]; } public int getScore() { return this.score; } public State getCurrentState() { return this.currentState; }} 当修改状态机时，我们只需要修改 transitionTable 和 actionTable 两个二维数组即可。实际上，如果把这两个二维数组存储在配置文件中，当需要修改状态机时，甚至可以不修改任何代码，只需要修改配置文件就可以了。 状态机实现方式三：状态模式在查表法的代码实现中，事件触发的动作只是简单的积分加减，所以，我们用一个 int 类型的二维数组 actionTable 就能表示，二维数组中的值表示积分的加减值。但是，如果要执行的动作并非这么简单，而是一系列复杂的逻辑操作（比如加减积分、写数据库，还有可能发送消息通知等等），就没法用如此简单的二维数组来表示了。也就是说，查表法的实现方式有一定局限性。 状态模式通过将事件触发的状态转移和动作执行，拆分到不同的状态类中，来避免分支判断逻辑。利用状态模式，补全 MarioStateMachine 类，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public interface IMario { State getName(); void obtainMushRoom(MarioStateMachine stateMachine); void obtainCape(MarioStateMachine stateMachine); void obtainFireFlower(MarioStateMachine stateMachine); void meetMonster(MarioStateMachine stateMachine);}public class SmallMario implements IMario { private static final SmallMario instance = new SmallMario(); private SmallMario() {} public static SmallMario getInstance() { return instance; } @Override public State getName() { return State.SMALL; } @Override public void obtainMushRoom(MarioStateMachine stateMachine) { stateMachine.setCurrentState(SuperMario.getInstance()); stateMachine.setScore(stateMachine.getScore() + 100); } @Override public void obtainCape(MarioStateMachine stateMachine) { stateMachine.setCurrentState(CapeMario.getInstance()); stateMachine.setScore(stateMachine.getScore() + 200); } @Override public void obtainFireFlower(MarioStateMachine stateMachine) { stateMachine.setCurrentState(FireMario.getInstance()); stateMachine.setScore(stateMachine.getScore() + 300); } @Override public void meetMonster(MarioStateMachine stateMachine) { // do nothing... }}// 省略SuperMario、CapeMario、FireMario类...public class MarioStateMachine { private int score; private IMario currentState; public MarioStateMachine() { this.score = 0; this.currentState = SmallMario.getInstance(); } public void obtainMushRoom() { this.currentState.obtainMushRoom(this); } public void obtainCape() { this.currentState.obtainCape(this); } public void obtainFireFlower() { this.currentState.obtainFireFlower(this); } public void meetMonster() { this.currentState.meetMonster(this); } public int getScore() { return this.score; } public State getCurrentState() { return this.currentState.getName(); } public void setScore(int score) { this.score = score; } public void setCurrentState(IMario currentState) { this.currentState = currentState; }} 实际上，像游戏这种比较复杂的状态机，包含的状态比较多，我优先推荐使用查表法，而状态模式会引入非常多的状态类，会导致代码比较难维护。相反，像电商下单、外卖下单这种类型的状态机，它们的状态并不多，状态转移也比较简单，但事件触发执行的动作包含的业务逻辑可能会比较复杂，所以，更加推荐使用状态模式来实现。","link":"/posts/52781.html"},{"title":"设计模式：策略模式","text":"策略模式的原理策略模式，英文全称是 Strategy Design Pattern。在 GoF 的《设计模式》一书中，它是这样定义的： Define a family of algorithms, encapsulate each one, and make them interchangeable. Strategy lets the algorithm vary independently from clients that use it. 翻译成中文就是：定义一族算法类，将每个算法分别封装起来，让它们可以互相替换。策略模式可以使算法的变化独立于使用它们的客户端。 策略模式用来解耦策略的定义、创建、使用。实际上，一个完整的策略模式就是由这三个部分组成的。 策略的定义策略类的定义比较简单，包含一个策略接口和一组实现这个接口的策略类。 1234567891011121314151617public interface Strategy { void algorithmInterface();}public class ConcreteStrategyA implements Strategy { @Override public void algorithmInterface() { //具体的算法... }}public class ConcreteStrategyB implements Strategy { @Override public void algorithmInterface() { //具体的算法... }} 策略的创建策略模式会包含一组策略，在使用它们的时候，一般会通过类型（type）来判断创建哪个策略来使用。为了封装创建逻辑，需要对客户端代码屏蔽创建细节，可以把根据 type 创建策略的逻辑抽离出来，放到工厂类中。 123456789101112131415public class StrategyFactory { private static final Map&lt;String, Strategy&gt; strategies = new HashMap&lt;&gt;(); static { strategies.put(&quot;A&quot;, new ConcreteStrategyA()); strategies.put(&quot;B&quot;, new ConcreteStrategyB()); } public static Strategy getStrategy(String type) { if (type == null || type.isEmpty()) { throw new IllegalArgumentException(&quot;type should not be empty.&quot;); } return strategies.get(type); }} 一般来讲，如果策略类是无状态的，不包含成员变量，只是纯粹的算法实现，这样的策略对象是可以被共享使用的，不需要在每次调用 getStrategy() 的时候，都创建一个新的策略对象。针对这种情况，我们可以使用上面这种工厂类的实现方式，事先创建好每个策略对象，缓存到工厂类中，用的时候直接返回。 相反，如果策略类是有状态的，根据业务场景的需要，我们希望每次从工厂方法中，获得的都是新创建的策略对象，而不是缓存好可共享的策略对象。 策略的使用策略模式包含一组可选策略，客户端代码一般如何确定使用哪个策略呢？最常见的是运行时动态确定使用哪种策略，这也是策略模式最典型的应用场景。 这里的“运行时动态”指的是，我们事先并不知道会使用哪个策略，而是在程序运行期间，根据配置、用户输入、计算结果等这些不确定因素，动态决定使用哪种策略。 12345678910111213141516171819202122232425262728293031323334// 策略接口：EvictionStrategy// 策略类：LruEvictionStrategy、FifoEvictionStrategy、LfuEvictionStrategy...// 策略工厂：EvictionStrategyFactorypublic class UserCache { private Map&lt;String, User&gt; cacheData = new HashMap&lt;&gt;(); private EvictionStrategy eviction; public UserCache(EvictionStrategy eviction) { this.eviction = eviction; } //...}// 运行时动态确定，根据配置文件的配置决定使用哪种策略public class Application { public static void main(String[] args) throws Exception { EvictionStrategy evictionStrategy = null; Properties props = new Properties(); props.load(new FileInputStream(&quot;./config.properties&quot;)); String type = props.getProperty(&quot;eviction_type&quot;); evictionStrategy = EvictionStrategyFactory.getEvictionStrategy(type); UserCache userCache = new UserCache(evictionStrategy); //... }}// 非运行时动态确定，在代码中指定使用哪种策略public class Application { public static void main(String[] args) { //... EvictionStrategy evictionStrategy = new LruEvictionStrategy(); UserCache userCache = new UserCache(evictionStrategy); //... }} 如何利用策略模式避免分支判断？实际上，能够移除分支判断逻辑的模式不仅仅有策略模式，状态模式也可以。对于使用哪种模式，具体还要看应用场景来定。 策略模式适用于根据不同类型的动态决定使用哪种策略这样一种应用场景。 先通过一个例子来看下，if-else 或 switch-case 分支判断逻辑是如何产生的。具体的代码如下所示： 123456789101112131415public class OrderService { public double discount(Order order) { double discount = 0.0; OrderType type = order.getType(); if (type.equals(OrderType.NORMAL)) { // 普通订单 //...省略折扣计算算法代码 } else if (type.equals(OrderType.GROUPON)) { // 团购订单 //...省略折扣计算算法代码 } else if (type.equals(OrderType.PROMOTION)) { // 促销订单 //...省略折扣计算算法代码 } return discount; }} 如何来移除掉分支判断逻辑呢？那策略模式就派上用场了。我们使用策略模式对上面的代码重构，将不同类型订单的打折策略设计成策略类，并由工厂类来负责创建策略对象。具体的代码如下所示： 123456789101112131415161718192021222324252627// 策略的定义public interface DiscountStrategy { double calDiscount(Order order);}// 省略NormalDiscountStrategy、GrouponDiscountStrategy、PromotionDiscountStrategy类代码...// 策略的创建public class DiscountStrategyFactory { private static final Map&lt;OrderType, DiscountStrategy&gt; strategies = new HashMap&lt;&gt;(); static { strategies.put(OrderType.NORMAL, new NormalDiscountStrategy()); strategies.put(OrderType.GROUPON, new GrouponDiscountStrategy()); strategies.put(OrderType.PROMOTION, new PromotionDiscountStrategy()); } public static DiscountStrategy getDiscountStrategy(OrderType type) { return strategies.get(type); }}// 策略的使用public class OrderService { public double discount(Order order) { OrderType type = order.getType(); DiscountStrategy discountStrategy = DiscountStrategyFactory.getDiscountStrategy(type); return discountStrategy.calDiscount(order); }} 重构之后的代码就没有了 if-else 分支判断语句了。实际上，这得益于策略工厂类。在工厂类中，我们用 Map 来缓存策略，根据 type 直接从 Map 中获取对应的策略，从而避免 if-else 分支判断逻辑。本质上都是借助“查表法”，根据 type 查表（代码中的 strategies 就是表）替代根据 type 分支判断。 应用案例假设有这样一个需求，希望写一个小程序，实现对一个文件进行排序的功能。文件中只包含整型数，并且，相邻的数字通过逗号来区隔。 你可能会说，这不是很简单嘛，只需要将文件中的内容读取出来，并且通过逗号分割成一个一个的数字，放到内存数组中，然后编写某种排序算法（比如快排），或者直接使用编程语言提供的排序函数，对数组进行排序，最后再将数组中的数据写入文件就可以了。 但是，如果文件很大呢？比如有 10GB 大小，因为内存有限（比如只有 8GB 大小），我们没办法一次性加载文件中的所有数据到内存中，这个时候，就要利用外部排序算法。 如果文件更大，比如有 100GB 大小，为了利用 CPU 多核的优势，可以在外部排序的基础之上进行优化，加入多线程并发排序的功能，这就有点类似“单机版”的 MapReduce。 如果文件非常大，比如有 1TB 大小，即便是单机多线程排序，这也算很慢了。这个时候，可以使用真正的 MapReduce 框架，利用多机的处理能力，提高排序的效率。 先用最简单直接的方式将它实现出来，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940public class Sorter { private static final long GB = 1000 * 1000 * 1000; public void sortFile(String filePath) { // 省略校验逻辑 File file = new File(filePath); long fileSize = file.length(); if (fileSize &lt; 6 * GB) { // [0, 6GB) quickSort(filePath); } else if (fileSize &lt; 10 * GB) { // [6GB, 10GB) externalSort(filePath); } else if (fileSize &lt; 100 * GB) { // [10GB, 100GB) concurrentExternalSort(filePath); } else { // [100GB, ~) mapreduceSort(filePath); } } private void quickSort(String filePath) { // 快速排序 } private void externalSort(String filePath) { // 外部排序 } private void concurrentExternalSort(String filePath) { // 多线程外部排序 } private void mapreduceSort(String filePath) { // 利用MapReduce多机排序 }}public class SortingTool { public static void main(String[] args) { Sorter sorter = new Sorter(); sorter.sortFile(args[0]); }} 如果只是开发一个简单的工具，那上面的代码实现就足够了。毕竟，代码不多，后续修改、扩展的需求也不多，怎么写都不会导致代码不可维护。但是，如果我们是在开发一个大型项目，排序文件只是其中的一个功能模块，就要在代码设计、代码质量上下点儿功夫了。只有每个小的功能模块都写好，整个项目的代码才能不差。 另外如果自己实现一下的话，你会发现，每种排序算法的实现逻辑都比较复杂，代码行数都比较多。所有排序算法的代码实现都堆在 Sorter 一个类中，这就会导致这个类的代码很多。 使用策略模式进行重构，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public interface ISortAlg { void sort(String filePath);}public class QuickSort implements ISortAlg { @Override public void sort(String filePath) { //... }}public class ExternalSort implements ISortAlg { @Override public void sort(String filePath) { //... }}public class ConcurrentExternalSort implements ISortAlg { @Override public void sort(String filePath) { //... }}public class MapReduceSort implements ISortAlg { @Override public void sort(String filePath) { //... }}public class SortAlgFactory { private static final Map&lt;String, ISortAlg&gt; algs = new HashMap&lt;&gt;(); static { algs.put(&quot;QuickSort&quot;, new QuickSort()); algs.put(&quot;ExternalSort&quot;, new ExternalSort()); algs.put(&quot;ConcurrentExternalSort&quot;, new ConcurrentExternalSort()); algs.put(&quot;MapReduceSort&quot;, new MapReduceSort()); } public static ISortAlg getSortAlg(String type) { if (type == null || type.isEmpty()) { throw new IllegalArgumentException(&quot;type should not be empty.&quot;); } return algs.get(type); }}public class Sorter { private static final long GB = 1000 * 1000 * 1000; public void sortFile(String filePath) { // 省略校验逻辑 File file = new File(filePath); long fileSize = file.length(); ISortAlg sortAlg; if (fileSize &lt; 6 * GB) { // [0, 6GB) sortAlg = SortAlgFactory.getSortAlg(&quot;QuickSort&quot;); } else if (fileSize &lt; 10 * GB) { // [6GB, 10GB) sortAlg = SortAlgFactory.getSortAlg(&quot;ExternalSort&quot;); } else if (fileSize &lt; 100 * GB) { // [10GB, 100GB) sortAlg = SortAlgFactory.getSortAlg(&quot;ConcurrentExternalSort&quot;); } else { // [100GB, ~) sortAlg = SortAlgFactory.getSortAlg(&quot;MapReduceSort&quot;); } sortAlg.sort(filePath); }} 可变的部分隔离到了策略工厂类和 Sorter 类中的静态代码段中，当要添加一个新的排序算法时，我们只需要修改策略工厂类和 Sort 类中的静态代码段，其他代码都不需要修改，这样就将代码改动最小化、集中化了。 小结一提到 if-else 分支判断，有人就觉得它是烂代码。如果 if-else 分支判断不复杂、代码不多，这并没有任何问题，毕竟 if-else 分支判断几乎是所有编程语言都会提供的语法，存在即有理由。遵循 KISS 原则，怎么简单怎么来，就是最好的设计。非得用策略模式，搞出 n 多类，反倒是一种过度设计。 一提到策略模式，有人就觉得，它的作用是避免 if-else 分支判断逻辑。实际上，这种认识是很片面的。策略模式主要的作用还是解耦策略的定义、创建和使用，控制代码的复杂度，让每个部分都不至于过于复杂、代码量过多。除此之外，对于复杂代码来说，策略模式还能让其满足开闭原则，添加新策略的时候，最小化、集中化代码改动，减少引入 bug 的风险。 实际上，设计原则和思想比设计模式更加普适和重要。掌握了代码的设计原则和思想，我们能更清楚的了解，为什么要用某种设计模式，就能更恰到好处地应用设计模式。","link":"/posts/45365.html"},{"title":"设计模式：组合模式","text":"组合模式（Composite Pattern），其定义如下： Compose objects into tree structure to represent part-whole hierarchies. Composite lets client treat individual objects and compositions of objects uniformly. 翻译成中文就是：将一组对象组织（Compose）成树形结构，以表示一种“部分 - 整体”的层次结构。组合让客户端可以统一单个对象和组合对象的处理逻辑。 案例分析假设我们有这样一个需求：设计一个类来表示文件系统中的目录，能方便地实现下面这些功能： 动态地添加、删除某个目录下的子目录或文件； 统计指定目录下的文件个数； 统计指定目录下的文件总大小 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// 文件和目录统一用 FileSystemNode 类来表示，并且通过 isFile 属性来区分。public class FileSystemNode { private String path; private boolean isFile; private List&lt;FileSystemNode&gt; subNodes = new ArrayList&lt;&gt;(); public FileSystemNode(String path, boolean isFile) { this.path = path; this.isFile = isFile; } public int countNumOfFiles() { if (isFile) { return 1; } int numOfFiles = 0; for (FileSystemNode fileOrDir : subNodes) { numOfFiles += fileOrDir.countNumOfFiles(); } return numOfFiles; } public long countSizeOfFiles() { if (isFile) { File file = new File(path); if (!file.exists()) return 0; return file.length(); } long sizeofFiles = 0; for (FileSystemNode fileOrDir : subNodes) { sizeofFiles += fileOrDir.countSizeOfFiles(); } return sizeofFiles; } public String getPath() { return path; } public void addSubNode(FileSystemNode fileOrDir) { subNodes.add(fileOrDir); } public void removeSubNode(FileSystemNode fileOrDir) { int size = subNodes.size(); int i = 0; for (; i &lt; size; ++i) { if (subNodes.get(i).getPath().equalsIgnoreCase(fileOrDir.getPath())) { break; } } if (i &lt; size) { subNodes.remove(i); } }} 单纯从功能实现角度来说，上面的代码没有问题，已经实现了我们想要的功能。但是，如果我们开发的是一个大型系统，从扩展性（文件或目录可能会对应不同的操作）、业务建模（文件和目录从业务上是两个概念）、代码的可读性（文件和目录区分对待更加符合人们对业务的认知）的角度来说，我们最好对文件和目录进行区分设计，定义为 File 和 Directory 两个类。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public abstract class FileSystemNode { protected String path; public FileSystemNode(String path) { this.path = path; } public abstract int countNumOfFiles(); public abstract long countSizeOfFiles(); public String getPath() { return path; }}public class File extends FileSystemNode { public File(String path) { super(path); } @Override public int countNumOfFiles() { return 1; } @Override public long countSizeOfFiles() { java.io.File file = new java.io.File(path); if (!file.exists()) return 0; return file.length(); }}public class Directory extends FileSystemNode { private List&lt;FileSystemNode&gt; subNodes = new ArrayList&lt;&gt;(); public Directory(String path) { super(path); } @Override public int countNumOfFiles() { int numOfFiles = 0; for (FileSystemNode fileOrDir : subNodes) { numOfFiles += fileOrDir.countNumOfFiles(); } return numOfFiles; } @Override public long countSizeOfFiles() { long sizeofFiles = 0; for (FileSystemNode fileOrDir : subNodes) { sizeofFiles += fileOrDir.countSizeOfFiles(); } return sizeofFiles; } public void addSubNode(FileSystemNode fileOrDir) { subNodes.add(fileOrDir); } public void removeSubNode(FileSystemNode fileOrDir) { int size = subNodes.size(); int i = 0; for (; i &lt; size; ++i) { if (subNodes.get(i).getPath().equalsIgnoreCase(fileOrDir.getPath())) { break; } } if (i &lt; size) { subNodes.remove(i); } }} 应用举例假设我们在开发一个 OA 系统（办公自动化系统）。公司的组织结构包含部门和员工两种数据类型。其中，部门又可以包含子部门和员工。在数据库中的表结构如下所示： 希望在内存中构建整个公司的人员架构图（部门、子部门、员工的隶属关系），并且提供接口计算出部门的薪资成本（隶属于这个部门的所有员工的薪资和）。 部门包含子部门和员工，这是一种嵌套结构，可以表示成树这种数据结构。计算每个部门的薪资开支这样一个需求，也可以通过在树上的遍历算法来实现。所以，从这个角度来看，这个应用场景可以使用组合模式来设计和实现。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public abstract class HumanResource { protected long id; protected double salary; public HumanResource(long id) { this.id = id; } public long getId() { return id; } public abstract double calculateSalary();}public class Employee extends HumanResource { public Employee(long id, double salary) { super(id); this.salary = salary; } @Override public double calculateSalary() { return salary; }}public class Department extends HumanResource { private List&lt;HumanResource&gt; subNodes = new ArrayList&lt;&gt;(); public Department(long id) { super(id); } @Override public double calculateSalary() { double totalSalary = 0; for (HumanResource hr : subNodes) { totalSalary += hr.calculateSalary(); } this.salary = totalSalary; return totalSalary; } public void addSubNode(HumanResource hr) { subNodes.add(hr); }}// 构建组织架构的代码public class Demo { private static final long ORGANIZATION_ROOT_ID = 1001; private DepartmentRepo departmentRepo; // 依赖注入 private EmployeeRepo employeeRepo; // 依赖注入 public void buildOrganization() { Department rootDepartment = new Department(ORGANIZATION_ROOT_ID); buildOrganization(rootDepartment); } private void buildOrganization(Department department) { List&lt;Long&gt; subDepartmentIds = departmentRepo.getSubDepartmentIds(department.getId()); for (Long subDepartmentId : subDepartmentIds) { Department subDepartment = new Department(subDepartmentId); department.addSubNode(subDepartment); buildOrganization(subDepartment); } List&lt;Long&gt; employeeIds = employeeRepo.getDepartmentEmployeeIds(department.getId()); for (Long employeeId : employeeIds) { double salary = employeeRepo.getEmployeeSalary(employeeId); department.addSubNode(new Employee(employeeId, salary)); } }}","link":"/posts/64507.html"},{"title":"设计模式：观察者模式","text":"原理观察者模式（Observer Design Pattern）也被称为发布订阅模式（Publish-Subscribe Design Pattern）。在 GoF 的《设计模式》一书中，它的定义是这样的： Define a one-to-many dependency between objects so that when one object changes state, all its dependents are notified and updated automatically. 翻译成中文就是：在对象之间定义一个一对多的依赖，当一个对象状态改变的时候，所有依赖的对象都会自动收到通知。 一般情况下，被依赖的对象叫作被观察者（Observable），依赖的对象叫作观察者（Observer）。不过，在实际的项目开发中，这两种对象的称呼是比较灵活的，有各种不同的叫法，比如：Subject-Observer、Publisher-Subscriber、Producer-Consumer、EventEmitter-EventListener、Dispatcher-Listener。不管怎么称呼，只要应用场景符合刚刚给出的定义，都可以看作观察者模式。 实际上，观察者模式是一个比较抽象的模式，根据不同的应用场景和需求，有完全不同的实现方式。先来看下最经典的一种实现方式，其具体的代码如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public interface Subject { void registerObserver(Observer observer); void removeObserver(Observer observer); void notifyObservers(Message message);}public interface Observer { void update(Message message);}public class ConcreteSubject implements Subject { private List&lt;Observer&gt; observers = new ArrayList&lt;Observer&gt;(); @Override public void registerObserver(Observer observer) { observers.add(observer); } @Override public void removeObserver(Observer observer) { observers.remove(observer); } @Override public void notifyObservers(Message message) { for (Observer observer : observers) { observer.update(message); } }}public class ConcreteObserverOne implements Observer { @Override public void update(Message message) { //TODO: 获取消息通知，执行自己的逻辑... System.out.println(&quot;ConcreteObserverOne is notified.&quot;); }}public class ConcreteObserverTwo implements Observer { @Override public void update(Message message) { //TODO: 获取消息通知，执行自己的逻辑... System.out.println(&quot;ConcreteObserverTwo is notified.&quot;); }}public class Demo { public static void main(String[] args) { ConcreteSubject subject = new ConcreteSubject(); subject.registerObserver(new ConcreteObserverOne()); subject.registerObserver(new ConcreteObserverTwo()); subject.notifyObservers(new Message()); }} 观察者模式UML类图如下： Subject（目标）：被观察者，它会向其他对象发送值得关注的事件。 事件会在发布者自身状态改变或执行特定行为后发生。 发布者中包含一个允许新订阅者加入和当前订阅者离开列表的订阅构架。 ConcreteSubject（具体目标）：具体目标是目标类的子类。 Observer（观察者）：观察者将对观察目标的改变做出反应，观察者一般定义为接口，该接口声明了更新数据的方法 update()，因此又称为抽象观察者。 ConcreteObserver（具体观察者）：可以执行一些操作来回应发布者的通知。 所有具体订阅者类都实现了同样的接口， 因此发布者不需要与具体类相耦合。 应用场景观察者模式的应用场景非常广泛，小到代码层面的解耦，大到架构层面的系统解耦，再或者一些产品的设计思路，都有这种模式的影子，比如，邮件订阅、RSS Feeds，本质上都是观察者模式。 不同的应用场景和需求下，这个模式也有截然不同的实现方式。有同步阻塞的实现方式，也有异步非阻塞的实现方式；有进程内的实现方式，也有跨进程的实现方式。 案例说明假设我们在开发一个 P2P 投资理财系统，用户注册成功之后，我们会给用户发放投资体验金。代码实现大致是下面这个样子的： 123456789101112public class UserController { private UserService userService; // 依赖注入 private PromotionService promotionService; // 依赖注入 public Long register(String telephone, String password) { //省略输入参数的校验代码 //省略userService.register()异常的try-catch代码 long userId = userService.register(telephone, password); promotionService.issueNewUserExperienceCash(userId); return userId; }} 虽然注册接口做了两件事情，注册和发放体验金，违反单一职责原则，但是，如果没有扩展和修改的需求，现在的代码实现是可以接受的。如果非得用观察者模式，就需要引入更多的类和更加复杂的代码结构，反倒是一种过度设计。 相反，如果需求频繁变动，比如，用户注册成功之后，不再发放体验金，而是改为发放优惠券，并且还要给用户发送一封“欢迎注册成功”的站内信。这种情况下，我们就需要频繁地修改 register() 函数中的代码，违反开闭原则。而且，如果注册成功之后需要执行的后续操作越来越多，那 register() 函数的逻辑会变得越来越复杂，也就影响到代码的可读性和可维护性。 这个时候，观察者模式就能派上用场了。利用观察者模式，对上面的代码进行了重构，重构之后的代码如下所示： 12345678910111213141516171819202122232425262728293031323334353637public interface RegObserver { void handleRegSuccess(long userId);}public class RegPromotionObserver implements RegObserver { private PromotionService promotionService; // 依赖注入 @Override public void handleRegSuccess(long userId) { promotionService.issueNewUserExperienceCash(userId); }}public class RegNotificationObserver implements RegObserver { private NotificationService notificationService; @Override public void handleRegSuccess(long userId) { notificationService.sendInboxMessage(userId, &quot;Welcome...&quot;); }}public class UserController { private UserService userService; // 依赖注入 private List&lt;RegObserver&gt; regObservers = new ArrayList&lt;&gt;(); // 一次性设置好，之后也不可能动态的修改 public void setRegObservers(List&lt;RegObserver&gt; observers) { regObservers.addAll(observers); } public Long register(String telephone, String password) { //省略输入参数的校验代码 //省略userService.register()异常的try-catch代码 long userId = userService.register(telephone, password); for (RegObserver observer : regObservers) { observer.handleRegSuccess(userId); } return userId; }} 当我们需要添加新的观察者的时候，比如，用户注册成功之后，推送用户注册信息给大数据征信系统，基于观察者模式的代码实现，UserController 类的 register() 函数完全不需要修改，只需要再添加一个实现了 RegObserver 接口的类，并且通过 setRegObservers() 函数将它注册到 UserController 类中即可。 不过，你可能会说，当我们把发送体验金替换为发送优惠券的时候，需要修改 RegPromotionObserver 类中 handleRegSuccess() 函数的代码，这还是违反开闭原则呀？你说得没错，不过，相对于 register() 函数来说，handleRegSuccess() 函数的逻辑要简单很多，修改更不容易出错，引入 bug 的风险更低。","link":"/posts/54052.html"},{"title":"设计模式：装饰器模式","text":"装饰器模式（Decorator Pattern）也叫作包装器模式（Wrapper Pattern），其定义如下： Attach additional responsibilities to an object dynamicallykeeping the same interface.Decorators provide a flexible alternativeto subclassing for extending functionality. 翻译中文：在不改变原有对象的基础上，动态地给一个对象添加一些额外的职责。 Java IO 类的“奇怪”用法Java IO 类库非常庞大和复杂，有几十个类，负责 IO 数据的读取和写入。如果对 Java IO 类做一下分类，我们可以从下面两个维度将它划分为四类。具体如下所示： 针对不同的读取和写入场景，Java IO 又在这四个父类基础之上，扩展出了很多子类。具体如下所示： 在初学 Java 的时候，曾经对 Java IO 的一些用法产生过很大疑惑，比如下面这样一段代码。我们打开文件 test.txt，从中读取数据。其中，InputStream 是一个抽象类，FileInputStream 是专门用来读取文件流的子类。BufferedInputStream 是一个支持带缓存功能的数据读取类，可以提高数据读取的效率。 123456InputStream in = new FileInputStream(&quot;/user/wangzheng/test.txt&quot;);InputStream bin = new BufferedInputStream(in);byte[] data = new byte[128];while (bin.read(data) != -1) { //...} 初看上面的代码，我们会觉得 Java IO 的用法比较麻烦，需要先创建一个FileInputStream对象，然后再传递给 BufferedInputStream 对象来使用。我在想，Java IO 为什么不设计一个继承 FileInputStream 并且支持缓存的 BufferedFileInputStream 类呢？这样我们就可以像下面的代码中这样，直接创建一个 BufferedFileInputStream 类对象，打开文件读取数据，用起来岂不是更加简单？ 基于继承的设计方案如果 InputStream 只有一个子类 FileInputStream 的话，那我们在 FileInputStream 基础之上，再设计一个孙子类 BufferedFileInputStream，也算是可以接受的，毕竟继承结构还算简单。但实际上，继承 InputStream 的子类有很多。我们需要给每一个 InputStream 的子类，再继续派生支持缓存读取的子类。 除了支持缓存读取之外，如果我们还需要对功能进行其他方面的增强，比如下面的 DataInputStream 类，支持按照基本数据类型（int、boolean、long 等）来读取数据。 123FileInputStream in = new FileInputStream(&quot;/user/wangzheng/test.txt&quot;);DataInputStream din = new DataInputStream(in);int data = din.readInt(); 在这种情况下，如果我们继续按照继承的方式来实现的话，就需要再继续派生出 DataFileInputStream、DataPipedInputStream 等类。如果我们还需要既支持缓存、又支持按照基本类型读取数据的类，那就要再继续派生出 BufferedDataFileInputStream、BufferedDataPipedInputStream 等 n 多类。这还只是附加了两个增强功能，如果我们需要附加更多的增强功能，那就会导致组合爆炸，类继承结构变得无比复杂，代码既不好扩展，也不好维护。 基于装饰器模式的设计方案针对刚刚的继承结构过于复杂的问题，我们可以通过将继承关系改为组合关系来解决。下面的代码展示了 Java IO 的这种设计思路。不过，我对代码做了简化，只抽象出了必要的代码结构，如果你感兴趣的话，可以直接去查看 JDK 源码。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public abstract class InputStream { //... public int read(byte b[]) throws IOException { return read(b, 0, b.length); } public int read(byte b[], int off, int len) throws IOException { //... } public long skip(long n) throws IOException { //... } public int available() throws IOException { return 0; } public void close() throws IOException {} public synchronized void mark(int readlimit) {} public synchronized void reset() throws IOException { throw new IOException(&quot;mark/reset not supported&quot;); } public boolean markSupported() { return false; }}public class BufferedInputStream extends InputStream { protected volatile InputStream in; protected BufferedInputStream(InputStream in) { this.in = in; } //...实现基于缓存的读数据接口... }public class DataInputStream extends InputStream { protected volatile InputStream in; protected DataInputStream(InputStream in) { this.in = in; } //...实现读取基本类型数据的接口} 从 Java IO 的设计来看，装饰器模式相对于简单的组合关系，还有两个比较特殊的地方。 第一个比较特殊的地方是：装饰器类和原始类继承同样的父类，这样我们可以对原始类“嵌套”多个装饰器类。比如，下面这样一段代码，我们对 FileInputStream 嵌套了两个装饰器类：BufferedInputStream 和 DataInputStream，让它既支持缓存读取，又支持按照基本数据类型来读取数据。 1234InputStream in = new FileInputStream(&quot;/user/wangzheng/test.txt&quot;);InputStream bin = new BufferedInputStream(in);DataInputStream din = new DataInputStream(bin);int data = din.readInt(); 第二个比较特殊的地方是：装饰器类是对功能的增强，这也是装饰器模式应用场景的一个重要特点。符合“组合关系”这种代码结构的设计模式有很多，就拿比较相似的代理模式和装饰器模式来说吧，代理模式中，代理类附加的是跟原始类无关的功能，而在装饰器模式中，装饰器类附加的是跟原始类相关的增强功能。 实际上，如果去查看 JDK 的源码，你会发现，BufferedInputStream、DataInputStream 并非继承自 InputStream，而是另外一个叫 FilterInputStream 的类。那这又是出于什么样的设计意图，才引入这样一个类呢？ 我们再重新来看一下 BufferedInputStream 类的代码。InputStream 是一个抽象类而非接口，而且它的大部分函数（比如 read()、available()）都有默认实现，按理来说，我们只需要在 BufferedInputStream 类中重新实现那些需要增加缓存功能的函数就可以了，其他函数继承 InputStream 的默认实现。但实际上，这样做是行不通的。 对于即便是不需要增加缓存功能的函数来说，BufferedInputStream 还是必须把它重新实现一遍，简单包裹对 InputStream 对象的函数调用。具体的代码示例如下所示。如果不重新实现，那 BufferedInputStream 类就无法将最终读取数据的任务，委托给传递进来的 InputStream 对象来完成。这一部分稍微有点不好理解，你自己多思考一下。 1234567891011public class BufferedInputStream extends InputStream { protected volatile InputStream in; protected BufferedInputStream(InputStream in) { this.in = in; } // f()函数不需要增强，只是重新调用一下InputStream in对象的f() public void f() { in.f(); } } 实际上，DataInputStream 也存在跟 BufferedInputStream 同样的问题。为了避免代码重复，Java IO 抽象出了一个装饰器父类 FilterInputStream，代码实现如下所示。 12345678910111213141516171819202122232425262728293031323334public class FilterInputStream extends InputStream { protected volatile InputStream in; protected FilterInputStream(InputStream in) { this.in = in; } public int read() throws IOException { return in.read(); } public int read(byte b[]) throws IOException { return read(b, 0, b.length); } public int read(byte b[], int off, int len) throws IOException { return in.read(b, off, len); } public long skip(long n) throws IOException { return in.skip(n); } public int available() throws IOException { return in.available(); } public void close() throws IOException { in.close(); } public synchronized void mark(int readlimit) { in.mark(readlimit); } public synchronized void reset() throws IOException { in.reset(); } public boolean markSupported() { return in.markSupported(); }} InputStream 的所有的装饰器类（BufferedInputStream、DataInputStream）都继承自这个装饰器父类。这样，装饰器类只需要实现它需要增强的方法就可以了，其他方法继承装饰器父类的默认实现。 总结装饰器模式主要解决继承关系过于复杂的问题，通过组合来替代继承。它主要的作用是给原始类添加增强功能。这也是判断是否该用装饰器模式的一个重要的依据。 除此之外，装饰器模式还有一个特点，那就是可以对原始类嵌套使用多个装饰器。为了满足这个应用场景，在设计的时候，装饰器类需要跟原始类继承相同的抽象类或者接口。","link":"/posts/13902.html"},{"title":"设计模式：解释器模式","text":"原理解释器模式的英文翻译是Interpreter Design Pattern。在 GoF 的《设计模式》一书中，它是这样定义的： Interpreter pattern is used to defines a grammatical representation for a language and provides an interpreter to deal with this grammar. 翻译成中文就是：解释器模式为某个语言定义它的语法（或者叫文法）表示，并定义一个解释器用来处理这个语法。 从广义上来讲，只要能承载信息的载体，都可以称之为“语言”，比如，哑语、摩斯密码等。要想了解“语言”表达的信息，就必须定义相应的语法规则。书写者就可以根据语法规则书写“句子”，阅读者根据语法规则来阅读“句子”，这样才能做到信息的正确传递。 解释器模式就是用来实现根据语法规则解读“句子”的解释器。 UML类图 解释器模式主要包含4个角色，如下： 抽象表达式（IExpression）：负责定义一个解释方法interpret，交由具体子类进行具体解释。 终结符表达式（TerminalExpression）：实现文法中与终结符有关的解释操作。文法中的每一个终结符都有一个具体终结表达式与之相对应，比如公式R=R1+R2，R1和R2就是终结符，对应的解析R1和R2的解释器就是终结符表达式。通常一个解释器模式中只有一个终结符表达式，但有多个实例，对应不同的终结符（如R1、R2）。 非终结符表达式（NonterminalExpression）：实现文法中与非终结符有关的解释操作。文法中的每条规则都对应一个非终结符表达式。非终结符表达式一般是文法中的运算符或者其他关键字，比如在公式R=R1+R2中，“+”就是非终结符，解析“+”的解释器就是一个非终结符表达式。非终结符表达式根据逻辑的复杂程度而增加，原则上每个文法规则都对应一个非终结符表达式。 上下文环境类（Context）：包含解释器之外的全局信息。它一般用来存放文法中各个终结符所对应的具体值，比如R=R1+R2，给R1赋值100，给R2赋值200，这些信息需要被存放到环境中。 案例一般来讲，监控系统支持开发者自定义告警规则，比如我们可以用下面这样一个表达式，来表示一个告警规则，它表达的意思是：每分钟 API 总出错数超过 100 或者每分钟 API 总调用数超过 10000 就触发告警。 1api_error_per_minute &gt; 100 || api_count_per_minute &gt; 10000 为了简化讲解和代码实现，我们假设自定义的告警规则只包含“||、&amp;&amp;、&gt;、&lt;、==”这五个运算符，其中，“&gt;、&lt;、==”运算符的优先级高于“||、&amp;&amp;”运算符，“&amp;&amp;”运算符优先级高于“||”。在表达式中，任意元素之间需要通过空格来分隔。除此之外，用户可以自定义要监控的 key，比如前面的 api_error_per_minute、api_count_per_minute。 具体代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185public interface Expression { boolean interpret(Context context);}public class GreaterExpression implements Expression { private String key; private Integer value; public GreaterExpression(String strExpression) { String[] elements = strExpression.trim().split(&quot;\\\\s+&quot;); if (elements.length != 3 || !elements[1].trim().equals(&quot;&gt;&quot;)) { throw new RuntimeException(&quot;Expression is invalid: &quot; + strExpression); } this.key = elements[0].trim(); this.value = Integer.parseInt(elements[2].trim()); } public GreaterExpression(String key, Integer value) { this.key = key; this.value = value; } @Override public boolean interpret(Context context) { if (context.get(key) == null) { return false; } Integer curValue = context.get(key); return curValue &gt; value; }}public class LessExpression implements Expression { private String key; private Integer value; public LessExpression(String strExpression) { String[] elements = strExpression.trim().split(&quot;\\\\s+&quot;); if (elements.length != 3 || !elements[1].trim().equals(&quot;&lt;&quot;)) { throw new RuntimeException(&quot;Expression is invalid: &quot; + strExpression); } this.key = elements[0].trim(); this.value = Integer.parseInt(elements[2].trim()); } public LessExpression(String key, Integer value) { this.key = key; this.value = value; } @Override public boolean interpret(Context context) { if (context.get(key) == null) { return false; } Integer curValue = context.get(key); return curValue &lt; value; }}public class EqualExpression implements Expression { private String key; private Integer value; public EqualExpression(String strExpression) { String[] elements = strExpression.trim().split(&quot;\\\\s+&quot;); if (elements.length != 3 || !elements[1].trim().equals(&quot;==&quot;)) { throw new RuntimeException(&quot;Expression is invalid: &quot; + strExpression); } this.key = elements[0].trim(); this.value = Integer.parseInt(elements[2].trim()); } public EqualExpression(String key, Integer value) { this.key = key; this.value = value; } @Override public boolean interpret(Context context) { if (context.get(key) == null) { return false; } Integer curValue = context.get(key); return curValue == value; }}public class AndExpression implements Expression { private List&lt;Expression&gt; expressions = new ArrayList&lt;&gt;(); public AndExpression(String strAndExpression) { String[] strExpressions = strAndExpression.split(&quot;&amp;&amp;&quot;); for (String strExpr : strExpressions) { if (strExpr.contains(&quot;&gt;&quot;)) { expressions.add(new GreaterExpression(strExpr)); } else if (strExpr.contains(&quot;&lt;&quot;)) { expressions.add(new LessExpression(strExpr)); } else if (strExpr.contains(&quot;==&quot;)) { expressions.add(new EqualExpression(strExpr)); } else { throw new RuntimeException(&quot;Expression is invalid: &quot; + strAndExpression); } } } public AndExpression(List&lt;Expression&gt; expressions) { this.expressions.addAll(expressions); } @Override public boolean interpret(Context context) { for (Expression expr : expressions) { if (!expr.interpret(context)) { return false; } } return true; }}public class OrExpression implements Expression { private List&lt;Expression&gt; expressions = new ArrayList&lt;&gt;(); public OrExpression(String strAndExpression) { String[] strExpressions = strAndExpression.split(&quot;\\\\|\\\\|&quot;); for (String andExpr : strExpressions) { expressions.add(new AndExpression(andExpr)); } } public OrExpression(List&lt;Expression&gt; expressions) { this.expressions.addAll(expressions); } @Override public boolean interpret(Context context) { for (Expression expr : expressions) { if (expr.interpret(context)) { return true; } } return false; }}public class AlertRuleInterpreter { private Expression expression; public AlertRuleInterpreter(String ruleExpression){ this.expression = new OrExpression(ruleExpression); } public boolean interpret(Context context){ return expression.interpret(context); }}public class ApplicationMain { public static void main(String[] args) { String rule = &quot;key1 &gt; 100 &amp;&amp; key2 &lt; 30 || key3 &lt; 100 || key4 == 88&quot;; AlertRuleInterpreter interpreter = new AlertRuleInterpreter(rule); Context context = new Context(); context.put(&quot;key1&quot;,101); context.put(&quot;key3&quot;,121); context.put(&quot;key4&quot;,88); boolean alter = interpreter.interpret(context); System.out.println(alter); }}","link":"/posts/62397.html"},{"title":"设计模式：访问者模式","text":"访问者模式在实际的软件开发中很少被用到，在没有特别必要的情况下，建议不要使用访问者模式。 原理访问者者模式的英文翻译是 Visitor Design Pattern。在 GoF 的《设计模式》一书中，它是这么定义的： Allows for one or more operation to be applied to a set of objects at runtime, decoupling the operations from the object structure. 翻译成中文就是：允许一个或者多个操作应用到一组对象上，解耦操作和对象本身。 假设我们从网站上爬取了很多资源文件，它们的格式有三种：PDF、PPT、Word。我们现在要开发一个工具来处理这批资源文件。这个工具的其中一个功能是，把这些资源文件中的文本内容抽取出来放到 txt 文件中。如果让你来实现，你会怎么来做呢？ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public abstract class ResourceFile{ protected String filePath; public ResourceFile(String filePath){ this.filePath = filePath; }}public class PPTFile extends ResourceFile{ public PPTFile(String filePath){ super(filePath); } public void extract2txt(){ System.out.println(&quot;Extract PTT.&quot;); }}public class PdfFile extends ResourceFile{ public PdfFile(String filePath){ super(filePath); } public void extract2txt(){ System.out.println(&quot;Extract PDF.&quot;); }}public class WordFile extends ResourceFile{ public WordFile(String filePath){ super(filePath); } public void extract2txt(){ System.out.println(&quot;Extract WORD.&quot;); }}public class ToolApplication{ public static void main(String[] args){ List&lt;ResourceFile&gt; resourceFiles = listAllResourceFiles(args[0]); for(ResourceFile resourceFile : resourceFiles){ resourceFile.extract2txt(); } } private static List&lt;ResourceFile&gt; listAllResourceFiles(String resourceDirectory){ List&lt;ResourceFile&gt; resourceFiles = new ArrayList&lt;ResourceFile&gt;(); resourceFiles.add(new PdfFile(&quot;a.pdf&quot;)); resourceFiles.add(new WordFile(&quot;b.word&quot;)); resourceFiles.add(new PTTFile(&quot;c.ppt&quot;)); }} 如果工具的功能不停地扩展，不仅要能抽取文本内容，还要支持压缩、提取文件元信息构建索引等一系列的功能，如果继续按照上面的实现思路，就会存在这样几个问题： 违背开闭原则，添加一个新的功能，所有类的代码都要需要修改； 虽然功能增加，每个类的代码都不断膨胀，可读性和可维护性都变差了； 把所有比较上层的业务逻辑都耦合到PdfFile、PPTFile、WordFile类中，导致这些类的职责不够单一，变成了大杂烩。 针对上面的问题，常用的解决方法就是拆分解耦，把业务操作跟具体的数据结构解耦，设计成独立的类。按照访问者模式的演进思路来对上面的代码进行重构，重构后的代码如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class Extractor{ public void extract2txt(PPTFile pptFile){ System.out.println(&quot;Extract PPT.&quot;); } public void extract2txt(PdfFile pdfFile){ System.out.println(&quot;Extract PDF.&quot;); } public void extract2txt(WordFile wordFile){ System.out.println(&quot;Extract WORD.&quot;); }}public abstract class ResourceFile{ protected String filePath; public ResourceFile(String filePath){ this.filePath = filePath; } abstract public void accept(Extractor extrator);}public class PPTFile extends ResourceFile{ public PPTFile(String filePath){ super(filePath); } public void accept(Extractor extrator){ extrator.extract2txt(this); }}// PdfFile、WordFile跟PPTFile类似public class ToolApplication{ public static void main(String[] args){ Extractor extrator = new Extractor(); List&lt;ResourceFile&gt; resourceFiles = listAllResourceFiles(args[0]); for(ResourceFile resourceFile : resourceFiles){ resourceFile.accept(extrator); } } private static List&lt;ResourceFile&gt; listAllResourceFiles(String resourceDirectory){ List&lt;ResourceFile&gt; resourceFiles = new ArrayList&lt;ResourceFile&gt;(); resourceFiles.add(new PdfFile(&quot;a.pdf&quot;)); resourceFiles.add(new WordFile(&quot;b.word&quot;)); resourceFiles.add(new PTTFile(&quot;c.ppt&quot;)); }} 现在，如果要继续添加新的功能，比如前面提到的压缩功能，根据不同的文件类型，使用不同的压缩算法来压缩资源文件，那我们该如何实现呢？我们需要实现一个类似 Extractor 类的新类 Compressor 类，在其中定义三个重载函数，实现对不同类型资源文件的压缩。除此之外，我们还要在每个资源文件类中定义新的 accept 重载函数。具体的代码如下所示： 1234567891011121314151617181920212223242526public abstract class ResourceFile{ protected String filePath; public ResourceFile(String filePath){ this.filePath = filePath; } abstract public void accept(Extractor extrator); abstract public void accept(Compressor compressor);}public class PPTFile extends ResourceFile{ public PPTFile(String filePath){ super(filePath); } public void accept(Extractor extrator){ extrator.extract2txt(this); } public void accept(Compressor compressor){ compressor.compressor(this); }} 上面代码还是存在一些问题，添加一个新的业务，还是需要修改每个资源文件类，违反了开闭原则。 针对这个问题，抽象出来一个 Visitor 接口，包含是三个命名非常通用的 visit() 重载函数，分别处理三种不同类型的资源文件。具体做什么业务处理，由实现这个 Visitor 接口的具体的类来决定，比如 Extractor 负责抽取文本内容，Compressor 负责压缩。当我们新添加一个业务功能的时候，资源文件类不需要做任何修改，只需要修改 ToolApplication 的代码就可以了。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879public interface Visitor{ void visit(FdfFile pdfFile); void visit(PPTFile pptFile); void visit(WordFile wordFile);}public class Extractor implements Visitor{ void visit(FdfFile pdfFile){ System.out.println(&quot;Extract PDF.&quot;); } void visit(PPTFile pptFile){ System.out.println(&quot;Extract PPT.&quot;); } void visit(WordFile wordFile){ System.out.println(&quot;Extract WORD.&quot;); }}public class Compressor implements Visitor{ void visit(FdfFile pdfFile){ System.out.println(&quot;Compressor PDF.&quot;); } void visit(PPTFile pptFile){ System.out.println(&quot;Compressor PPT.&quot;); } void visit(WordFile wordFile){ System.out.println(&quot;Compressor WORD.&quot;); }}public abstract class ResourceFile{ protected String filePath; public ResourceFile(String filePath){ this.filePath = filePath; } abstract public void accept(Visitor visitor);}public class PPTFile extends ResourceFile{ public PPTFile(String filePath){ super(filePath); } public void accept(Visitor visitor){ visitor.visit(this); }}public class ToolApplication{ public static void main(String[] args){ Extractor extrator = new Extractor(); List&lt;ResourceFile&gt; resourceFiles = listAllResourceFiles(args[0]); for(ResourceFile resourceFile : resourceFiles){ resourceFile.accept(extrator); } Compressor compressor = new Compressor(); List&lt;ResourceFile&gt; resourceFiles = listAllResourceFiles(args[0]); for(ResourceFile resourceFile : resourceFiles){ resourceFile.accept(compressor); } } private static List&lt;ResourceFile&gt; listAllResourceFiles(String resourceDirectory){ List&lt;ResourceFile&gt; resourceFiles = new ArrayList&lt;ResourceFile&gt;(); resourceFiles.add(new PdfFile(&quot;a.pdf&quot;)); resourceFiles.add(new WordFile(&quot;b.word&quot;)); resourceFiles.add(new PTTFile(&quot;c.ppt&quot;)); }} 访问者模式UML类图： 应用场景一般来说，访问者模式针对的是一组类型不同的对象（PdfFile、PPTFile、WordFile）。不过，尽管这组对象的类型是不同的，但是，它们继承相同的父类（ResourceFile）或者实现相同的接口。 在不同的应用场景下，\b我们需要对这组对象进行一系列不相关的业务操作（抽取文本、压缩等），但为了避免不断添加功能导致类（PdfFile、PPTFile、WordFile）不断膨胀，职责越来越不单一，以及避免频繁地添加功能导致的频繁代码修改，我们使用访问者模式，将对象与操作解耦，将这些业务操作抽离出来，定义在独立细分的访问者类（Extractor、Compressor）中。","link":"/posts/11755.html"},{"title":"设计模式：适配器模式","text":"概述适配器模式（Adapter Design Pattern）其定义如下： Convert the interface of a class into another interface clients expect. Adapter lets classes work together that couldn’t otherwise because of incompatible interfaces. 翻译中文就是：将一个类的接口变成客户端所期望的另一种接口，从而使原本因接口不匹配而导致无法在一起工作的两个类能够一起工作。 当前系统存在两种接口A和B，客户端只支持访问A接口，但是当前系统没有A接口对象，有B接口对象，而客户端无法识别B接口，因此需要通过一个适配器C，将B接口内容转换成A接口，从而使得客户端能够从A接口获取B接口的内容。 在软件开发中，基本上任何问题都可以通过增加一个中间层来解决。适配器模式其实就是一个中间层。综上，适配器模式起着转化/委托的作用，将一种接口转化为另一种符合需求的接口。 适配器模式有3种形式：类适配器、对象适配器。适配器模式一般包含3个角色。 目标角色（ITarget）：也就是我们期望的接口。 源角色（Adaptee）：存在于系统中，是指内容满足客户需求（需转换）但接口不匹配的接口实例。 适配器（Adapter）：将Adaptee转化为目标角色ITarget的类实例。 客户端需要访问的是ITarget接口，但ITarget接口没有一个实例符合需求，而Adaptee实例符合需求，但是客户端无法直接使用Adaptee（接口不兼容）；因此，需要一个Adapter来进行中转，使Adaptee能转化为ITarget接口的形式。 类适配器类适配器的原理就是通过继承来实现适配器功能。具体做法是，让Adapter实现ITarget接口，并且继承Adaptee，这样Adapter就具备了ITarget和Adaptee的特性，可以将两者进行转化。 对象适配器对象适配器的原理就是通过组合来实现适配器功能。具体做法是，首先让Adapter实现ITarget接口，然后内部持有Adaptee实例，最后在ITarget接口规定的方法内转换Adaptee。 案例分析Java 中有很多日志框架，比较常用的有 log4j、logback，以及 JDK 提供的 JUL(java.util.logging) 和 Apache 的 JCL(Jakarta Commons Logging) 等。大部分日志框架都提供了相似的功能，比如按照不同级别（debug、info、warn、erro……）打印日志等，但它们却并没有实现统一的接口。 Slf4j提供了统一的接口定义，还提供了针对不同日志框架的适配器。对不同日志框架的接口进行二次封装，适配成统一的 Slf4j 接口定义。具体的代码示例如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162// slf4j统一的接口定义package org.slf4j;public interface Logger { public boolean isTraceEnabled(); public void trace(String msg); public void trace(String format, Object arg); public void trace(String format, Object arg1, Object arg2); public void trace(String format, Object[] argArray); public void trace(String msg, Throwable t); public boolean isDebugEnabled(); public void debug(String msg); public void debug(String format, Object arg); public void debug(String format, Object arg1, Object arg2) public void debug(String format, Object[] argArray) public void debug(String msg, Throwable t); //...省略info、warn、error等一堆接口}// log4j日志框架的适配器// Log4jLoggerAdapter实现了LocationAwareLogger接口，// 其中LocationAwareLogger继承自Logger接口，// 也就相当于Log4jLoggerAdapter实现了Logger接口。package org.slf4j.impl;public final class Log4jLoggerAdapter extends MarkerIgnoringBase implements LocationAwareLogger, Serializable { final transient org.apache.log4j.Logger logger; // log4j public boolean isDebugEnabled() { return logger.isDebugEnabled(); } public void debug(String msg) { logger.log(FQCN, Level.DEBUG, msg, null); } public void debug(String format, Object arg) { if (logger.isDebugEnabled()) { FormattingTuple ft = MessageFormatter.format(format, arg); logger.log(FQCN, Level.DEBUG, ft.getMessage(), ft.getThrowable()); } } public void debug(String format, Object arg1, Object arg2) { if (logger.isDebugEnabled()) { FormattingTuple ft = MessageFormatter.format(format, arg1, arg2); logger.log(FQCN, Level.DEBUG, ft.getMessage(), ft.getThrowable()); } } public void debug(String format, Object[] argArray) { if (logger.isDebugEnabled()) { FormattingTuple ft = MessageFormatter.arrayFormat(format, argArray); logger.log(FQCN, Level.DEBUG, ft.getMessage(), ft.getThrowable()); } } public void debug(String msg, Throwable t) { logger.log(FQCN, Level.DEBUG, msg, t); } //...省略一堆接口的实现...} 在开发业务系统或者开发框架、组件的时候，统一使用 Slf4j 提供的接口来编写打印日志的代码，具体使用哪种日志框架实现（log4j、logback……），是可以动态地指定的，只需要将相应的 SDK 导入到项目中即可。 如果一些老的项目没有使用 Slf4j，而是直接使用比如 JCL 来打印日志，那如果想要替换成其他日志框架，比如 log4j，该怎么办呢？实际上，Slf4j 不仅仅提供了从其他日志框架到 Slf4j 的适配器，还提供了反向适配器，也就是从 Slf4j 到其他日志框架的适配。我们可以先将 JCL 切换为 Slf4j，然后再将 Slf4j 切换为 log4j。经过两次适配器的转换，我们就能成功将 log4j 切换为了 logback。 应用场景一般来说，适配器模式可以看作一种“补偿模式”，用来补救设计上的缺陷。应用这种模式算是“无奈之举”。如果在设计初期，我们就能协调规避接口不兼容的问题，那这种模式就没有应用的机会了。 封装有缺陷的接口设计假设我们依赖的外部系统在接口设计方面有缺陷（比如包含大量静态方法），引入之后会影响到我们自身代码的可测试性。为了隔离设计上的缺陷，我们希望对外部系统提供的接口进行二次封装，抽象出更好的接口设计，这个时候就可以使用适配器模式了。 1234567891011121314151617181920212223242526272829303132333435363738public class CD { //这个类来自外部sdk，我们无权修改它的代码 //... public static void staticFunction1() { //... } public void uglyNamingFunction2() { //... } public void tooManyParamsFunction3(int paramA, int paramB, ...) { //... } public void lowPerformanceFunction4() { //... }} // 使用适配器模式进行重构public class ITarget { void function1(); void function2(); void fucntion3(ParamsWrapperDefinition paramsWrapper); void function4(); //...} // 注意：适配器类的命名不一定非得末尾带Adaptorpublic class CDAdaptor extends CD implements ITarget { //... public void function1() { super.staticFunction1(); } public void function2() { super.uglyNamingFucntion2(); } public void function3(ParamsWrapperDefinition paramsWrapper) { super.tooManyParamsFunction3(paramsWrapper.getParamA(), ...); } public void function4() { //...reimplement it... }} 统一多个类的接口设计某个功能的实现依赖多个外部系统（或者说类）。通过适配器模式，将它们的接口适配为统一的接口定义，然后我们就可以使用多态的特性来复用代码逻辑。 假设我们的系统要对用户输入的文本内容做敏感词过滤，为了提高过滤的召回率，我们引入了多款第三方敏感词过滤系统，依次对用户输入的内容进行过滤，过滤掉尽可能多的敏感词。但是，每个系统提供的过滤接口都是不同的。这就意味着我们没法复用一套逻辑来调用各个系统。这个时候，我们就可以使用适配器模式，将所有系统的接口适配为统一的接口定义，这样我们可以复用调用敏感词过滤的代码。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970public class ASensitiveWordsFilter { // A敏感词过滤系统提供的接口 //text是原始文本，函数输出用***替换敏感词之后的文本 public String filterSexyWords(String text) { // ... } public String filterPoliticalWords(String text) { // ... } }public class BSensitiveWordsFilter { // B敏感词过滤系统提供的接口 public String filter(String text) { //... }}public class CSensitiveWordsFilter { // C敏感词过滤系统提供的接口 public String filter(String text, String mask) { //... }}// 未使用适配器模式之前的代码：代码的可测试性、扩展性不好public class RiskManagement { private ASensitiveWordsFilter aFilter = new ASensitiveWordsFilter(); private BSensitiveWordsFilter bFilter = new BSensitiveWordsFilter(); private CSensitiveWordsFilter cFilter = new CSensitiveWordsFilter(); public String filterSensitiveWords(String text) { String maskedText = aFilter.filterSexyWords(text); maskedText = aFilter.filterPoliticalWords(maskedText); maskedText = bFilter.filter(maskedText); maskedText = cFilter.filter(maskedText, &quot;***&quot;); return maskedText; }}// 使用适配器模式进行改造public interface ISensitiveWordsFilter { // 统一接口定义 String filter(String text);}public class ASensitiveWordsFilterAdaptor implements ISensitiveWordsFilter { private ASensitiveWordsFilter aFilter; public String filter(String text) { String maskedText = aFilter.filterSexyWords(text); maskedText = aFilter.filterPoliticalWords(maskedText); return maskedText; }}//...省略BSensitiveWordsFilterAdaptor、CSensitiveWordsFilterAdaptor...// 扩展性更好，更加符合开闭原则，如果添加一个新的敏感词过滤系统，// 这个类完全不需要改动；而且基于接口而非实现编程，代码的可测试性更好。public class RiskManagement { private List&lt;ISensitiveWordsFilter&gt; filters = new ArrayList&lt;&gt;(); public void addSensitiveWordsFilter(ISensitiveWordsFilter filter) { filters.add(filter); } public String filterSensitiveWords(String text) { String maskedText = text; for (ISensitiveWordsFilter filter : filters) { maskedText = filter.filter(maskedText); } return maskedText; }} 替换依赖的外部系统当我们把项目中依赖的一个外部系统替换为另一个外部系统的时候，利用适配器模式，可以减少对代码的改动。 123456789101112131415161718192021222324252627282930313233// 外部系统Apublic interface IA { //... void fa();}public class A implements IA { //... public void fa() { //... }}// 在我们的项目中，外部系统A的使用示例public class Demo { private IA a; public Demo(IA a) { this.a = a; } //...} Demo d = new Demo(new A());// 将外部系统A替换成外部系统Bpublic class BAdaptor implemnts IA { private B b; public BAdaptor(B b) { this.b= b; } public void fa() { //... b.fb(); }}// 借助BAdaptor，Demo的代码中，调用IA接口的地方都无需改动，// 只需要将BAdaptor如下注入到Demo即可。Demo d = new Demo(new BAdaptor(new B())); 适配不同格式的数据适配器模式主要用于接口的适配，实际上，它还可以用在不同格式的数据之间的适配。比如，把从不同征信系统拉取的不同格式的征信数据，统一为相同的格式，以方便存储和使用。再比如，Java 中的 Arrays.asList() 也可以看作一种数据适配器，将数组类型的数据转化为集合容器类型。 1List&lt;String&gt; stooges = Arrays.asList(&quot;Larry&quot;, &quot;Moe&quot;, &quot;Curly&quot;);","link":"/posts/22428.html"},{"title":"设计模式：迭代器模式","text":"原理和实现迭代器模式（Iterator Design Pattern），也叫作游标模式（Cursor Design Pattern）。它用来遍历集合对象（也可以叫“容器”或“聚合对象”），实际上就是包含一组对象的对象，比如数组、链表、树、图、跳表。 迭代器模式将集合对象的遍历操作从集合类中拆分出来，放到迭代器类中，让两者的职责更加单一。一个完整的迭代器模式一般会涉及容器和容器迭代器两部分内容。为了达到基于接口而非实现编程的目的，容器又包含容器接口、容器实现类，迭代器又包含迭代器接口、迭代器实现类。如下图所示： 现在我们需要针对 ArrayList 和 LinkedList 两个线性容器，设计实现对应的迭代器。按照之前给出的迭代器模式的类图，我们定义一个迭代器接口 Iterator，以及针对两种容器的具体的迭代器实现类 ArrayIterator 和 ListIterator。 1、Iterator 接口的定义，代码如下所示： 123456789101112// 接口定义方式一public interface Iterator&lt;E&gt; { boolean hasNext(); void next(); E currentItem();}// 接口定义方式二public interface Iterator&lt;E&gt; { boolean hasNext(); E next();} 在第一种定义中，next() 函数用来将游标后移一位元素，currentItem() 函数用来返回当前游标指向的元素。在第二种定义中，返回当前元素与后移一位这两个操作，要放到同一个函数 next() 中完成。 第一种定义方式更加灵活一些，比如我们可以多次调用 currentItem() 查询当前元素，而不移动游标。所以，在接下来的实现中，我们选择第一种接口定义方式。 2、 ArrayIterator的代码实现，具体如下所示 1234567891011121314151617181920212223242526272829303132333435363738public class ArrayIterator&lt;E&gt; implements Iterator&lt;E&gt; { private int cursor; private ArrayList&lt;E&gt; arrayList; public ArrayIterator(ArrayList&lt;E&gt; arrayList) { this.cursor = 0; this.arrayList = arrayList; } @Override public boolean hasNext() { return cursor != arrayList.size(); //注意这里，cursor在指向最后一个元素的时候，hasNext()仍旧返回true。 } @Override public void next() { cursor++; } @Override public E currentItem() { if (cursor &gt;= arrayList.size()) { throw new NoSuchElementException(); } return arrayList.get(cursor); }}public class Demo { public static void main(String[] args) { ArrayList&lt;String&gt; names = new ArrayList&lt;&gt;(); names.add(&quot;xzg&quot;); names.add(&quot;wang&quot;); names.add(&quot;zheng&quot;); Iterator&lt;String&gt; iterator = new ArrayIterator(names); while (iterator.hasNext()) { System.out.println(iterator.currentItem()); iterator.next(); } }} 通过这个例子，总结一下迭代器的设计思路。 迭代器中需要定义 hasNext()、currentItem()、next() 三个最基本的方法； 待遍历的容器对象通过依赖注入传递到迭代器类中； 容器通过 iterator() 方法来创建迭代器。 迭代器模式的优势 首先，对于类似数组和链表这样的数据结构，遍历方式比较简单，直接使用 for 循环来遍历就足够了。但是，对于复杂的数据结构（比如树、图）来说，有各种复杂的遍历方式。 比如，树有前中后序、按层遍历，图有深度优先、广度优先遍历等等。如果由客户端代码来实现这些遍历算法，势必增加开发成本，而且容易写错。如果将这部分遍历的逻辑写到容器类中，也会导致容器类代码的复杂性。 其次，将游标指向的当前位置等信息，存储在迭代器类中，每个迭代器独享游标信息。这样，我们就可以创建多个不同的迭代器，同时对同一个容器进行遍历而互不影响。 最后，容器和迭代器都提供了抽象的接口，方便我们在开发的时候，基于接口而非具体的实现编程。当需要切换新的遍历算法的时候，比如，从前往后遍历链表切换成从后往前遍历链表，客户端代码只需要将迭代器类从 LinkedIterator 切换为 ReversedLinkedIterator 即可，其他代码都不需要修改。除此之外，添加新的遍历算法，我们只需要扩展新的迭代器类，也更符合开闭原则。","link":"/posts/41807.html"},{"title":"设计模式：门面模式","text":"门面模式，也叫外观模式，英文全称是 Facade Design Pattern。在 GoF 的《设计模式》一书中，门面模式是这样定义的： Provide a unified interface to a set of interfaces in a subsystem. Facade Pattern defines a higher-level interface that makes the subsystem easier to use. 翻译成中文就是：门面模式为子系统提供一组统一的接口，定义一组高层接口让子系统更易用。 门面模式定义中的“子系统（subsystem）”可以有多种理解方式。它既可以是一个完整的系统，也可以是更细粒度的类或者模块。 在日常生活中，门面模式也是很常见的。比如，我们去医院就诊，很多医院都设置了导诊台，这个导诊台就好比一个门面。有了这个导诊台，我们全程就诊都不需要到处乱转，就诊路线变得非常清楚。 门面模式的UML类图如下： 外观角色（Facade）：也叫作门面角色，是系统对外的统一接口。 子系统角色（SubSystem）：可以同时有一个或多个SubSystem。每个SubSytem都不是一个单独的类，而是一个类的集合。SubSystem并不知道Facade的存在，对于SubSystem而言，Facade 只是另一个客户端而已（即Facade对SubSystem透明）","link":"/posts/45656.html"},{"title":"限流算法","text":"作者：占小狼链接：https://www.jianshu.com/p/76cc8ba5ca91 为什么需要限流按照服务的调用方，可以分为以下几种类型服务 与用户打交道的服务比如web服务、对外API，这种类型的服务有以下几种可能导致机器被拖垮： 用户增长过快（这是好事） 因为某个热点事件（微博热搜） 竞争对象爬虫 恶意的刷单 这些情况都是无法预知的，不知道什么时候会有10倍甚至20倍的流量打进来，如果真碰上这种情况，扩容是根本来不及的（弹性扩容都是虚谈，一秒钟你给我扩一下试试） 对内的RPC服务一个服务A的接口可能被B、C、D和E多个服务进行调用，在B服务发生突发流量时，直接把A服务给调用挂了，导致A服务对CDE也无法提供服务。 这种情况时有发生，解决方案有两种： 每个调用方采用线程池进行资源隔离； 使用限流手段对每个调用方进行限流； 限流算法实现常见的限流算法有：计数器、令牌桶、漏桶。 计数器算法采用计数器实现限流有点简单粗暴，一般我们会限制一秒钟的能够通过的请求数，比如限流qps为100，算法的实现思路就是从第一个请求进来开始计时，在接下去的1s内，每来一个请求，就把计数加1，如果累加的数字达到了100，那么后续的请求就会被全部拒绝。等到1s结束后，把计数恢复成0，重新开始计数。 具体的实现可以是这样的：对于每次服务调用，可以通过AtomicLong#incrementAndGet()方法来给计数器加1并返回最新值，通过这个最新值和阈值进行比较。 这种实现方式，相信大家都知道有一个弊端：如果我在单位时间1s内的前10ms，已经通过了100个请求，那后面的990ms，拒绝请求，我们把这种现象称为“突刺现象”。 漏桶算法为了消除”突刺现象”，可以采用漏桶算法实现限流，漏桶算法这个名字就很形象，算法内部有一个容器，类似生活用到的漏斗，当请求进来时，相当于水倒入漏斗，然后从下端小口慢慢匀速的流出。不管上面流量多大，下面流出的速度始终保持不变。 不管服务调用方多么不稳定，通过漏桶算法进行限流，每10毫秒处理一次请求。因为处理的速度是固定的，请求进来的速度是未知的，可能突然进来很多请求，没来得及处理的请求就先放在桶里，既然是个桶，肯定是有容量上限，如果桶满了，那么新进来的请求就丢弃。 在算法实现方面，可以准备一个队列，用来保存请求，另外通过一个线程池（ScheduledExecutorService）来定期从队列中获取请求并执行，可以一次性获取多个并发执行。 这种算法，在使用过后也存在弊端：无法应对短时间的突发流量。 令牌桶算法从某种意义上讲，令牌桶算法是对漏桶算法的一种改进，桶算法能够限制请求调用的速率，而令牌桶算法能够在限制调用的平均速率的同时还允许一定程度的突发调用。 在令牌桶算法中，存在一个桶，用来存放固定数量的令牌。算法中存在一种机制，以一定的速率往桶中放令牌。每次请求调用需要先获取令牌，只有拿到令牌，才有机会继续执行，否则选择选择等待可用的令牌、或者直接拒绝。 放令牌这个动作是持续不断的进行，如果桶中令牌数达到上限，就丢弃令牌，所以就存在这种情况，桶中一直有大量的可用令牌，这时进来的请求就可以直接拿到令牌执行，比如设置qps为100，那么限流器初始化完成一秒后，桶中就已经有100个令牌了，这时服务还没完全启动好，等启动完成对外提供服务时，该限流器可以抵挡瞬时的100个请求。所以，只有桶中没有令牌时，请求才会进行等待，最后相当于以一定的速率执行。 实现思路：可以准备一个队列，用来保存令牌，另外通过一个线程池定期生成令牌放到队列中，每来一个请求，就从队列中获取一个令牌，并继续执行。","link":"/posts/62837.html"},{"title":"零拷贝","text":"原文地址：https://www.programmersought.com/article/72346565091/ 零拷贝（Zero Copy）是一个耳熟能详的名词，很多高性能网络框架如Netty，Kafka，RocketMQ都将零拷贝作为其特点。那么究竟什么是零拷贝？ Zero copy维基百科对零拷贝的定义如下： “Zero-copy” describes computer operations in which the CPU does not perform the task of copying data from one memory area to another. This is frequently used to save CPU cycles and memory bandwidth when transmitting a file over a network. “零拷贝”描述了计算机操作，其中 CPU 不执行将数据从一个内存区域复制到另一个内存区域的任务。这通常用于在通过网络传输文件时节省 CPU 周期和内存带宽。 Zero copy in Linux system术语 内核空间 计算机内存分为用户空间和内核空间。内核空间运行OS内核代码，可以访问所有内存、机器指令和硬件资源，拥有最高权限。 用户空间 内核外部的所有空间，用于正常的用户进程操作。用户空间的进程不能访问内核空间，只能通过内核系统调用(系统调用)公开的接口访问内核的一小部分。如果用户进程请求执行系统调用，则需要向内核发送系统中断(软件中断)，内核会调度相应的中断处理程序来处理请求。 DMA 直接内存访问(DMA)是为了响应CPU与硬盘之间的速度大小不匹配，它允许某些硬件子系统独立于CPU主存进行访问。 如果没有DMA、CPU执行IO操作的整个过程被阻塞，无法执行其他工作，这将导致计算机陷入挂起。如果有DMA干预，IO过程就变成这样了：CPU启动DMA传输过程中，可以执行其他操作；DMA控制器（DMAC）传输完成后，会给CPU发送一个中断信号，然后CPU就可以对传输的数据进行处理了。 Traditional network transmission网络IO的一个常见场景是从硬盘读取文件，通过网卡发送到网络。下面是一个简单的伪代码： 1234// read data from hard diskFile.read(fileDesc, buf, len);// Send data to the networkSocket.write(socket, buf, len); 在代码层面，这是一个非常简单的操作，但在系统层面，让我们看看幕后发生了什么： User initiated read() System call (syscall), request hard disk data. At this point, it will happen once Context switch(context switch)。 DMA Read the file from the hard disk. At this time, a copy is generated:hard disk–&gt;DMA buffer。 DMA Copy data toUser space，read() The call returns. At this time, it happened once Context switch And a data copy: DMA buffer–&gt;User space。 User initiated write() System call, request to send data. This happens once Context switch And a data copy: User space–&gt;DMA buffer。 DMA Copy the data to the network card for network transmission. The fourth data copy occurs at this time:DMA buffer–&gt;Socket buffer write() The call returns and happens again Context switch。 数据流如下： 可以发现，涉及4次上下文切换和4次数据拷贝。对于简单的网络文件发送，有很多不必要的开销。 sendfile transmission对于上面的场景，可以发现从DMA缓冲区到用户空间和从用户空间到套接字缓冲区两次CPU复制是完全没有必要的，零拷贝由此诞生。针对这种情况，Linux内核提供了sendfile系统调用。 如果使用sendfile()执行上述请求，系统流程可以简化如下： sendfile()系统调用，可以在DMA中实现数据的内部复制，而不需要将数据复制到用户空间。因此，上下文切换次数减少到2次，数据拷贝次数减少到3次。 Here is a question: whyDMAThere will be a copy inside (this copy needsCPUparticipate)? This is because the early network cards required the data to be sent to be continuous in physical space, so there wasSocket Buffer. But if the network card itself supports scatter-gather, that is, it can gather and send data from discontinuous memory addresses, then it can be further optimized. Network card support scatter-gather of sendfile transmissioninLinuxThis has been optimized after kernel version 2.4. If the computer network card supports the collection operation,sendfile()Operation can be omittedSocket BufferThe data is copied, instead, the descriptors of the data location and length are directly passed toSocket Buffer： 在网卡的支持下，上下文切换次数为2次，数据复制次数也减少到2次。两次数据复制是必须的，即数据在内存中复制已经完全避免了。 对于从硬盘向网络发送文件的场景，如果网卡支持采集操作，那么sendfile()系统调用，真正意义上的零拷贝。 Memory mapping (mmap)在“通过互联网发送文件”的情况下，使用sendfile()系统调用可以极大地提高性能(根据测试，吞吐量可以达到传统方法的三倍)。但有一个缺点是，它只支持“读-&gt;发送”的“连续操作”，所以，sendfile()一般用来处理一些静态的网络资源，如果要对数据进行额外的操作，它是无能为力的。内存映射（Memory mapping）为此提供了一种解决方案。 mmap是一种内存映射文件的方法。它可以将文件映射到进程的地址空间，实现文件磁盘地址和进程虚拟地址空间中虚拟地址的对应。这样，用户进程就可以使用指针对这块内存进行读写操作，内核空间对这块内存的修改直接反映在用户空间中。 简而言之，mmap实现了用户空间和内核空间的数据共享。可以猜到，如果使用mmap 系统调用，上述场景的步骤如下： 数据流如下： **与传统方式相比，mmap它保存了一份数据拷贝，这也可以从广义上称为零拷贝(Zero Copy)**。同时，它还允许用户自定义数据操作，这与发送文件相比，其优势在于。 Zero copy in JDK NIO从1.4版开始，JDK引入了NIO，提供了正确的Zero Copy支持。由于JVM运行在上述操作系统上，其功能只是对系统底层API的封装，如果操作系统不支持Zero Copy(mmap/sendfile)，那么JVM就无能为力了。JDK正确的Zero Copy Package，主要体现在FileChannel这个类上。 map()map()方法如下： 123456public abstract class FileChannel extends AbstractInterruptibleChannel implements SeekableByteChannel, GatheringByteChannel, ScatteringByteChannel { public abstract MappedByteBuffer map(MapMode mode, long position, long size) throws IOException;} map()方法描述如下： 将此通道文件的一个区域直接映射到内存中。对于大多数操作系统来说，将文件映射到内存比通过通常的读写方法读取或写入数十KB的数据更昂贵。从性能的角度来看，通常只值得将相对较大的文件映射到内存中。 map()方法返回MappdByteBuffer，DirectByteBuffer就是它的子类别。它引用一个独立于JVM内存的块，它位于GC之外，由该机制控制，您需要自己管理创建和销毁操作。 transferTo()transferTo()方法如下： 123456public abstract class FileChannel extends AbstractInterruptibleChannel implements SeekableByteChannel, GatheringByteChannel, ScatteringByteChannel { public abstract long transferTo(long position, long count, WritableByteChannel target) throws IOException;} transferTo()方法描述如下： 将字节从此通道的文件传输到给定的可写字节通道时，此方法可能比从此通道读取并写入目标通道的简单循环高效得多。 许多操作系统可以直接将字节从文件系统缓存传输到目标通道，而无需实际复制它们。 请注意，由于sendfile()只适用于套接字缓冲区发送数据，因此通过Zero Copy技术提高性能只能用于通过网络发送数据的场景。 那是什么意思？如果您只是使用Transfer To()将数据从硬盘上的一个文件写入到另一个文件，则不会产生任何性能提升效果。 Zero copy in Netty// todo","link":"/posts/45215.html"},{"title":"HBase：基础篇","text":"HBase 概述HBase（Hadoop Database）是一个高可靠、高性能、面向列、可伸缩的KV分布式数据库，利用HBase技术可在廉价PC上搭建起大规模结构化存储集群。 HBase参考Google的BigTable建模，使用类似GFS的HDFS作为底层文件存储系统，在其上可以运行MapReduce批量处理数据，使用ZooKeeper作为协同服务组件。 发展历史google“三篇论文”——GFS、MapReduce、BigTable。 GFS: （The Google File System）：揭示了如何在大量廉价机器基础上存储海量数据。 MapReduce（Simplef iedData Processing on Large Clusters）：论文论述了如何在大量廉价机器的基础上稳定地实现超大规模的并行数据处理。 BigTable: A Distributed StorageSystem for Structured Data 》，用于解决Google内部海量结构化数据的存储以及高效读写问题。 Apache HBase最初是Powerset公司为了处理自然语言搜索产生的海量数据而开展的项目，由Chad Walters和Jim Kellerman两人发起，经过两年的发展之后被Apache基金会收录为顶级项目，同时成为非常活跃、影响重大的项目。 2015年2月社区发布了1.0.0版本，规范了Hbase的版本号，此后的版本号都统一遵循semanticversioning语义，如下图： 可以理解为MAJOR、MINOR相同的情况下，PATCH版本越大，系统越可靠。 Hbase优缺点优点 容量巨大：HBase的单表可以支持千亿行、百万列的数据规模，数据容量可以达到TB甚至PB级别。传统的关系型数据库，如Oracle和MySQL等，如果单表记录条数超过亿行，读写性能都会急剧下降，在HBase中并不会出现这样的问题。 良好的可扩展性：HBase集群可以非常方便地实现集群容量扩展，主要包括数据存储节点扩展以及读写服务节点扩展。HBase底层数据存储依赖于HDFS系统，HDFS可以通过简单地增加DataNode实现扩展，HBase读写服务节点也一样，可以通过简单的增加RegionServer节点实现计算层的扩展。 稀疏性：HBase支持大量稀疏存储，即允许大量列值为空，并不占用任何存储空间。这与传统数据库不同，传统数据库对于空值的处理要占用一定的存储空间，这会造成一定程度的存储空间浪费。因此可以使用HBase存储多至上百万列的数据，即使表中存在大量的空值，也不需要任何额外空间。 高性能：HBase目前主要擅长于OLTP场景，数据写操作性能强劲，对于随机单点读以及小范围的扫描读，其性能也能够得到保证。对于大范围的扫描读可以使用MapReduce提供的API，以便实现更高效的并行扫描。 多版本：HBase支持多版本特性，即一个KV可以同时保留多个版本，用户可以根据需要选择最新版本或者某个历史版本。 Hadoop原生支持：HBase是Hadoop生态中的核心成员之一，很多生态组件都可以与其直接对接。 缺点 HBase本身不支持很复杂的聚合运算（如Join、GroupBy等）。如果业务中需要使用聚合运算，可以在HBase之上架设Phoenix组件或者Spark组件，前者主要应用于小规模聚合的OLTP场景，后者应用于大规模聚合的OLAP场景。 HBase本身并没有实现二级索引功能，所以不支持二级索引查找。好在针对HBase实现的第三方二级索引方案非常丰富，比如目前比较普遍的使用Phoenix提供的二级索引功能。 HBase原生不支持全局跨行事务，只支持单行事务模型。同样，可以使用Phoenix提供的全局事务模型组件来弥补HBase的这个缺陷。 数据模型逻辑视图在具体了解逻辑视图之前有必要先看看HBase中的基本概念。 table：一个表包含多行数据。 RowKey：表中每条记录的主键； Column Family：列族，将表进行横向切割，后面简称CF； Column：属于某一个列族，可动态添加列； Version Number：类型为Long，默认值是系统时间戳，可由用户自定义； Value：真实的数据。 Region：一段数据的集合； 物理视图 HBase 原理LSMTTDO 整体架构 HBase客户端 HBase客户端（Client）提供了Shell命令行接口、原生Java API编程接口、Thrift/REST API编程接口以及MapReduce编程接口 HBase客户端支持所有常见的DML操作以及DDL操作，即数据的增删改查和表的日常维护等。其中Thrift/REST API主要用于支持非Java的上层业务需求，MapReduce接口主要用于批量数据导入以及批量数据读取。 HBase客户端访问数据行之前，首先需要通过元数据表定位目标数据所在RegionServer，之后才会发送请求到该RegionServer。同时这些元数据会被缓存在客户端本地，以方便之后的请求访问。如果集群RegionServer发生宕机或者执行了负载均衡等，从而导致数据分片发生迁移，客户端需要重新请求最新的元数据并缓存在本地。 ZooKeeper 实现Master高可用：通常情况下系统中只有一个Master工作，一旦ActiveMaster由于异常宕机，ZooKeeper会检测到该宕机事件，并通过一定机制选举出新的Master，保证系统正常运转。 管理系统核心元数据：比如，管理当前系统中正常工作的RegionServer集合，保存系统元数据表hbase:meta所在的RegionServer地址等。 参与RegionServer宕机恢复：ZooKeeper通过心跳可以感知到RegionServer是否宕机，并在宕机后通知Master进行宕机处理。 实现分布式表锁：HBase中对一张表进行各种管理操作（比如alter操作）需要先加表锁，防止其他用户对同一张表进行管理操作，造成表状态不一致。和其他RDBMS表不同，HBase中的表通常都是分布式存储，ZooKeeper可以通过特定机制实现分布式表锁。 Master 处理用户的各种管理请求，包括建表、修改表、权限操作、切分表、合并数据分片以及Compaction等。 管理集群中所有RegionServer，包括RegionServer中Region的负载均衡、RegionServer的宕机恢复以及Region的迁移等。 清理过期日志以及文件，Master会每隔一段时间检查HDFS中HLog是否过期、HFile是否已经被删除，并在过期之后将其删除。 RegionServer RegionServer主要用来响应用户的IO请求，是HBase中最核心的模块，由WAL(HLog)、BlockCache以及多个Region构成。 WAL(HLog)：HLog在HBase中有两个核心作用——其一，用于实现数据的高可靠性，HBase数据随机写入时，并非直接写入HFile数据文件，而是先写入缓存，再异步刷新落盘。为了防止缓存数据丢失，数据写入缓存之前需要首先顺序写入HLog，这样，即使缓存数据丢失，仍然可以通过HLog日志恢复；其二，用于实现HBase集群间主从复制，通过回放主集群推送过来的HLog日志实现主从复制。 BlockCache：HBase系统中的读缓存。客户端从磁盘读取数据之后通常会将数据缓存到系统内存中，后续访问同一行数据可以直接从内存中获取而不需要访问磁盘。 Region：数据表的一个分片，当数据表大小超过一定阈值就会“水平切分”，分裂为两个Region。Region是集群负载均衡的基本单位。通常一张表的Region会分布在整个集群的多台RegionServer上，一个RegionServer上会管理多个Region，当然，这些Region一般来自不同的数据表。 Store：一个Region由一个或者多个Store构成，Store的个数取决于表中列簇（columnfamily）的个数，多少个列簇就有多少个Store。HBase中，每个列簇的数据都集中存放在一起形成一个存储单元Store，因此建议将具有相同IO特性的数据设置在同一个列簇中。 MemStore &amp; HFile：MemStore称为写缓存，用户写入数据时首先会写到MemStore，当MemStore写满之后（缓存数据超过阈值，默认128M）系统会异步地将数据f lush成一个HFile文件。显然，随着数据不断写入，HFile文件会越来越多，当HFile文件数超过一定阈值之后系统将会执行Compact操作，将这些小文件通过一定策略合并成一个或多个大文件。 HDFSHBase底层依赖HDFS组件存储实际数据，包括用户数据文件、HLog日志文件等最终都会写入HDFS落盘。HDFS是Hadoop生态圈内最成熟的组件之一，数据默认三副本存储策略可以有效保证数据的高可靠性。HBase内部封装了一个名为DFSClient的HDFS客户端组件，负责对HDFS的实际数据进行读写访问。 HBase 读写流程HBase Region查找 客户端根据写入的表以及rowkey在元数据缓存中查找，如果能够查找出该rowkey所在的RegionServer以及Region，就可以直接发送写入请求（携带Region信息）到目标RegionServer。 如果客户端缓存中没有查到对应的rowkey信息，需要首先到ZooKeeper上/hbase-root/meta-region-server节点查找Hbase元数据表所在的RegionServer。向hbase:meta所在的RegionServer发送查询请求，在元数据表中查找rowkey所在的RegionServer以及Region信息。客户端接收到返回结果之后会将结果缓存到本地，以备下次使用。 客户端根据rowkey相关元数据信息将写入请求发送给目标RegionServer，Region Server接收到请求之后会解析出具体的Region信息，查到对应的Region对象，并将数据写入目标Region的MemStore中。 写流程 1）客户端处理阶段：客户端将用户的写入请求进行预处理，并根据集群元数据定位写入数据所在的RegionServer，将请求发送给对应的RegionServer。 2）Region写入阶段：RegionServer接收到写入请求之后将数据解析出来，首先写入WAL，再写入对应Region列簇的MemStore。 3）MemStore Flush阶段：当Region中MemStore容量超过一定阈值，系统会异步执行flush操作，将内存中的数据写入文件，形成HFile。 读流程 和写流程相比，HBase读数据的流程更加复杂。主要基于两个方面的原因： HBase一次范围查询可能会涉及多个Region、多块缓存甚至多个数据存储文件； HBase中更新操作以及删除操作的实现都很简单，更新操作并没有更新原有数据，而是使用时间戳属性实现了多版本；删除操作也并没有真正删除原有数据，只是插入了一条标记为”deleted”标签的数据，而真正的数据删除发生在系统异步执行Major Compact的时候。 很显然，这种实现思路大大简化了数据更新、删除流程，但是对于数据读取来说却意味着套上了层层枷锁：读取过程需要根据版本进行过滤，对已经标记删除的数据也要进行过滤。 HBase Compaction HBase根据合并规模将Compaction分为两类：Minor Compaction和MajorCompaction。 Minor Compaction是指选取部分小的、相邻的HFile，将它们合并成一个更大的HFile。 Major Compaction是指将一个Store中所有的HFile合并成一个HFile，这个过程还会完全清理三类无意义数据：被删除的数据、TTL过期数据、版本号超过设定版本号的数据。 一般情况下，Major Compaction持续时间会比较长，整个过程会消耗大量系统资源，对上层业务有比较大的影响。因此线上部分数据量较大的业务通常推荐关闭自动触发Major Compaction功能，改为在业务低峰期手动触发（或设置策略自动在低峰期触发）。 Compaction核心作用如下： 合并小文件，减少文件数，稳定随机读延迟。 提高数据的本地化率。 清除无效数据，减少数据存储量。 HBase中触发Compaction的时机有很多，最常见的时机有如下三种：MemStoreFlush、后台线程周期性检查以及手动触发。 HBase热点问题热点是指发生在大量的Client直接访问集群的一个或极少数个节点（访问可能是读，写或者其他操作）。 大量访问会使热点Region所在的单个机器超出自身承受能力，引起性能下降甚至Region不可用，也会影响同一个RegionServer上的其他Region。 产生原因 HBase创建表默认只有一个分区 RowKey设计不合理 解决方案 HBase创建表时指定分区 合理设计RowKey 预分区默认情况下，在创建 HBase 表的时候会自动创建一个 region 分区，当导入数据的时候，所有的 HBase 客户端都向这一个 region 写数据，直到这个 region 足够大了才进行切分。还有一种是可以加快批量写入速度的方法，就是通过预先创建一些空的 regions，这样当数据写入 HBase 时，会按照 region 分区情况，在集群内做数据的负载均衡。 RowKey设计设计原则 唯一性 必须在设计上保证其唯一性。RowKey是按照字典顺序排序存储的，因此，设计RowKey 的时候，要充分利用这个排序的特点，将经常读取的数据存储到一块，将最近可能会被访问 的数据放到一块。 长度原则 RowKey 是一个二进制码流，RowKey的长度被很多开发者建议说设计在10~100 个字节，不过建议是越短越好，不要超过16个字节。 原因如下: 数据的持久化文件HFile中是按照KeyValue存储的，如果RowKey过长比如100 个字节，1000万列数据光RowKey就要占用100*1000 万=10亿个字节，将近1G数据，这会极大影响HFile的存储效率; MemStore将缓存部分数据到内存，如果RowKey字段过长内存的有效利用率会降低， 系统将无法缓存更多的数据，这会降低检索效率。因此RowKey的字节长度越短越好。 目前操作系统是都是 64 位系统，内存 8 字节对齐。控制在 16 个字节，8 字节的整数倍利用操作系统的最佳特性。 散列原则 如果RowKey按照时间戳的方式递增，不要将时间放在二进制码的前面，建议将RowKey的高位作为散列字段，由程序随机生成，低位放时间字段，这样将提高数据均衡分布在每个RegionServer，以实现负载均衡的几率。如果没有散列字段，首字段直接是时间信息，所有的数据都会集中在一个RegionServer上，这样在数据检索的时候负载会集中在个别的RegionServer上，造成热点问题，会降低查询效率。 散列常见设计方法 Salting（加盐） Hashing（哈希） Reversing（反转） 反转可以使得RowKey中经常改变的部分（最没有意义的部分）放在前面，这样可以有效的随机rowkey，但是牺牲RowKey的有序性。反转RowKey的例子以手机号为RowKey，可以将手机号反转后的字符串作为RowKey，这样的就避免了以手机号那样比较固定开头导致热点问题。","link":"/posts/33835.html"},{"title":"Hbase：分布式集群环境搭建","text":"原文地址：https://www.awebone.com/posts/440e5582/ 原文作者：:Awebone 通过搭建 ZooKeeper 分布式集群，以感知服务器上下线，接着基于 ZooKeeper 搭建 Hadoop HA 集群，保证 HDFS 文件系统高可用，再在高可用分布式 HDFS 文件系统的基础上搭建高可用分布式 HBase 数据库。 完整实现这个过程，学习到分布式系统中关于数据一致性的知识，在系统中，均可保证最终一致性。掌握了 ZooKeeper、Hadoop、HBase 高可用集群的搭建部署，学习了在分布式文件系统 HDFS 之上，数据库的应用场景。 系统介绍系统基于 ZooKeeper 搭建 Hadoop HA 集群，在高可用分布式 HDFS 文件系统的基础上，搭建高可用分布式 HBase 数据库集群。 Hadoop 分布式集群采用主从架构，在 Hadoop HA 集群中，ZooKeeper 用来解决 SPOF 单点故障问题。如果 active namenod 宕机，就从剩下的 standby namenodes 中选举出来一个新的 active namenode，并且做到瞬时切换，使得在需求增长的前提下，分布式集群仍然可以向外提供服务。 同样的，分布式 HBase 数据库也采用主从架构，在 master server 宕机的情况下，瞬时切换到 backup master，使得 HBase 高可用。分布式 HBase 数据库的底层存储采用 HDFS 文件系统。 环境要求和版本选择（1）四台 Linux 服务器，分别为 Hadoop01、Hadoop02、Hadoop03、Hadoop04，采用 Centos 6.8 版本； （2）Java 采用 JDK 1.8 版本； （3）ZooKeeper 采用 3.4.10 版本； （4）Hadoop 采用 2.7.6 版本； （5）HBase 采用 1.2.6 版本； 集群规划和架构设计Hadoop 和 HBase 均采用主从架构模式，其系统内部均使用自己提供的负载均衡器。系统的集群规划如下表所示。 Hadoop01Hadoop02Hadoop03Hadoop04NameNode√√DataNode√√√√ResourceManager√√NodeManager√√√√JobHistoryServer√ZooKeeper√√√JournalNode√√√Zkfc√√HMaster√√HRegionServer√√√√ ZooKeeper 集群安装配置安排安装 ZooKeeper 集群时需要注意集群的节点个数必须是奇数，因为奇数个数是为了方便进行选举 leader。这里将 Hadoop01、Hadoop02、Hadoop03 和 Hadoop04 均作为 ZooKeeper 集群的节点，但是将 Hadoop04 节点中的角色固定设置为 observer，observer 其实跟 follower 类似，只不过是为了给 ZooKeeper 进行扩充之使用，不会改变原来集群的主从所属关系，仅仅只是接受请求，然后进行处理，没有投票的权利，也没有被选举成为 leader 的权利。 配置步骤 获取安装包 zookeeper-3.4.10.tar.gz 解压 tar -zxvf zookeeper-3.4.10.tar.gz -C ~/apps/ 修改配置文件vim zoo.cfg 1234567891011// znode数据存储系统中的所有节点的数据存储目录dataDir=/home/hadoop/data/zkdata/server.1=hadoop01:2888:3888server.2=hadoop02:2888:3888server.3=hadoop03:2888:3888server.4=hadoop04:2888:3888:observer 在每个节点的 / home/hadoop/data/zkdata / 目录下创建一个 myid 的文件, 该文件中直接存储一个 id 值即可 12345hadoop01:echo 1 &gt; myidhadoop02:echo 2 &gt; myidhadoop03:echo 3 &gt; myid 配置环境变量vim .bashrc 123export ZOOKEEPER_HOME=/home/hadoop/apps/zookeeper-3.4.10export PATH=$PATH:$ZOOKEEPER_HOME/bin 启动 在每个节点上都执行：zkServer.sh start 客户端连接与 shell 操作 客户端连接：zkCli.sh -server hostname:2181 Hadoop HA 集群安装HA设计和配置安排使用共享存储和 ZooKeeper 实现 Hadoop HA。这里将 Hadoop01 作为 NameNode 的 active 节点，将 Hadoop02 作为 NameNode 的 standby 节点，Hadoop02 是 Hadoop01 的热备，NameNode 的元数据都存储在 qjournal 日志系统这个共享存储中，Hadoop01、Hadoop02、Hadoop03、Hadoop04 都作为 DataNode 的节点，定时向 NameNode 发送报告和心跳。ZooKeeper 中的 zkfc 进程监控 NameNode 的情况，当 NameNode active 节点失去心跳，即宕机时，自动切换，将 standby 节点激活，实现 Hadoop 集群的高可用。 配置步骤 获取安装包hadoop-2.7.6.tar.gz 解压 tar -zxvf hadoop-2.7.6.tar.gz -C ~/apps/ 修改配置文件 hadoop-env.sh 文件： export JAVA_HOME=/home/hadoop/apps/jdk1.8.0_73 core-site.xml 文件： 123456789101112131415161718192021222324252627&lt;configuration&gt; &lt;!-- 指定hdfs的nameservice为myha01 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://myha/&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop临时目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/hadoopdata/&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zookeeper地址 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop01:2181,hadoop02:2181,hadoop03:2181,hadoop04:2181&lt;/value&gt; &lt;/property&gt; &lt;!-- hadoop链接zookeeper的超时时长设置 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.session-timeout.ms&lt;/name&gt; &lt;value&gt;1000&lt;/value&gt; &lt;description&gt;ms&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml 文件： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114&lt;configuration&gt; &lt;!-- 指定副本数 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置namenode和datanode的工作目录-数据存储目录 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/hadoopdata/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/hadoopdata/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;!-- 启用webhdfs --&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;myha&lt;/value&gt; &lt;/property&gt; &lt;!-- myha01下面有两个NameNode，分别是nn1，nn2 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.myha&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.myha.nn1&lt;/name&gt; &lt;value&gt;hadoop01:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.myha.nn1&lt;/name&gt; &lt;value&gt;hadoop01:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.myha.nn2&lt;/name&gt; &lt;value&gt;hadoop02:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.myha.nn2&lt;/name&gt; &lt;value&gt;hadoop02:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://hadoop01:8485;hadoop02:8485;hadoop03:8485/myha&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/journaldata&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启NameNode失败自动切换 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置失败自动切换实现方式 --&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.myha&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt; sshfence shell(/bin/true) &lt;/value&gt; &lt;/property&gt; &lt;!-- 使用sshfence隔离机制时需要ssh免登陆 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置sshfence隔离机制超时时间 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt; &lt;value&gt;30000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;ha.failover-controller.cli-check.rpc-timeout.ms&lt;/name&gt; &lt;value&gt;60000&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; mapred-site.xml 文件： 12345678910111213141516171819&lt;configuration&gt; &lt;!-- 指定mr框架为yarn方式 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定mapreduce jobhistory地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;hadoop01:10020&lt;/value&gt; &lt;/property&gt; &lt;!-- 任务历史服务器的web地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;hadoop01:19888&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-sitem.xml 文件： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263&lt;configuration&gt; &lt;!-- 开启RM高可用 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定RM的cluster id --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;yrc&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定RM的名字 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;!-- 分别指定RM的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;hadoop03&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;hadoop04&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zk集群地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;hadoop01:2181,hadoop02:2181,hadoop03:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;86400&lt;/value&gt; &lt;/property&gt; &lt;!-- 启用自动恢复 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 制定resourcemanager的状态信息存储在zookeeper集群上 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; slaves 文件： 1234567hadoop01hadoop02hadoop03hadoop04 分发，在一个节点上配置，分发到其他节点，配置信息全部一致 scp -r hadoop-2.7.6 hadoop02:~/apps/ scp -r hadoop-2.7.6 hadoop03:~/apps/ scp -r hadoop-2.7.6 hadoop04:~/apps/ 配置环境变量. bashrc export HADOOP_HOME=/home/hadoop/apps/hadoop-2.7.6 export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 第一次启动 Hadoop HA 集群启动前，需要先启动 ZooKeeper 集群。需要找到 qjournal 系统的所有节点执行：hadoop-daemon.sh start journalnode，找到其中的一个 HDFS 主节点执行初始化：hadoop namenode -format。把当前初始化成功的那个 namenode 节点的工作目录中的数据文件全部拷贝到剩下的其他 namenode 节点中的对应目录：scp -r hadoopdata/ hadoop02:~/data/，选择其中的一个 namenode 节点然后执行命令去进行 zkfc 的初始化：hdfs zkfc –formatZK 正式启动 启动 hdfs：start-dfs.sh 启动 yarn 集群：start-yarn.sh 启动 mr 历史服务器：mr-jobhistory-daemon.sh start historyserver HBase HA集群安装配置安排HBase分布式数据库底层数据存储采用刚刚搭建的HDFS分布式文件系统，使用 ZooKeeper 来作为 HMaster 和 Backup HMaster 的协调，在 HMaster 失去心跳宕机时，Backup HMaster 自动切换成 active 状态，实现高可用分布式数据库。 这里将 Hadoop01 和 Hadoop04 作为 HMaster 节点，将 Hadoop01、Hadoop02、Hadoop03、Hadoop04 作为 HRegionServer 节点。 配置步骤 安装包hbase-1.2.6.tar.gz 解压 tar -zxvf hbase-1.2.6.tar.gz -C ~/apps/ 修改配置文件 hbase-env.sh 文件： export JAVA_HOME=/home/hadoop/jdk1.8.0_73 export HBASE_MANAGES_ZK=false hbase-site.xml 文件： 1234567891011121314151617&lt;property&gt; &lt;!-- 指定 hbase 在 HDFS 上存储的路径 --&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://myha/myhbase&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;!-- 指定 hbase 是分布式的 --&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;!-- 指定 zk 的地址，多个用“,”分割 --&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop01:2181,hadoop02:2181,hadoop03:2181&lt;/value&gt;&lt;/property&gt; regionservers 文件： 1234hadoop01hadoop02hadoop03hadoop04 backup-masters 文件： hadoop01 将 hadoop 的 hdfs-site.xml、core-site.xml 放在 hbase 的 conf 下 cp /home/hadoop/hadoop-2.7.6/etc/hadoop/core-site.xml . cp /home/hadoop/hadoop-2.7.6/etc/hadoop/hdfs-site.xml . 将 hbase 的安装包发送到其他节点 scp -r hbase-1.2.6 hadoop01:$PWD scp -r hbase-1.2.6 hadoop02:$PWD scp -r hbase-1.2.6 hadoop03:$PWD scp -r hbase-1.2.6 hadoop04:$PWD 启动 start-hbase.sh 系统测试在四台机器上运行jps命令，查看相关进程是否启动。 ZooKeeper状态查看在四台机器上使用zkServer.sh status命令查看 ZooKeeper 的状态。 Hadoop HA展示HDFS 启动过程： YARN 启动过程： 机架感知状态查看： Web 页面查看： HBase HA展示HBase 启动过程： Web 页面查看： HBase 底层存储情况： 系统根据需求，进行架构设计，确定一主多从的架构，接着根据系统环境和版本兼容性的选择，确定最终系统中安装包的版本，然后在仅有的四台服务器上进行集群规划，尽量使得各个节点负载均衡。","link":"/posts/10314.html"},{"title":"Java基础：泛型","text":"原文地址：http://www.importnew.com/24029.html 泛型是一个非常重要的知识点，在集合类框架中泛型被广泛应用。本文我们将从零开始来看一下泛型的设计，将会涉及到通配符处理，以及让人苦恼的类型擦除。 泛型基础泛型类我们首先定义一个简单的Box类： 12345public class Box { private String object; public void set(String object) { this.object = object; } public String get() { return object; }} 这是最常见的做法，这样做的一个坏处是Box里面现在只能装入String类型的元素，今后如果我们需要装入Integer等其他类型的元素，还必须要另外重写一个Box，代码得不到复用，使用泛型可以很好的解决这个问题。 123456public class Box&lt;T&gt; { // T stands for &quot;Type&quot; private T t; public void set(T t) { this.t = t; } public T get() { return t; }} 这样我们的Box类便可以得到复用，我们可以将T替换成任何我们想要的类型： 123Box&lt;Integer&gt; integerBox = new Box&lt;Integer&gt;();Box&lt;Double&gt; doubleBox = new Box&lt;Double&gt;();Box&lt;String&gt; stringBox = new Box&lt;String&gt;(); 泛型方法看完了泛型类，接下来我们来了解一下泛型方法。声明一个泛型方法很简单，只要在返回类型前面加上一个类似&lt;K, V&gt;的形式就行了： 123456789101112131415161718192021public class Util { public static &lt;K, V&gt; boolean compare(Pair&lt;K, V&gt; p1, Pair&lt;K, V&gt; p2) { return p1.getKey().equals(p2.getKey()) &amp;&amp; p1.getValue().equals(p2.getValue()); }}public class Pair&lt;K, V&gt; { private K key; private V value; public Pair(K key, V value) { this.key = key; this.value = value; } public void setKey(K key) { this.key = key; } public void setValue(V value) { this.value = value; } public K getKey() { return key; } public V getValue() { return value; }} 我们可以像下面这样去调用泛型方法： 123Pair&lt;Integer, String&gt; p1 = new Pair&lt;&gt;(1, &quot;apple&quot;);Pair&lt;Integer, String&gt; p2 = new Pair&lt;&gt;(2, &quot;pear&quot;);boolean same = Util.&lt;Integer, String&gt;compare(p1, p2); 或者在1.7/1.8利用type inference，让自动推导出相应的类型参数： 123Pair&lt;Integer, String&gt; p1 = new Pair&lt;&gt;(1, &quot;apple&quot;);Pair&lt;Integer, String&gt; p2 = new Pair&lt;&gt;(2, &quot;pear&quot;);boolean same = Util.compare(p1, p2); 边界符现在我们要实现这样一个功能，查找一个泛型数组中大于某个特定元素的个数，我们可以这样实现： 1234567public static &lt;T&gt; int countGreaterThan(T[] anArray, T elem) { int count = 0; for (T e : anArray) if (e &gt; elem) // compiler error ++count; return count;} 但是这样很明显是错误的，因为除了short, int, double, long, float, byte, char等原始类型，其他的类并不一定能使用操作符&gt;，所以编译器报错，那怎么解决这个问题呢？ 答案是使用边界符。 123public interface Comparable&lt;T&gt; { public int compareTo(T o);} 做一个类似于下面这样的声明，这样就等于告诉编译器类型参数T代表的都是实现了Comparable接口的类，这样等于告诉编译器它们都至少实现了compareTo方法。 123456public static &lt;T extends Comparable&lt;T&gt;&gt; int countGreaterThan(T[] anArray, T elem) { int count = 0; for (T e : anArray) if (e.compareTo(elem) &gt; 0) ++count; return count;} 通配符在了解通配符之前，我们首先必须要澄清一个概念，还是借用我们上面定义的Box类，假设我们添加一个这样的方法： 123public void boxTest(Box&lt;Number&gt; n) { /* ... */ } 那么现在Box n允许接受什么类型的参数？我们是否能够传入Box或者Box呢？ 答案是否定的，虽然Integer和Double是Number的子类，但是在泛型中Box或者Box与Box之间并没有任何的关系。这一点非常重要，接下来我们通过一个完整的例子来加深一下理解。 首先我们先定义几个简单的类，下面我们将用到它： 123class Fruit {}class Apple extends Fruit {}class Orange extends Fruit {} 下面这个例子中，我们创建了一个泛型类Reader，然后在f1()中当我们尝试Fruit f = fruitReader.readExact(apples)编译器会报错，因为List与List之间并没有任何的关系。 12345678910111213141516public class GenericReading { static List&lt;Apple&gt; apples = Arrays.asList(new Apple()); static List&lt;Fruit&gt; fruit = Arrays.asList(new Fruit()); static class Reader&lt;T&gt; { T readExact(List&lt;T&gt; list) { return list.get(0); } } static void f1() { Reader&lt;Fruit&gt; fruitReader = new Reader&lt;Fruit&gt;(); // Errors: List&lt;Fruit&gt; cannot be applied to List&lt;Apple&gt;. // Fruit f = fruitReader.readExact(apples); } public static void main(String[] args) { f1(); }} 但是按照我们通常的思维习惯，Apple和Fruit之间肯定是存在联系，然而编译器却无法识别，那怎么在泛型代码中解决这个问题呢？我们可以通过使用通配符来解决这个问题： 12345678910111213static class CovariantReader&lt;T&gt; { T readCovariant(List&lt;? extends T&gt; list) { return list.get(0); }}static void f2() { CovariantReader&lt;Fruit&gt; fruitReader = new CovariantReader&lt;Fruit&gt;(); Fruit f = fruitReader.readCovariant(fruit); Fruit a = fruitReader.readCovariant(apples);}public static void main(String[] args) { f2();} 这样就相当与告诉编译器， fruitReader的readCovariant方法接受的参数只要是满足Fruit的子类就行(包括Fruit自身)，这样子类和父类之间的关系也就关联上了。 PECS原则上面我们看到了类似&lt;? extends T&gt;的用法，利用它我们可以从list里面get元素，那么我们可不可以往list里面add元素呢？我们来尝试一下： 1234567891011121314public class GenericsAndCovariance { public static void main(String[] args) { // Wildcards allow covariance: List&lt;? extends Fruit&gt; flist = new ArrayList&lt;Apple&gt;(); // Compile Error: can't add any type of object: // flist.add(new Apple()) // flist.add(new Orange()) // flist.add(new Fruit()) // flist.add(new Object()) flist.add(null); // Legal but uninteresting // We Know that it returns at least Fruit: Fruit f = flist.get(0); }} 答案是否定，编译器不允许我们这样做，为什么呢？对于这个问题我们不妨从编译器的角度去考虑。因为List&lt;? extends Fruit&gt; flist它自身可以有多种含义： 123List&lt;? extends Fruit&gt; flist = new ArrayList&lt;Fruit&gt;();List&lt;? extends Fruit&gt; flist = new ArrayList&lt;Apple&gt;();List&lt;? extends Fruit&gt; flist = new ArrayList&lt;Orange&gt;(); 当我们尝试add一个Apple的时候，flist可能指向new ArrayList(); 当我们尝试add一个Orange的时候，flist可能指向new ArrayList(); 当我们尝试add一个Fruit的时候，这个Fruit可以是任何类型的Fruit，而flist可能只想某种特定类型的Fruit，编译器无法识别所以会报错。 所以对于实现了&lt;? extends T&gt;的集合类只能将它视为Producer向外提供(get)元素，而不能作为Consumer来对外获取(add)元素。 如果我们要add元素应该怎么做呢？可以使用&lt;? super T&gt;： 123456789101112131415161718192021public class GenericWriting { static List&lt;Apple&gt; apples = new ArrayList&lt;Apple&gt;(); static List&lt;Fruit&gt; fruit = new ArrayList&lt;Fruit&gt;(); static &lt;T&gt; void writeExact(List&lt;T&gt; list, T item) { list.add(item); } static void f1() { writeExact(apples, new Apple()); writeExact(fruit, new Apple()); } static &lt;T&gt; void writeWithWildcard(List&lt;? super T&gt; list, T item) { list.add(item) } static void f2() { writeWithWildcard(apples, new Apple()); writeWithWildcard(fruit, new Apple()); } public static void main(String[] args) { f1(); f2(); }} 这样我们可以往容器里面添加元素了，但是使用super的坏处是以后不能get容器里面的元素了，原因很简单，我们继续从编译器的角度考虑这个问题，对于List&lt;? super Apple&gt; list，它可以有下面几种含义： 123List&lt;? super Apple&gt; list = new ArrayList&lt;Apple&gt;();List&lt;? super Apple&gt; list = new ArrayList&lt;Fruit&gt;();List&lt;? super Apple&gt; list = new ArrayList&lt;Object&gt;(); 当我们尝试通过list来get一个Apple的时候，可能会get得到一个Fruit，这个Fruit可以是Orange等其他类型的Fruit。 根据上面的例子，我们可以总结出一条规律，”Producer Extends, Consumer Super”： “Producer Extends” – 如果你需要一个只读List，用它来produce T，那么使用? extends T。 “Consumer Super” – 如果你需要一个只写List，用它来consume T，那么使用? super T。 如果需要同时读取以及写入，那么我们就不能使用通配符了。 如何阅读过一些集合类的源码，可以发现通常我们会将两者结合起来一起用，比如像下面这样： 123456public class Collections { public static &lt;T&gt; void copy(List&lt;? super T&gt; dest, List&lt;? extends T&gt; src) { for (int i=0; i&lt;src.size(); i++) dest.set(i, src.get(i)); }} 类型擦除泛型中最令人苦恼的地方或许就是类型擦除了，特别是对于有C++经验的程序员。类型擦除就是说泛型只能用于在编译期间的静态类型检查，然后编译器生成的代码会擦除相应的类型信息，这样到了运行期间实际上JVM根本就知道泛型所代表的具体类型。 这样做的目的是因为泛型是1.5之后才被引入的，为了保持向下的兼容性，所以只能做类型擦除来兼容以前的非泛型代码。对于这一点，如果阅读集合框架的源码，可以发现有些类其实并不支持泛型。 说了这么多，那么泛型擦除到底是什么意思呢？我们先来看一下下面这个简单的例子： 12345678910public class Node&lt;T&gt; { private T data; private Node&lt;T&gt; next; public Node(T data, Node&lt;T&gt; next) { this.data = data; this.next = next; } public T getData() { return data; } // ...} 编译器做完相应的类型检查之后，实际上到了运行期间上面这段代码实际上将转换成： 12345678910public class Node { private Object data; private Node next; public Node(Object data, Node next) { this.data = data; this.next = next; } public Object getData() { return data; } // ...} 这意味着不管我们声明Node还是Node，到了运行期间，JVM统统视为Node 12345678910public class Node&lt;T extends Comparable&lt;T&gt;&gt; { private T data; private Node&lt;T&gt; next; public Node(T data, Node&lt;T&gt; next) { this.data = data; this.next = next; } public T getData() { return data; } // ...} 这样编译器就会将T出现的地方替换成Comparable而不再是默认的Object了： 12345678910public class Node { private Comparable data; private Node next; public Node(Comparable data, Node next) { this.data = data; this.next = next; } public Comparable getData() { return data; } // ...} 上面的概念或许还是比较好理解，但其实泛型擦除带来的问题远远不止这些，接下来我们系统地来看一下类型擦除所带来的一些问题，有些问题在C++的泛型中可能不会遇见，但是在中却需要格外小心。 问题一不允许创建泛型数组，类似下面这样的做法编译器会报错： 1List&lt;Integer&gt;[] arrayOfLists = new List&lt;Integer&gt;[2]; // compile-time error 为什么编译器不支持上面这样的做法呢？继续使用逆向思维，我们站在编译器的角度来考虑这个问题。 我们先来看一下下面这个例子： 123bject[] strings = new String[2];strings[0] = &quot;hi&quot;; // OKstrings[1] = 100; // An ArrayStoreException is thrown. 对于上面这段代码还是很好理解，字符串数组不能存放整型元素，而且这样的错误往往要等到代码运行的时候才能发现，编译器是无法识别的。接下来我们再来看一下假设支持泛型数组的创建会出现什么后果： 1234Object[] stringLists = new List&lt;String&gt;[]; // compiler error, but pretend it's allowedstringLists[0] = new ArrayList&lt;String&gt;(); // OK// An ArrayStoreException should be thrown, but the runtime can't detect it.stringLists[1] = new ArrayList&lt;Integer&gt;(); 假设我们支持泛型数组的创建，由于运行时期类型信息已经被擦除，JVM实际上根本就不知道new ArrayList()和new ArrayList()的区别。类似这样的错误假如出现才实际的应用场景中，将非常难以察觉。 如果你对上面这一点还抱有怀疑的话，可以尝试运行下面这段代码： 1234567public class ErasedTypeEquivalence { public static void main(String[] args) { Class c1 = new ArrayList&lt;String&gt;().getClass(); Class c2 = new ArrayList&lt;Integer&gt;().getClass(); System.out.println(c1 == c2); // true }} 问题二继续复用我们上面的Node的类，对于泛型代码，编译器实际上还会偷偷帮我们实现一个Bridge method。 123456789101112131415public class Node&lt;T&gt; { public T data; public Node(T data) { this.data = data; } public void setData(T data) { System.out.println(&quot;Node.setData&quot;); this.data = data; }}public class MyNode extends Node&lt;Integer&gt; { public MyNode(Integer data) { super(data); } public void setData(Integer data) { System.out.println(&quot;MyNode.setData&quot;); super.setData(data); }} 看完上面的分析之后，你可能会认为在类型擦除后，编译器会将Node和MyNode变成下面这样： 123456789101112131415public class Node { public Object data; public Node(Object data) { this.data = data; } public void setData(Object data) { System.out.println(&quot;Node.setData&quot;); this.data = data; }}public class MyNode extends Node { public MyNode(Integer data) { super(data); } public void setData(Integer data) { System.out.println(&quot;MyNode.setData&quot;); super.setData(data); }} 实际上不是这样的，我们先来看一下下面这段代码，这段代码运行的时候会抛出ClassCastException异常，提示String无法转换成Integer： 1234MyNode mn = new MyNode(5);Node n = mn; // A raw type - compiler throws an unchecked warningn.setData(&quot;Hello&quot;); // Causes a ClassCastException to be thrown.// Integer x = mn.data; 如果按照我们上面生成的代码，运行到第3行的时候不应该报错(注意我注释掉了第4行)，因为MyNode中不存在setData(String data)方法，所以只能调用父类Node的setData(Object data)方法，既然这样上面的第3行代码不应该报错，因为String当然可以转换成Object了，那ClassCastException到底是怎么抛出的？ 实际上编译器对上面代码自动还做了一个处理： 1234567891011class MyNode extends Node { // Bridge method generated by the compiler public void setData(Object data) { setData((Integer) data); } public void setData(Integer data) { System.out.println(&quot;MyNode.setData&quot;); super.setData(data); } // ...} 这也就是为什么上面会报错的原因了，setData((Integer) data)；的时候String无法转换成Integer。所以上面第2行编译器提示unchecked warning的时候，我们不能选择忽略，不然要等到运行期间才能发现异常。如果我们一开始加上Node n = mn就好了，这样编译器就可以提前帮我们发现错误。 问题三正如我们上面提到的，泛型很大程度上只能提供静态类型检查，然后类型的信息就会被擦除，所以像下面这样利用类型参数创建实例的做法编译器不会通过： 1234public static &lt;E&gt; void append(List&lt;E&gt; list) { E elem = new E(); // compile-time error list.add(elem);} 但是如果某些场景我们想要需要利用类型参数创建实例，我们应该怎么做呢？可以利用反射解决这个问题： 1234public static &lt;E&gt; void append(List&lt;E&gt; list, Class&lt;E&gt; cls) throws Exception { E elem = cls.newInstance(); // OK list.add(elem);} 我们可以像下面这样调用： 12List&lt;String&gt; ls = new ArrayList&lt;&gt;();append(ls, String.class); 实际上对于上面这个问题，还可以采用Factory和Template两种设计解决，感兴趣的朋友不妨去看一下。 问题四我们无法对泛型代码直接使用instanceof关键字，因为编译器在生成代码的时候会擦除所有相关泛型的类型信息，正如我们上面验证过的JVM在运行时期无法识别出ArrayList和ArrayList的之间的区别： 123456public static &lt;E&gt; void rtti(List&lt;E&gt; list) { if (list instanceof ArrayList&lt;Integer&gt;) { // compile-time error // ... }}=&gt; { ArrayList&lt;Integer&gt;, ArrayList&lt;String&gt;, LinkedList&lt;Character&gt;, ... } 和上面一样，我们可以使用通配符重新设置bounds来解决这个问题： 1234public static void rtti(List&lt;?&gt; list) { if (list instanceof ArrayList&lt;?&gt;) { // OK; instanceof requires a reifiable type // ...}","link":"/posts/60984.html"},{"title":"Java并发编程：AQS","text":"基本知识AbstractQueuedSynchronizer（简称AQS，队列同步器），它是构建锁或者其他同步组件的基础框架（如ReentrantLock、ReentrantReadWriteLock、Semaphore等。 它使用了一个int成员变量表示同步状态，通过内置的FIFO队列来完成资源获取线程的排队工作。JUC并发包的作者（Doug Lea）期望它能够成为实现大部分同步需求的基础。 管理同步状态同步器的主要使用方式是继承：子类通过继承同步器并实现它的抽象方法来管理同步状态。在抽象方法的实现过程中免不了要对同步状态进行更改，这时就需要使用同步器提供的3个方法来进行操作，因为它们能够保证状态的改变是安全的。方法如下： getState() setState(int newState) compareAndSetState(int expect,int update) 子类推荐被定义为自定义同步组件的静态内部类。同步器自身没有实现任何同步接口，它仅仅是定义了若干同步状态获取和释放的方法来供自定义同步组件使用，同步器既可以支持独占式地获取同步状态，也可以支持共享式地获取同步状态，这样就可以方便实现不同类型的同步组件（ReentrantLock、 ReentrantReadWriteLock和CountDownLatch等）。 同步器是实现锁（也可以是任意同步组件）的关键，在锁的实现中聚合同步器，利用同步器实现锁的语义。可以这样理解二者之间的关系：锁是面向使用者的，它定义了使用者与锁交互的接口（比如可以允许两个线程并行访问），隐藏了实现细节；同步器面向的是锁的实现者，它简化了锁的实现方式，屏蔽了同步状态管理、线程的排队、等待与唤醒等底层操作。锁和同步器很好地隔离了使用者和实现者所需关注的领域。 如何使用AQS同步器的设计是基于模板方法模式的，使用者需要继承同步器并重写指定的方法。随后将同步器组合在自定义同步组件的实现中，并调用同步器提供的模板方法，而这些模板方法将会调用使用者重写的方法。 同步器可重写的方法与描述如下所示: protected boolean tryAcquire(int arg) 独占式获取同步状态，实现该方法需要查询当前状态并判断同步状态是否符合预期，然后再进行CAS设置同步状态。 protected boolean tryRelease(int arg) 独占式释放同步状态，等待获取同步状态的线程有机会获取同步状态。 protected int tryAcquireShared(int arg) 共享式获取同步状态，返回大于等于0的值，表示获取成功，反之，获取失败 。 protected boolean tryReleaseShared(int arg) 共享式释放同步。 protected boolean isHeldExclusively() 当前同步器是否在独占模式下被线程占用，一般该方法表示是否被当前线程所独占。 利用AQS实现独占锁的简单示例，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class Mutex implements Lock { // 静态内部类，自定义同步器 private static class Sync extends AbstractQueuedSynchronizer { // 是否处于占用状态 protected boolean isHeldExclusively() { return getState() == 1; } // 当状态为0的时候获取锁 public boolean tryAcquire(int acquires) { if (compareAndSetState(0, 1)) { setExclusiveOwnerThread(Thread.currentThread()); return true; } return false; } // 释放锁，将状态设置为0 protected boolean tryRelease(int releases) { if (getState() == 0) throw new IllegalMonitorStateException(); setExclusiveOwnerThread(null); setState(0); return true; } // 返回一个Condition，每个condition都包含了一个condition队列 Condition newCondition() { return new ConditionObject(); } } // 仅需要将操作代理到Sync上即可 private final Sync sync = new Sync(); public void lock() { sync.acquire(1); } public boolean tryLock() { return sync.tryAcquire(1); } public void unlock() { sync.release(1); } public Condition newCondition() { return sync.newCondition(); } public boolean isLocked() { return sync.isHeldExclusively(); } public boolean hasQueuedThreads() { return sync.hasQueuedThreads(); } public void lockInterruptibly() throws InterruptedException { sync.acquireInterruptibly(1); } public boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException { return sync.tryAcquireNanos(1, unit.toNanos(timeout)); }} 获取和释放数据结构同步器依赖内部的同步队列（一个FIFO双向队列）来完成同步状态的管理，当前线程获取同步状态失败时，同步器会将当前线程以及等待状态等信息构造成为一个节点（Node）并将其加入同步队列，同时会阻塞当前线程；当同步状态释放时，会把首节点中的线程唤醒，使其再次尝试获取同步状态。 同步器拥有首节点（head）和尾节点（tail），没有成功获取同步状态的线程将会成为节点加入该队列的尾部，同步队列的基本结构如下所示： 加入队列的过程必须要保证线程安全，因此同步器提供了一个基于CAS的设置尾节点的方法：compareAndSetTail(Node expect,Node update)，它需要传递当前线程“认为”的尾节点和当前节点，只有设置成功后，当前节点才正式与之前的尾节点建立关联。 独占式获取和释放acquire(int arg)12345public final void acquire(int arg) { if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) // 防止获取锁成功的线程调用中断方法 selfInterrupt();} 其主要逻辑是： 首先调用自定义同步器实现的tryAcquire(int arg)方法，该方法保证线程安全的获取同步状态； 如果同步状态获取失败，则构造同步节点（独占式Node.EXCLUSIVE，同一时刻只能有一个线程成功获取同步状态）并通过addWaiter(Node node)方法将该节点加入到同步队列的尾部； 最后调用acquireQueued(Node node,int arg)方法，使得该节点以“死循环”的方式获取同步状态。如果获取不到则阻塞节点中的线程，而被阻塞线程的唤醒主要依靠前驱节点的出队或阻塞线程被中断来实现。 节点的构造以及加入同步队列，代码如下： 123456789101112131415161718192021222324252627282930private Node addWaiter(Node mode) { Node node = new Node(Thread.currentThread(), mode); Node pred = tail; // 尾节点存在时 if (pred != null) { node.prev = pred; if (compareAndSetTail(pred, node)) { pred.next = node; return node; } } enq(node); return node;}private Node enq(final Node node) { for (;;) { Node t = tail; if (t == null) { // Must initialize if (compareAndSetHead(new Node())) tail = head; } else { node.prev = t; if (compareAndSetTail(t, node)) { t.next = node; return t; } } }} 在enq(final Node node)方法中，同步器通过“死循环”来保证节点的正确添加，在“死循环”中只有通过CAS将节点设置成为尾节点之后，当前线程才能从该方法返回。否则，当前线程不断地尝试设置。可以看出，enq(final Node node)方法将并发添加节点的请求通过CAS变得“串行化”。 自旋过程 节点进入同步队列之后，就进入了一个自旋的过程，每个节点（或者说每个线程）都在自省地观察，当条件满足，获取到了同步状态，就可以从这个自旋过程中退出，否则依旧留在这个自旋过程中（并会阻塞节点的线程）。源码如下： 123456789101112131415161718192021final boolean acquireQueued(final Node node, int arg) { boolean failed = true; try { boolean interrupted = false; for (;;) { final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) { setHead(node); p.next = null; // help GC failed = false; return interrupted; } if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; } } finally { if (failed) cancelAcquire(node); }} release(int arg)线程获取同步状态并执行了相应逻辑之后，就需要释放同步状态，使得后续节点能够继续获取同步状态。通过调用同步器的release(int arg)方法可以释放同步状态，该方法在释放了同步状态之后，会唤醒其后继节点（进而使后继节点重新尝试获取同步状态）。源码如下： 123456789101112131415161718192021222324252627282930313233343536public final boolean release(int arg) { if (tryRelease(arg)) { Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; } return false;}private void unparkSuccessor(Node node) { /* * If status is negative (i.e., possibly needing signal) try * to clear in anticipation of signalling. It is OK if this * fails or if status is changed by waiting thread. */ int ws = node.waitStatus; if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); /* * Thread to unpark is held in successor, which is normally * just the next node. But if cancelled or apparently null, * traverse backwards from tail to find the actual * non-cancelled successor. */ Node s = node.next; if (s == null || s.waitStatus &gt; 0) { s = null; for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; } if (s != null) LockSupport.unpark(s.thread);} 该方法执行时，会唤醒头节点的后继节点线程，unparkSuccessor(Node node)方法使用LockSupport来唤醒处于等待状态的线程。 共享式获取和释放共享式获取与独占式获取最主要的区别在于同一时刻能否有多个线程同时获取到同步状态。以文件的读写为例，如果一个程序在对文件进行读操作，那么这一时刻对于该文件的写操作均被阻塞，而读操作能够同时进行。写操作要求对资源的独占式访问，而读操作可以是共享式访问。 acquireShared(int arg)通过调用同步器的acquireShared(int arg)方法可以共享式地获取同步状态,源码如下： 1234567891011121314151617181920212223242526272829303132public final void acquireShared(int arg) { if (tryAcquireShared(arg) &lt; 0) doAcquireShared(arg);}private void doAcquireShared(int arg) { final Node node = addWaiter(Node.SHARED); boolean failed = true; try { boolean interrupted = false; for (;;) { final Node p = node.predecessor(); if (p == head) { int r = tryAcquireShared(arg); if (r &gt;= 0) { setHeadAndPropagate(node, r); p.next = null; // help GC if (interrupted) selfInterrupt(); failed = false; return; } } if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; } } finally { if (failed) cancelAcquire(node); }} 在acquireShared(int arg)方法中，同步器调用tryAcquireShared(int arg)方法尝试获取同步状态，tryAcquireShared(int arg)方法返回值为int类型，当返回值大于等于0时，表示能够获取到同步状态。因此，在共享式获取的自旋过程中，成功获取到同步状态并退出自旋的条件就是tryAcquireShared(int arg)方法返回值大于等于0。可以看到，在doAcquireShared(int arg)方法的自旋过程中，如果当前节点的前驱为头节点时，尝试获取同步状态，如果返回值大于等于0，表示该次获取同步状态成功并从自旋过程中退出。 releaseShared(int arg)与独占式一样，共享式获取也需要释放同步状态，通过调用releaseShared(int arg)方法可以释放同步状态，源码如下： 12345678910111213141516171819202122232425public final boolean releaseShared(int arg) { if (tryReleaseShared(arg)) { doReleaseShared(); return true; } return false;}private void doReleaseShared() { for (;;) { Node h = head; if (h != null &amp;&amp; h != tail) { int ws = h.waitStatus; if (ws == Node.SIGNAL) { if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases unparkSuccessor(h); } else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS } if (h == head) // loop if head changed break; }} 对于能够支持多个线程同时访问的并发组件（比如Semaphore），它和独占式主要区别在于tryReleaseShared(int arg)方法必须确保同步状态（或者资源数）线程安全释放，一般是通过循环和CAS来保证的，因为释放同步状态的操作会同时来自多个线程。 超时和响应中断获取通过调用同步器的doAcquireNanos(int arg,long nanosTimeout)方法可以超时获取同步状态，即在指定的时间段内获取同步状态，如果获取到同步状态则返回true。否则，返回false。该方法提供了传统Java同步操作（比如synchronized关键字）所不具备的特性。 在分析该方法的实现前，先介绍一下响应中断的同步状态获取过程。在Java 5之前，当一个线程获取不到锁而被阻塞在synchronized之外时，对该线程进行中断操作，此时该线程的中断标志位会被修改，但线程依旧会阻塞在synchronized上，等待着获取锁。在Java 5中，同步器提供了acquireInterruptibly(int arg)方法，这个方法在等待获取同步状态时，如果当前线程被中断，会立刻返回，并抛出InterruptedException。 超时获取同步状态过程可以被视作响应中断获取同步状态过程的“增强版”，doAcquireNanos(int arg,long nanosTimeout)方法在支持响应中断的基础上，增加了超时获取的特性。 针对超时获取，主要需要计算出需要睡眠的时间间隔nanosTimeout，为了防止过早通知，nanosTimeout计算公式为：nanosTimeout-=now-lastTime。其中now为当前唤醒时间，lastTime为上次唤醒时间，如果nanosTimeout大于0则表示超时时间未到，需要继续睡眠nanosTimeout纳秒，反之，表示已经超时。源码如下： 123456789101112131415161718192021222324252627282930313233private boolean doAcquireNanos(int arg, long nanosTimeout) throws InterruptedException { long lastTime = System.nanoTime(); final Node node = addWaiter(Node.EXCLUSIVE); boolean failed = true; try { for (; ; ) { final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) { setHead(node); p.next = null; // help GC failed = false; return true; } if (nanosTimeout &lt;= 0) return false; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; nanosTimeout &gt; spinForTimeoutThreshold) LockSupport.parkNanos(this, nanosTimeout); long now = System.nanoTime(); //计算时间，当前时间now减去睡眠之前的时间lastTime得到已经睡眠 //的时间delta，然后被原有超时时间nanosTimeout减去，得到了 //还应该睡眠的时间 nanosTimeout -= now - lastTime; lastTime = now; if (Thread.interrupted()) throw new InterruptedException(); } } finally { if (failed) cancelAcquire(node); }} 该方法在自旋过程中，当节点的前驱节点为头节点时尝试获取同步状态，如果获取成功则从该方法返回，这个过程和独占式同步获取的过程类似，但是在同步状态获取失败的处理上有所不同。 如果当前线程获取同步状态失败，则判断是否超时（nanosTimeout小于等于0表示已经超时），如果没有超时，重新计算超时间隔nanosTimeout，然后使当前线程等待nanosTimeout纳秒（当已到设置的超时时间，该线程会从LockSupport.parkNanos(Object blocker,long nanos)方法返回）。 如果nanosTimeout小于等于spinForTimeoutThreshold（1000纳秒）时，将不会使该线程进行超时等待，而是进入快速的自旋过程。原因在于，非常短的超时等待无法做到十分精确，如果这时再进行超时等待，相反会让nanosTimeout的超时从整体上表现得反而不精确。因此，在超时非常短的场景下，同步器会进入无条件的快速自旋。 等待通知基本知识任意一个Java对象，都拥有一组监视器方法（定义在java.lang.Object上），主要包括wait()、wait(long timeout)、notify()以及notifyAll()方法，这些方法和synchronized关键字配合使用，可以实现等待/通知模式。 JUC提供了Condition接口实现类似Object的等待/通知机制，现在我们一起认识Condition接口。 Condition接口 void await() throws InterruptedException 当前线程进入等待状态直到被通知（signal）或中断。当前线程将进入运行状态且await()方法返回的情况，包括： 其他线程调用该Condition的signal()或signalAll()方法。而当前线程被选中唤醒： 其他线程（调用interrupt()方法）中断当前线程 如果当前等待线程从await()方法返回，那么表明该线程已经获取了Condition对象所对应的锁 void awaitUninterruptibly() 当前线程进入等待状态直到被通知，对中断不敏感。 long awaitNanos(long nanosTimeout) throws InterruptedException 当前线程进入等待状态直到被通知、中断或者超时。返回值表示剩余的时间，如果在nanosTimeout纳秒之前被唤醒，那么返回值就是(nanosTimeout - 实际耗时)。如果返回值是0或者负数，那么可以认定已经超时了。 boolean awaitUntil(Date deadline) throws InterruptedException 当前线程进入等待状态直到被通知、中断或者到某个时间。如果没有到指定时间被通知，方法返回true，否则，表示到了指定时间，方法返回false void signal() 唤醒一个等待在Condition上的线程，该线程从等待方法返回前必须获得与Condition相关联的锁。 void signalAll() 唤醒所有等待在Condition上的线程，能够从等待方法返回的线程必须获得与Condition相关联的锁。 Condition接口和Object的监视器区别Condition接口和Object的监视器方法两者在使用方式以及功能特性上还是有差别的，对比如下： 等待队列个数 Object Monitor Methods：一个Condition：多个 当前线程释放锁并进入等待状态，在等待状态中响应中断 Object Monitor Methods：不支持 Condition：支持 当前线程释放锁并进入等待状态到将来的某个时间Object Monitor Methods：不支持Condition：支持 怎么使用Condition当前线程调用这些方法时，需要提前获取到Condition对象关联的锁。Condition对象是由Lock对象（调用Lock对象的newCondition()方法）创建出来的，换句话说，Condition是依赖Lock对象的。 一般都会将Condition对象作为成员变量。当调用await()方法后,当前线程会释放锁并在此等待,而其他线程调用Condition对象的signal()方法,通知当前线程后,当前线程才从await()方法返回,并且在返回前已经获取了锁。 通过Condition实现基于数组的有界队列，源码如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/ * * 通过Condition实现基于数组的有界队列 * /public class BoundedQueue&lt;T&gt; { private Object[] items; private int addIndex, removeIndex, count; private Lock lock = new ReentrantLock(); private Condition notEmpty = lock.newCondition(); private Condition notFull = lock.newCondition(); public BoundedQueue(int size) { items = new Object[size]; } // 添加一个元素，如果数组满，则添加线程进入等待状态，直到有&quot;空位&quot; public void add(T t) throws InterruptedException { lock.lock(); try { while (count == items.length) notFull.await(); items[addIndex] = t; if (++addIndex == items.length) addIndex = 0; ++count; notEmpty.signal(); } finally { lock.unlock(); } } // 由头部删除一个元素，如果数组空，则删除线程进入等待状态，直到有新添加元素 public T remove() throws InterruptedException { lock.lock(); try { while (count == 0) notEmpty.await(); Object x = items[removeIndex]; if (++removeIndex == items.length) removeIndex = 0; --count; notFull.signal(); return (T) x; } finally { lock.unlock(); } }} 实现原理数据结构ConditionObject是同步器AbstractQueuedSynchronizer的内部类，因为Condition的操作需要获取相关联的锁，所以作为同步器的内部类也较为合理。每个ConditionObject对象都包含着一个队列（简称等待队列），该队列是Condition对象实现等待/通知功能的关键。 ConditionObject拥有首节点（firstWaiter）和尾节点（lastWaiter），当调用==Condition.await()==方法，将会以当前线程构造节点,并将节点从尾部加入等待队列。等待队列的基本结构如图所示： ConditionObject拥有首尾节点的引用，新增节点只需要将原有的尾节点nextWaiter指向它,并且更新尾节点即可。ConditionObject是同步器的内部类，因此每个ConditionObject实例都能够访问同步器提供的方法。 上述节点引用更新的过程并没有使用CAS保证,原因在于调用await()方法的线程必定是获取了锁的线程,也就是说该过程是由锁来保证线程安全的。 等待调用Condition的await()方法或者以await开头的方法，该方法会将当前线程构造成节点并加入等待队列中，然后释放同步状态，唤醒同步队列中的后继节点，然后当前线程会进入等待状态。 当等待队列中的节点被唤醒，则唤醒节点的线程开始尝试获取同步状态。如果不是通过其他线程调用Condition.signal()方法唤醒，而是对等待线程进行中断，则会抛出InterruptedException。 await()方法源码如下： 1234567891011121314151617181920212223242526public final void await() throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); // 1. 新增等待节点并将其保存到等待队列 Node node = addConditionWaiter(); // 2. 释放锁 int savedState = fullyRelease(node); int interruptMode = 0; // 3. 判断等待节点是否在同步队列中 while (!isOnSyncQueue(node)) { // 3.1 如果在等待队列中，则阻塞 LockSupport.park(this); // 3.2 尝试修改等待状态且添加到同步队列 if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; } // 4. 尝试获取锁 if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) // 清除已取消的等待节点 unlinkCancelledWaiters(); if (interruptMode != 0) // 抛出异常或者重新中断 reportInterruptAfterWait(interruptMode);} 通知调用Condition的signal()方法，将会唤醒在等待队列中等待时间最长的节点（首节点），在唤醒节点之前，会将节点移到同步队列中。 调用该方法的前置条件是当前线程必须获取了锁，可以看到signal()方法进行了isHeldExclusively()检查，也就是当前线程必须是获取了锁的线程。接着获取等待队列的首节点，将其移动到同步队列并使用LockSupport唤醒节点中的线程。 通过调用同步器的enq(Node node)方法，等待队列中的头节点线程安全地移动到同步队列。当节点移动到同步队列后，然后通过使用LockSupport唤醒该节点的线程。被唤醒后的线程，将从await()方法中的while循环中退出（isOnSyncQueue(Node node)方法返回true，节点已经在同步队列中），进而调用同步器的acquireQueued()方法加入到获取同步状态的竞争中。 成功获取同步状态（或者说锁）之后，被唤醒的线程将从先前调用的await()方法返回，此时该线程已经成功地获取了锁。 signal()源码如下： 123456789101112131415161718192021222324252627282930public final void signal() { // 判断当前线程是否是获取锁的线程 if (!isHeldExclusively()) throw new IllegalMonitorStateException(); Node first = firstWaiter; if (first != null) doSignal(first);}private void doSignal(Node first) { do { // 移除等待队列中的头节点 if ((firstWaiter = first.nextWaiter) == null) lastWaiter = null; first.nextWaiter = null; } while (!transferForSignal(first) &amp;&amp; (first = firstWaiter) != null);}final boolean transferForSignal(Node node) { if (!compareAndSetWaitStatus(node, Node.CONDITION, 0)) return false; // 将等待队列中的头节点动到同步队列 Node p = enq(node); int ws = p.waitStatus; if (ws &gt; 0 || !compareAndSetWaitStatus(p, ws, Node.SIGNAL)) // 唤醒等待的线程 LockSupport.unpark(node.thread); return true;} Condition的signalAll()方法，相当于对等待队列中的每个节点均执行一次signal()方法，效果就是将等待队列中所有节点全部移动到同步队列中，并唤醒每个节点的线程。 signalAll()源码如下： 1234567891011121314151617public final void signalAll() { if (!isHeldExclusively()) throw new IllegalMonitorStateException(); Node first = firstWaiter; if (first != null) doSignalAll(first);}private void doSignalAll(Node first) { lastWaiter = firstWaiter = null; do { Node next = first.nextWaiter; first.nextWaiter = null; transferForSignal(first); first = next; } while (first != null);}","link":"/posts/6926.html"},{"title":"Java并发编程：Thread（线程）","text":"Java是一种多线程语言，从诞生开始就内置了对多线程的支持。正确地使用多线程可以显著提高程序性能，但过多地创建线程和对线程的不当管理也很容易造成问题。 线程简介线程定义现代操作系统在运行一个程序时，会为其创建一个进程。例如，启动一个Java程序，操作系统就会创建一个Java进程。 线程是现代操作系统调度的最小单元，也叫轻量级进程，在一个进程里可以创建多个线程，这些线程都拥有各自的计数器、堆栈和局部变量等属性，并且能够访问共享的内存变量。处理器在这些线程上高速切换，让使用者感觉到这些线程在同时执行。 可以通过JMX查看一个普通的Java程序包含那些线程，代码如下： 1234567891011121314public class MutilThread { public static void main(String[] args) { // 获取Java线程管理MXBean ThreadMXBean threadMXBean = ManagementFactory.getThreadMXBean(); // 获取线程和线程堆栈信息 ThreadInfo[] threadInfos = threadMXBean.dumpAllThreads(false,false); // 遍历线程线程，仅打印线程ID和线程名称信息 for(ThreadInfo threadInfo:threadInfos){ System.out.println(&quot;[&quot;+threadInfo.getThreadId()+&quot;]&quot; +threadInfo.getThreadName()); } }} 运行结果如下： 使用多线程的原因正确使用多线程，总是能够给开发人员带来显著的好处，而使用多线程的原因主要有以下几点： 1、充分利用多核处理器 随着处理器上的核心数量越来越多，以及超线程技术的广泛运用，现在大多数计算机都比以往更加擅长并行计算，而处理器性能的提升方式，也从更高的主频向更多的核心发展。 2、更快的响应时间 有时我们会编写一些业务逻辑比较复杂的代码，例如，一笔订单的创建，它包括插入订单数据、生成订单快照、发送邮件通知卖家和记录货品销售数量等。用户从单击“订购”按钮开始，就要等待这些操作全部完成才能看到订购成功的结果。但是这么多业务操作，如何能够让其更快地完成呢？ 在上面的场景中，可以使用多线程技术，即将数据一致性不强的操作派发给其他线程处理（也可以使用消息队列），如生成订单快照、发送邮件等。这样做的好处是响应用户请求的线程能够尽可能快地处理完成，缩短了响应时间，提升了用户体验。 3、更好的编程模型 Java为多线程编程提供了一致的编程模型，使开发人员能够更加专注于问题的解决，即为所遇到的问题建立合适的模型，而不是绞尽脑汁地考虑如何将其多线程化。 线程优先级现代操作系统基本采用时分的形式调度运行的线程，操作系统会分出一个个时间片，线程会分配到若干时间片，当线程的时间片用完了就会发生线程调度，并等待着下次分配。 线程分配到的时间片多少也就决定了线程使用处理器资源的多少，而线程优先级就是决定线程需要多或者少分配一些处理器资源的线程属性。 在Java线程中，通过一个整型成员变量priority来控制优先级，优先级的范围从1~10，在线程构建的时候可以通过setPriority(int)方法来修改优先级，默认优先级是5，优先级高的线程分配时间片的数量要多于优先级低的线程。 设置线程优先级时，针对频繁阻塞（休眠或者I/O操作）的线程需要设置较高优先级，而偏重计算（需要较多CPU时间或者偏运算）的线程则设置较低的优先级，确保处理器不会被独占。 注意：线程优先级不能作为程序正确性的依赖，因为操作系统可以完全不用理会Java线程对于优先级的设定。 线程的状态Java线程在运行的生命周期中可能处于下表所示的6种不同的状态，在给定的一个时刻，线程只能处于其中的一个状态。 状态名称 说明 NEW 初始状态，线程被构建，但是还没有调用start()方法 RUNNABLE 运行状态，Java线程将操作系统中的就绪和运行两种状态笼统地称作“运行中” BLOCKED 阻塞状态，表示线程阻塞于锁 WAITING 等待状态，表示线程进入等待状态，进入该状态表示当前线程需要等待其他线程做出一些特定动作（通知或中断） TIME_WAITING 超时等待状态，该状态不同于WAITING，它是可以在指定的时间自行返回的 TERMINATED 终止状态，表示当前线程已经执行完毕 线程在自身的生命周期中，并不是固定地处于某个状态，而是随着代码的执行在不同的状态之间进行切换，Java线程状态变迁如下图： Java将操作系统中的运行和就绪两个状态合并称为运行状态，阻塞状态是线程阻塞在进入synchronized关键字修饰的方法或代码块（获取锁）时的状态，但是阻塞在java.concurrent包中Lock接口的线程状态却是等待状态，因为java.concurrent包中Lock接口对于阻塞的实现均使用了LockSupport类中的相关方法。 Daemon线程Daemon线程是一种支持型线程，因为它主要被用作程序中后台调度以及支持性工作。当一个Java虚拟机中不存在非Daemon线程的时候，Java虚拟机将会退出。 可以通过调用Thread.setDaemon(true)将线程设置为Daemon线程。Daemon属性需要在启动线程之前设置，不能在启动线程之后设置。 在构建Daemon线程时，不能依靠finally块中的内容来确保执行关闭或清理资源的逻辑。如下代码： 12345678910111213141516171819202122public class Daemon { public static void main(String[] args) { Thread thread = new Thread(new DeamonRunner(),&quot;DeamonRunner&quot;); thread.setDaemon(true); thread.start(); } static class DeamonRunner implements Runnable{ @Override public void run() { try { Thread.sleep(2000l); } catch (InterruptedException e) { // }finally { System.out.println(&quot;DeamonThread finally run.&quot;); } } }} 运行Deamon程序，可以看到在终端或者命令提示符没有任何输出。 启动线程在运行线程之前首先要构造一个线程对象，线程对象在构造的时候需要提供线程所需要的属性，如线程所属的线程组、线程优先级、是否是Daemon线程等信息。 123456789101112131415161718192021private void init(ThreadGroup g, Runnable target, String name, long stackSize,AccessControlContext acc) { if (name == null) { throw new NullPointerException(&quot;name cannot be null&quot;); } // 当前线程就是该线程的父线程 Thread parent = currentThread(); this.group = g; // 将daemon、priority属性设置为父线程的对应属性 this.daemon = parent.isDaemon(); this.priority = parent.getPriority(); this.name = name.toCharArray(); this.target = target; setPriority(priority); // 将父线程的InheritableThreadLocal复制过来 if (parent.inheritableThreadLocals != null) this.inheritableThreadLocals= ThreadLocal.createInheritedMap(parent.inheritableThreadLocals); // 分配一个线程ID tid = nextThreadID();} 在上述过程中，一个新构造的线程对象是由其parent线程来进行空间分配的，而child线程继承了parent是否为Deamon、优先级和加载资源的ContextClassLoader以及可继承的ThreadLocal，同时还会分配一个唯一的ID来标识这个child线程。 线程对象在初始化完成之后，调用start()方法就可以启动这个线程。线程start()方法的含义是：当前线程（即parent线程）同步告知Java虚拟机，只要线程规划器空闲，应立即启动调用start()方法的线程。 启动一个线程前，最好为这个线程设置线程名称，因为这样在使用jstack分析程序或者进行问题排查时，就会给开发人员提供一些提示，自定义的线程最好能够起个名字。 理解中断中断可以理解为线程的一个标识位属性，它表示一个运行中的线程是否被其他线程进行了中断操作。中断好比，其他线程对该线程打了个招呼，其他线程通过调用该线程的interrupt()方法对其进行中断操作。 线程通过检查自身是否被中断来进行响应，线程通过方法isInterrupted()来进行判断是否被中断，也可以调用静态方法Thread.interrupted()对当前线程的中断标识位进行复位。如果该线程已经处于终结状态，即使该线程被中断过，在调用该线程对象的isInterrupted()时依旧会返回false。 从Java的API中可以看到，许多声明抛出InterruptedException的方法（例如Thread.sleep(longmillis)方法）这些方法在抛出InterruptedException之前，Java虚拟机会先将该线程的中断标识位清除，然后抛出InterruptedException，此时调用isInterrupted()方法将会返回false。 过期的suspend()、resume()和stop()suspend()、resume()和stop()方法完成了线程的暂停、恢复和终止工作，而且非常“人性化”。但是这些API是过期的，也就是不建议使用的。 不建议使用的原因主要有：以suspend()方法为例，在调用后，线程不会释放已经占有的资源（比如锁），而是占有着资源进入睡眠状态，这样容易引发死锁问题。同样，stop()方法在终结一个线程时不保证线程的资源正常释放，通常是没有给予线程完成资源释放工作的机会，因此会导致程序可能工作在不确定状态下。 因为suspend()、resume()和stop()方法带来的副作用，这些方法才被标注为不建议使用的过期方法，而暂停和恢复操作可以用等待/通知机制来替代。 安全地终止线程中断操作是一种简便的线程间交互方式，而这种交互方式最适合用来取消或停止任务。除了中断以外，还可以利用一个boolean变量来控制是否需要停止任务并终止该线程。 123456789101112131415161718192021222324252627282930313233public class Shutdown { public static void main(String[] args) throws Exception { Runner one = new Runner(); Thread countThread = new Thread(one, &quot;CountThread&quot;); countThread.start(); // 睡眠1秒，main线程对CountThread进行中断，使CountThread能够感知中断而结束 TimeUnit.SECONDS.sleep(1); countThread.interrupt(); Runner two = new Runner(); countThread = new Thread(two, &quot;CountThread&quot;); countThread.start(); // 睡眠1秒，main线程对Runner two进行取消，使CountThread能够感知on为false而结束 TimeUnit.SECONDS.sleep(1); two.cancel(); } private static class Runner implements Runnable { private long i; private volatile boolean on = true; @Override public void run() { while (on &amp;&amp; !Thread.currentThread().isInterrupted()) { i++; } System.out.println(&quot;Count i = &quot; + i); } public void cancel() { on = false; } }} main线程通过中断操作和cancel()方法均可使CountThread得以终止。这种通过标识位或者中断操作的方式能够使线程在终止时有机会去清理资源，而不是武断地将线程停止，因此这种终止线程的做法显得更加安全和优雅。 线程间通信线程开始运行，拥有自己的栈空间，就如同一个脚本一样，按照既定的代码一步一步地执行，直到终止。但是，每个运行中的线程，如果仅仅是孤立地运行，那么没有一点儿价值，或者说价值很少，如果多个线程能够相互配合完成工作，这将会带来巨大的价值。 volatile和synchronized关键字Java支持多个线程同时访问一个对象或者对象的成员变量，由于每个线程可以拥有这个变量的拷贝（虽然对象以及成员变量分配的内存是在共享内存中的，但是每个执行的线程还是可以拥有一份拷贝，这样做的目的是加速程序的执行，这是现代多核处理器的一个显著特性），所以程序在执行过程中，一个线程看到的变量并不一定是最新的。 关键字volatile可以用来修饰字段（成员变量），就是告知程序任何对该变量的访问均需要从共享内存中获取，而对它的改变必须同步刷新回共享内存，它能保证所有线程对变量访问的可见性。 关键字synchronized可以修饰方法或者以同步块的形式来进行使用，它主要确保多个线程在同一个时刻，只能有一个线程处于方法或者同步块中，它保证了线程对变量访问的可见性和排他性。 通过使用javap工具查看生成的class文件信息来分析synchronized关键字的实现细节，代码如下 123456789101112public class Synchronized { public static void main(String[] args) { synchronized (Synchronized.class){ m(); } } public static synchronized void m(){ }} 执行javap -v Synchronized.class，部分相关输出如下所示： 对于同步块的实现使用了monitorenter和monitorexit指令，而同步方法则是依赖方法修饰符上的ACC_SYNCHRONIZED来完成。无论采用哪种方式，其本质是对一个对象的监视器进行获取，而这个获取过程是排他的，也就是同一时刻只能有一个线程获取到由synchronized所保护对象的监视器。 任意一个对象都拥有自己的监视器，当这个对象由同步块或者这个对象的同步方法调用时，执行方法的线程必须先获取到该对象的监视器才能进入同步块或者同步方法，而没有获取到监视器（执行该方法）的线程将会被阻塞在同步块和同步方法的入口处，进入BLOCKED状态。 等待/通知机制等待／通知机制是指一个线程A调用了对象O的wait()方法进入等待状态，而另一个线程B调用了对象O的notify()或notifyAll()方法，线程A收到通知后从对象O的wait()方法返回，进而执行后续操作。上述两个线程对象O来完成交互，而对象上的wait()和notify/notifyAll()的关系就如同开关信号一样，用来完成等待方通知方之间的交互工作。 等待/通知的相关方法是任意Java对象都具备的，这些方法被定义在所有对象的超类java.lang.Object上。 1、实现生产者-消费者模型，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class WaitNotify { private final static int CONTAINER_MAX_LENGTH = 3; private static Queue&lt;Integer&gt; resources = new LinkedList&lt;Integer&gt;(); //作为synchronized的对象监视器 private static final Object lock = new Object(); /** * 消息者 */ static class Consumer implements Runnable { @Override public void run() { synchronized (lock) { // 不能使用if判断，防止过早唤醒 while (resources.isEmpty()) { try { // 当前释放锁，线程进入等待状态。 lock.wait(); } catch (InterruptedException e) { e.printStackTrace(); } } System.out.println(Thread.currentThread().getName() + &quot; get number is &quot; + resources.remove()); // 唤醒所有等待状态的线程 lock.notifyAll(); } } } /** * 生产者 */ static class Producer implements Runnable { @Override public void run() { synchronized (lock) { while (resources.size() == CONTAINER_MAX_LENGTH) { try { lock.wait(); } catch (InterruptedException e) { e.printStackTrace(); } } int number = (int) (Math.random() * 100); System.out.println(Thread.currentThread().getName() + &quot; produce number is &quot; + number); resources.add(number); lock.notifyAll(); } } } public static void main(String[] args) { for (int i = 0; i &lt; 50; i++) { new Thread(new Consumer(), &quot;consumer-&quot; + i).start(); } for (int i = 0; i &lt; 50; i++) { new Thread(new Producer(), &quot;producer-&quot; + i).start(); } }} 调用wait()、notify()以及notifyAll()时需要注意的细节，如下: 使用wait()、notify()和notifyAll()时需要先对调用对象加锁。 调用wait()方法后，线程状态由RUNNING变为WAITING，并将当前线程放置到对象的等待队列。 notify()或notifyAll()方法调用后，等待线程依旧不会从wait()返回，需要调用notify()或notifAll()的线程释放锁之后，等待线程才有机会从wait()返回。 notify()方法将等待队列中的一个等待线程从等待队列中移到同步队列中，而notifyAll()方法则是将等待队列中所有的线程全部移到同步队列，被移动的线程状态由WAITING变为BLOCKED。 从wait()方法返回的前提是获得了调用对象的锁。 2、面试题：设计一个程序，启动三个线程A,B,C,各个线程只打印特定的字母，各打印10次，例如A线程只打印‘A’。要求在控制台依次显示“ABCABC…” 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class WaitNotify02 { public static void main(String[] args) { Print print = new Print(15); new Thread(print, &quot;A&quot;).start(); new Thread(print, &quot;B&quot;).start(); new Thread(print, &quot;C&quot;).start(); } private final static Object lock = new Object(); static class Print implements Runnable { private int max_print; private int count = 0; private String str = &quot;A&quot;; public Print(int max_print) { this.max_print = max_print; } @Override public void run() { synchronized (lock) { String name = Thread.currentThread().getName(); while (count &lt; max_print) { if (str.equals(name)) { System.out.print(name); if (str.equals(&quot;A&quot;)) { str = &quot;B&quot;; } else if (str.equals(&quot;B&quot;)) { str = &quot;C&quot;; } else { count++; str = &quot;A&quot;; } lock.notifyAll(); } else { try { lock.wait(); } catch (InterruptedException e) { e.printStackTrace(); } } } } } }} 管道输入/输出流管道输入/输出流和普通的文件输入/输出流或者网络输入/输出流不同之处在于，它主要用于线程之间的数据传输，而传输的媒介为内存。 管道输入/输出流主要包括了如下4种具体实现：PipedOutputStream、PipedInputStream、PipedReader和PipedWriter。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public static void piped() throws IOException { //面向于字符 PipedInputStream 面向于字节 PipedWriter writer = new PipedWriter(); PipedReader reader = new PipedReader(); //输入输出流建立连接 writer.connect(reader); Thread t1 = new Thread(new Runnable() { @Override public void run() { LOGGER.info(&quot;running&quot;); try { for (int i = 0; i &lt; 10; i++) { writer.write(i+&quot;&quot;); Thread.sleep(10); } } catch (Exception e) { } finally { try { writer.close(); } catch (IOException e) { e.printStackTrace(); } } } }); Thread t2 = new Thread(new Runnable() { @Override public void run() { LOGGER.info(&quot;running2&quot;); int msg = 0; try { while ((msg = reader.read()) != -1) { LOGGER.info(&quot;msg={}&quot;, (char) msg); } } catch (Exception e) { } } }); t1.start(); t2.start();} Java虽说是基于内存通信的，但也可以使用管道通信。需要注意的是，输入流和输出流需要首先建立连接。这样线程B就可以收到线程A发出的消息了。","link":"/posts/39519.html"},{"title":"Java并发编程：线程池","text":"Java中的线程池是运用场景最多的并发框架，几乎所有需要异步或并发执行任务的程序都可以使用线程池。在开发过程中,合理地使用线程池能够带来3个好处： 降低资源消耗：通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度：当任务到达时,任务可以不需要等到线程创建就能立即执行。 提高线程的可管理性：线程是稀缺资源,如果无限制地创建,不仅会消耗系统资源,还会降低系统的稳定性,使用线程池可以进行统一分配、调优和监控。但是,要做到合理利用线程池,必须对其实现原理了如指掌。 如何创建线程池1、使用Executors工厂类提供的静态方法来创建线程池，方法如下： 123456789101112131415161718public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());}public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));}public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());} 2、通过ThreadPoolExecutor的构造函数创建，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue) { this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), defaultHandler);}public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory) { this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, threadFactory, defaultHandler);}public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, RejectedExecutionHandler handler) { this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), handler);}public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) { if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.acc = System.getSecurityManager() == null ? null : AccessController.getContext(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;} 参数说明： 1、corePoolSize 线程池中的核心线程数，当提交一个任务时，线程池创建一个新线程执行任务，直到当前线程数等于corePoolSize；如果当前线程数为corePoolSize，继续提交的任务被保存到阻塞队列中，等待被执行；如果执行了线程池的prestartAllCoreThreads()方法，线程池会提前创建并启动所有核心线程。 2、maximumPoolSize 线程池中允许的最大线程数。如果当前阻塞队列满了，且继续提交任务，则创建新的线程执行任务，前提是当前线程数小于maximumPoolSize。 3、keepAliveTime 线程空闲时的存活时间，即当线程没有任务执行时，继续存活的时间；默认情况下，该参数只在线程数大于corePoolSize时才有用。 4、unit keepAliveTime的单位 5、workQueue 用来保存等待被执行的任务的阻塞队列，且任务必须实现Runable接口。 在JDK中提供了如下阻塞队列：ArrayBlockingQueue：基于数组结构的有界阻塞队列，按FIFO排序任务；LinkedBlockingQuene：基于链表结构的阻塞队列，按FIFO排序任务，吞吐量通常要 ArrayBlockingQuene；SynchronousQuene：一个不存储元素的阻塞队列，每个插入操作必须等到另一个线程调用移除操作，否则插入操作一直处于阻塞状态，吞吐量通常要高于LinkedBlockingQuene；priorityBlockingQuene：具有优先级的无界阻塞队列； 6、threadFactory 创建线程的工厂，通过自定义的线程工厂可以给每个新建的线程设置一个具有识别度的线程名。 7、handler 线程池的饱和策略，当阻塞队列满了，且没有空闲的工作线程，如果继续提交任务，必须采取一种策略处理该任务，线程池提供了4种策略： AbortPolicy：直接抛出异常，默认策略； CallerRunsPolicy：用调用者所在的线程来执行任务； DiscardOldestPolicy：丢弃阻塞队列中靠最前的任务，并执行当前任务； DiscardPolicy：直接丢弃任务； 当然也可以根据应用场景实现RejectedExecutionHandler接口，自定义饱和策略，如记录日志或持久化存储不能处理的任务。 使用示例 123456789101112131415161718192021222324252627282930313233343536373839404142public class ThreadPoolExecutorExample { // 1、通过threadPoolExecutor的构造函数创建线程池 private static ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(5, 10, 30, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;Runnable&gt;(10)); public static void main(String[] args) throws ExecutionException, InterruptedException { // 2、使用execute方法执行没有返回结果的任务 threadPoolExecutor.execute(new Runnable() { @Override public void run() { new Task().doSomething(); } }); // 3、使用submit方法执行有返回结果的任务且需要实现Callable接口 Future future = threadPoolExecutor.submit(new Callable&lt;Integer&gt;() { @Override public Integer call() throws Exception { return new Task().doOtherthing(); } } ); System.out.println(future.get()); }}class Task { public void doSomething() { System.out.println(&quot;doSomeThing ...&quot;); } public int doOtherthing() { System.out.println(&quot;doOtherthing ..., and return 10.&quot;); return 10; }} execute方法和submit方法的区别？ execute方法: 用于提交不需要返回值的任务,所以无法判断任务是否被线程池执行成功。 submit方法： 用于提交需要返回值的任务。线程池会返回一个future类型的对象，通过这个future对象可以判断任务是否执行成功。get()方法会阻塞当前线程直到任务完成；get(long timeout,TimeUnit unit)方法则会阻塞当前线程一段时间后立即返回,这时候有可能任务没有执行完。 ThreadPoolExecutor的实现原理当向线程池提交一个任务之后，线程池是如何处理这个任务的呢？线程池的主要处理流程如下： 如果当前运行的线程小于corePoolSize，则创建新线程来执行； 如果运行的线程等于或多于corePoolSize，则将任务加入BlockingQueue； 如果无法将任务加入BlockingQueue（队列已满），则创建新的线程来处理任务； 如果创建新线程将使当前运行的线程超出maximumPoolSize,任务将被拒绝，并调用handler.rejectedExecution(command, this)方法。 ThreadPoolExecutor采取上述步骤的总体设计思路，是为了在执行executor()方法时，尽可能地避免获取全局锁（那将会是一个严重的可伸缩瓶颈）。在ThreadPoolExecutor完成预热之后（当前运行的线程数大于等于corePoolSize），几乎所有的execute()方法调用都是执行步骤2，而步骤2不需要获取全局锁。 ThreadPoolExecutor源码分析类结构图 核心变量与方法（状态转换）123456789101112131415161718192021222324// 初始化状态和数量，状态为RUNNING，线程数为0private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));// 前3位表示状态，所有线程数占29位private static final int COUNT_BITS = Integer.SIZE - 3;// 线程池容量大小为 1 &lt;&lt; 29 - 1private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1;// 线程池状态// RUNNING状态：11100000000000000000000000000000(前3位为111)private static final int RUNNING = -1 &lt;&lt; COUNT_BITS;// SHUTDOWN状态：00000000000000000000000000000000(前3位为000)private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;// STOP状态：00100000000000000000000000000000(前3位为001)private static final int STOP = 1 &lt;&lt; COUNT_BITS;// TIDYING状态：01000000000000000000000000000000(前3位为010)private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;// TERMINATED状态：01100000000000000000000000000000(前3位为011)private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS;// 得到状态private static int runStateOf(int c) { return c &amp; ~CAPACITY; }// 得到线程数private static int workerCountOf(int c) { return c &amp; CAPACITY; }private static int ctlOf(int rs, int wc) { return rs | wc; } ThreadPoolExecutor线程池有5个状态，分别是： RUNNING：可以接受新的任务，也可以处理阻塞队列里的任务。 SHUTDOWN：不接受新的任务，但是可以处理阻塞队列里的任务。 STOP：不接受新的任务，不处理阻塞队列里的任务，中断正在处理的任务。 TIDYING：过渡状态，也就是说所有的任务都执行完了，当前线程池已经没有有效的线程，这个时候线程池的状态将会TIDYING，并且将要调用terminated方法。 TERMINATED：终止状态。terminated方法调用完成以后的状态。 线程池的状态转换过程： RUNNING -&gt; SHUTDOWNOn invocation of shutdown(), perhaps implicitly in finalize() (RUNNING or SHUTDOWN) -&gt; STOPOn invocation of shutdownNow() SHUTDOWN -&gt; TIDYINGWhen both queue and pool are empty STOP -&gt; TIDYINGWhen pool is empty TIDYING -&gt; TERMINATEDWhen the terminated() hook method has completed Threads waiting in awaitTermination() will return when the state reaches TERMINATED. 核心方法execute方法使用ThreadPoolExecutor执行任务的时候，可以使用execute或submit方法，submit方法如下： 123456public &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task) { if (task == null) throw new NullPointerException(); RunnableFuture&lt;T&gt; ftask = newTaskFor(task); execute(ftask); return ftask;} 通过源码可知submit方法同样也是由execute()完成的，execute()方法源码如下： 12345678910111213141516171819202122232425public void execute(Runnable command) { if (command == null) throw new NullPointerException(); int c = ctl.get(); // 过程1 if (workerCountOf(c) &lt; corePoolSize) { if (addWorker(command, true)) return; c = ctl.get(); } // 过程2 if (isRunning(c) &amp;&amp; workQueue.offer(command)) { int recheck = ctl.get(); // 再次检查线程池状态 if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); // 线程池中没有可用的工作线程时 else if (workerCountOf(recheck) == 0) addWorker(null, false); } else if (!addWorker(command, false)) // 过程3 // 过程4 reject(command);} addWorker方法addWorker方法的主要工作就是创建一个工作线程执行任务，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091/* * firstTask参数：用于指定新增的线程执行的第一个任务。 * core为true：表示在新增线程时会判断当前活动线程数是否少于corePoolSize， * false表示新增线程前需要判断当前活动线程数是否少于maximumPoolSize。 */private boolean addWorker(Runnable firstTask, boolean core) { retry: for (;;) { int c = ctl.get(); int rs = runStateOf(c); /** * 只有当下面两种情况会继续执行，其他直接返回false（添加失败） * 1、rs == RUNNING * 2、rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; !workQueue.isEmpty() *（执行了shutdown方法，但是阻塞队列还有任务没有执行） */ if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; for (;;) { int wc = workerCountOf(c); // 判断工作线程的数量是否超过线程池的限制 if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; // workerCount加1成功，跳出两层循坏。 if (compareAndIncrementWorkerCount(c)) break retry; /** * 能执行到这里，都是因为多线程竞争，只有两种情况 * 1、workCount发生变化，compareAndIncrementWorkerCount失败， * 这种情况不需要重新获取ctl，继续for循环即可。 * 2、runState发生变化，可能执行了shutdown或者shutdownNow， * 这种情况重新走retry，取得最新的ctl并判断状态。 */ c = ctl.get(); // Re-read ctl if (runStateOf(c) != rs) continue retry; } } // worker是否执行标识 boolean workerStarted = false; // worker是否添加成功标识 boolean workerAdded = false; // 保存创建的worker变量 Worker w = null; try { w = new Worker(firstTask); final Thread t = w.thread; // 检查线程是否创建成功 if (t != null) { // 加锁 final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { // 加锁成功，重新检查线程池的状态 int rs = runStateOf(ctl.get()); if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) { if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); // 将w存储到workers容器中 workers.add(w); int s = workers.size(); if (s &gt; largestPoolSize) largestPoolSize = s; // 添加成功标识 workerAdded = true; } } finally { mainLock.unlock(); } if (workerAdded) { // 执行任务 t.start(); workerStarted = true; } } } finally { if (! workerStarted) // 失败回退,从 wokers 移除 w, 线程数减一，尝试结束线程池(调用tryTerminate 方法) addWorkerFailed(w); } return workerStarted;} Worker类在分析t.start()之前，需要了解Worker类。其源码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859private final class Worker extends AbstractQueuedSynchronizer implements Runnable{ // 工作线程 final Thread thread; // 初始化任务 Runnable firstTask; /** Per-thread task counter */ volatile long completedTasks; Worker(Runnable firstTask) { // 禁止中断，直到runWorker setState(-1); this.firstTask = firstTask; // 很重要，worker实例被包装成thread执行的任务。 // 这样t.start启动后，将运行Worker的run方法。 this.thread = getThreadFactory().newThread(this); } public void run() { runWorker(this); } // 实现AQS的相关方法 protected boolean isHeldExclusively() { return getState() != 0; } protected boolean tryAcquire(int unused) { if (compareAndSetState(0, 1)) { setExclusiveOwnerThread(Thread.currentThread()); return true; } return false; } protected boolean tryRelease(int unused) { setExclusiveOwnerThread(null); setState(0); return true; } public void lock() { acquire(1); } public boolean tryLock() { return tryAcquire(1); } public void unlock() { release(1); } public boolean isLocked() { return isHeldExclusively(); } void interruptIfStarted() { Thread t; if (getState() &gt;= 0 &amp;&amp; (t = thread) != null &amp;&amp; !t.isInterrupted()) { try { t.interrupt(); } catch (SecurityException ignore) { } } }} Worker#runWorker方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253final void runWorker(Worker w) { Thread wt = Thread.currentThread(); Runnable task = w.firstTask; w.firstTask = null; w.unlock(); // allow interrupts boolean completedAbruptly = true; try { // 自旋操作，获取队列中的任务 while (task != null || (task = getTask()) != null) { // 加锁的作用是线程池关闭时，防止正在执行工作线程被中断。 w.lock(); /* * 在执行任务之前先做一些处理。 * 1. 如果线程池已经处于STOP状态并且当前线程没有被中断，中断线程。 * 2. 如果线程池还处于RUNNING或SHUTDOWN状态，并且当前线程已经被中断了， * 重新检查一下线程池状态，如果处于* * STOP状态并且没有被中断，那么中断线程。 */ if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); try { // hook method beforeExecute(wt, task); Throwable thrown = null; try { // 真正的开始执行任务，调用的是run方法，而不是start方法。 // 这里run的时候可能会被中断，比如线程池调用了shutdownNow方法 task.run(); } catch (RuntimeException x) { thrown = x; throw x; } catch (Error x) { thrown = x; throw x; } catch (Throwable x) { thrown = x; throw new Error(x); } finally { // hook method afterExecute(task, thrown); } } finally { task = null; w.completedTasks++; w.unlock(); } } completedAbruptly = false; } finally { // 回收woker processWorkerExit(w, completedAbruptly); }} Worker#getTask方法12345678910111213141516171819202122232425262728293031323334353637383940private Runnable getTask() { boolean timedOut = false; for (;;) { int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) { decrementWorkerCount(); return null; } int wc = workerCountOf(c); // 计算从队列获取任务的方式( poll or take) boolean timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; // 当工作线程超过其最大值或者timed = true时其workQueue.isEmpty()时，返回null。 // 这意味为该worker将被回收。 if ((wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut)) &amp;&amp; (wc &gt; 1 || workQueue.isEmpty())) { if (compareAndDecrementWorkerCount(c)) return null; continue; } try { Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); if (r != null) return r; timedOut = true; } catch (InterruptedException retry) { timedOut = false; } }} 由上可知，当allowCoreThreadTimeOut为true时，如果队列长时间没有任务，工作线程最终都会被销毁。 FutureTask上述分析submit()方法时，没有对newTaskFor()进行讲解。下面一起看看，源码如下： 123protected &lt;T&gt; RunnableFuture&lt;T&gt; newTaskFor(Runnable runnable, T value){ return new FutureTask&lt;T&gt;(runnable, value);} FutureTask定义A cancellable asynchronous computation. This class provides a base implementation of Future, with methods to start and cancel a computation, query to see if the computation is complete, and retrieve the result of the computation. The result can only be retrieved when the computation has completed; the get methods will block if the computation has not yet completed. Once the computation has completed, the computation cannot be restarted or cancelled (unless the computation is invoked using runAndReset). A FutureTask can be used to wrap a Callable or Runnable object. Because FutureTask implements Runnable, a FutureTask can be submitted to an Executor for execution. 类层次结构 属性及变量12345678910// 状态 private volatile int state;// 具体任务 private Callable&lt;V&gt; callable;// 任务执行完成返回的值private Object outcome; // 执行任务的线程private volatile Thread runner;// 等待队列，阻塞this.get()方法 private volatile WaitNode waiters; state状态具有如下值： Possible state transitions: NEW -&gt; COMPLETING -&gt; NORMAL NEW -&gt; COMPLETING -&gt; EXCEPTIONAL NEW -&gt; CANCELLED NEW -&gt; INTERRUPTING -&gt; INTERRUPTED WaitNode类 构造方法12345678910public FutureTask(Callable&lt;V&gt; callable) { if (callable == null) throw new NullPointerException(); this.callable = callable; this.state = NEW; // ensure visibility of callable}public FutureTask(Runnable runnable, V result) { this.callable = Executors.callable(runnable, result); this.state = NEW; // ensure visibility of callable} 方法java.util.concurrent.FutureTask#run()方法 123456789101112131415161718192021222324252627282930313233343536373839404142public void run() { if (state != NEW || !UNSAFE.compareAndSwapObject(this, runnerOffset,null, Thread.currentThread())) // FutureTask#runner属性赋值 return; try { Callable&lt;V&gt; c = callable; if (c != null &amp;&amp; state == NEW) { V result; boolean ran; try { result = c.call(); ran = true; } catch (Throwable ex) { result = null; ran = false; setException(ex); } if (ran) // 处理任务成功完成情况 set(result); } } finally { // runner must be non-null until state is settled to // prevent concurrent calls to run() runner = null; // state must be re-read after nulling runner to prevent // leaked interrupts int s = state; if (s &gt;= INTERRUPTING) handlePossibleCancellationInterrupt(s); }}protected void set(V v) { if (UNSAFE.compareAndSwapInt(this, stateOffset, NEW, COMPLETING)) { outcome = v; UNSAFE.putOrderedInt(this, stateOffset, NORMAL); // final state // 唤醒等待队列中所有线程 finishCompletion(); }} java.util.concurrent.FutureTask#get()方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public V get() throws InterruptedException, ExecutionException { int s = state; if (s &lt;= COMPLETING) // 任务未完成，则阻塞当前线程 s = awaitDone(false, 0L); return report(s);}private V report(int s) throws ExecutionException { Object x = outcome; if (s == NORMAL) return (V)x; if (s &gt;= CANCELLED) throw new CancellationException(); throw new ExecutionException((Throwable)x);}public V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException { if (unit == null) throw new NullPointerException(); int s = state; if (s &lt;= COMPLETING &amp;&amp; (s = awaitDone(true, unit.toNanos(timeout))) &lt;= COMPLETING) throw new TimeoutException(); return report(s);}private int awaitDone(boolean timed, long nanos) throws InterruptedException { final long deadline = timed ? System.nanoTime() + nanos : 0L; WaitNode q = null; boolean queued = false; for (;;) { if (Thread.interrupted()) { removeWaiter(q); throw new InterruptedException(); } int s = state; if (s &gt; COMPLETING) { if (q != null) q.thread = null; return s; } else if (s == COMPLETING) // cannot time out yet Thread.yield(); else if (q == null) q = new WaitNode(); else if (!queued) queued = UNSAFE.compareAndSwapObject(this, waitersOffset, q.next = waiters, q); else if (timed) { nanos = deadline - System.nanoTime(); if (nanos &lt;= 0L) { removeWaiter(q); return state; } LockSupport.parkNanos(this, nanos); } else LockSupport.park(this); }} 异常处理https://www.jianshu.com/p/30e488f4e021 关闭线程池可以通过调用线程池的shutdown()或shutdownNow()方法来关闭线程池。它们的原理是遍历线程池中的工作线程,然后逐个调用线程的interrupt方法来中断线程,所以无法响应中断的任务可能永远无法终止。 但是它们存在一定的区别，shutdownNow首先将线程池的状态设置成STOP，然后尝试停止所有的正在执行或暂停任务的线程，并返回等待执行任务的列表；而shutdown只是将线程池的状态设置成SHUTDOWN状态,然后中断所有没有正在执行任务的线程。 只要调用了这两个关闭方法中的任意一个，isShutdown方法就会返回true。当所有的任务都已关闭后，才表示线程池关闭成功，这时调用isTerminaed方法会返回true。 合理地配置线程池要想合理地配置线程池,就必须首先分析任务特性,可以从以下几个角度来分析。 任务的性质：CPU密集型任务、IO密集型任务和混合型任务。 任务的优先级：高、中和低。 任务的执行时间：长、中和短。 任务的依赖性：是否依赖其他系统资源,如数据库连接。 性质不同的任务可以用不同规模的线程池分开处理： 1）CPU密集型任务应配置尽可能小的线程,如配置N cpu +1个线程的线程池。 2）IO密集型任务线程并不是一直在执行任务,则应配置尽可能多的线程,如2*N cpu 。 3）混合型的任务,如果可以拆分,将其拆分成一个CPU密集型任务和一个IO密集型任务，只要这两个任务执行的时间相差不是太大，那么分解后执行的吞吐量将高于串行执行的吞吐量。如果这两个任务执行时间相差太大，则没必要进行分解。 优先级不同的任务可以使用优先级队列PriorityBlockingQueue来处理。它可以让优先级高的任务先得到执行，需要注意的是如果一直有优先级高的任务提交到队列里，那么优先级低的任务可能永远不能执行。 执行时间不同的任务可以交给不同规模的线程池来处理，或者也可以使用优先级队列，让执行时间短的任务先执行。 依赖数据库连接池的任务，因为线程提交SQL后需要等待数据库返回结果，如果等待的时间越长CPU空闲时间就越长，那么线程数应该设置越大，这样才能更好的利用CPU。 可以通过Runtime.getRuntime().availableProcessors()方法获得当前设备的CPU个数。 建议使用有界队列，有界队列能增加系统的稳定性和预警能力，可以根据需要设大一点，比如几千。有一次我们组使用的后台任务线程池的队列和线程池全满了，不断的抛出抛弃任务的异常，通过排查发现是数据库出现了问题，导致执行SQL变得非常缓慢，因为后台任务线程池里的任务全是需要向数据库查询和插入数据的，所以导致线程池里的工作线程全部阻塞住，任务积压在线程池里。 如果当时我们设置成无界队列，线程池的队列就会越来越多，有可能会撑满内存，导致整个系统不可用，而不只是后台任务出现问题。当然我们的系统所有的任务是用的单独的服务器部署的，而我们使用不同规模的线程池跑不同类型的任务，但是出现这样问题时也会影响到其他任务。 线程池监控如果在系统中大量使用线程池，则有必要对线程池进行监控，方便在出现问题时，可以根据线程池的使用状况快速定位问题。可以通过线程池提供的参数进行监控，在监控线程池的时机可以使用以下属性： taskCount：线程池需要执行的任务数量。 completedTaskCount：线程池在运行过程中已完成的任务数量，小于或等于taskCount。 largestPoolSize：线程池里曾经创建过的最大线程数量。通过这个数据可以直到线程池是否曾经满过。 getPoolSize：线程池的线程数量。 getActiveCount：获取活动的线程数。 通过扩展线程池进行监控。可以通过继承线程池来自定义线程池，重写线程池的beforeExecute、afterExecute和terminated方法，也可以在任务执行前、执行后和线程池关闭之前执行一些代码来进行监控。 参考资料 Java线程池ThreadPoolExecutor源码分析 【细谈Java并发】谈谈线程池：ThreadPoolExecutor","link":"/posts/35356.html"},{"title":"Java类加载：classloader的原理及应用","text":"什么是 classloaderclassloader 顾名思义，即是类加载。虚拟机把描述类的数据从 class 字节码文件加载到内存，并对数据进行检验、转换解析和初始化，最终形成可以被虚拟机直接使用的 Java 类型，这就是虚拟机的类加载机制。了解 java 的类加载机制，可以快速解决运行时的各种加载问题并快速定位其背后的本质原因，也是解决疑难杂症的利器。因此学好类加载原理也至关重要。 classloader 的加载过程类从被加载到虚拟机内存到被卸载，整个完整的生命周期包括：类加载、验证、准备、解析、初始化、使用和卸载七个阶段。其中验证，准备，解析三个部分统称为连接。接下来我们可以详细了解下类加载的各个过程。 classloader 的整个加载过程还是非常复杂的，具体的细节可以参考《深入理解 java 虚拟机》进行深入了解。为了方便记忆，我们可以使用一句话来表达其加载的整个过程，“家宴准备了西式菜”，即家 (加载) 宴(验证)准备 (准备) 了西 (解析) 式(初始化)菜。保证你以后能够很快的想起来。 虽然 classloader 的加载过程有复杂的 5 步，但事实上除了加载之外的四步，其它都是由 JVM 虚拟机控制的，我们除了适应它的规范进行开发外，能够干预的空间并不多。而加载则是我们控制 classloader 实现特殊目的最重要的手段了。也是接下来我们介绍的重点了。 classloader 双亲委托机制classloader 的双亲委托机制是指多个类加载器之间存在父子关系的时候，某个 class 类具体由哪个加载器进行加载的问题。 其具体的过程表现为：当一个类加载的过程中，它首先不会去加载，而是委托给自己的父类去加载，父类又委托给自己的父类。因此所有的类加载都会委托给顶层的父类，即 Bootstrap Classloader 进行加载，然后父类自己无法完成这个加载请求，子加载器才会尝试自己去加载。 使用双亲委派模型，Java 类随着它的加载器一起具备了一种带有优先级的层次关系，通过这种层次模型，可以避免类的重复加载，也可以避免核心类被不同的类加载器加载到内存中造成冲突和混乱，从而保证了 Java 核心库的安全。 启动类加载器 (Bootstrap Classloader) 负责将 &lt; JAVA_HOME&gt;/lib 目录下并且被虚拟机识别的类库加载到虚拟机内存中。我们常用基础库，例如 java.util.**，java.io.，java.lang. 等等都是由根加载器加载。 扩展类加载器 (Extention Classloader) 负责加载 JVM 扩展类，比如 swing 系列、内置的 js 引擎、xml 解析器等，这些类库以 javax 开头，它们的 jar 包位于 &lt; JAVA_HOME&gt;/lib/ext 目录中。 应用程序加载器 (Application Classloader) 也叫系统类加载器，它负责加载用户路径 (ClassPath) 上所指定的类库。我们自己编写的代码以及使用的第三方的 jar 包都是由它来加载的。 自定义加载器 (Custom Classloader) 通常是我们为了某些特殊目的实现的自定义加载器。 双亲委托机制看起来比较复杂，但是其本身的核心代码逻辑却是非常的清晰简单，我们着重抽取了类加载的双亲委托的核心代码如下，不过二十行左右。 根据java.lang.ClassLoader#loadClass(java.lang.String, boolean)方法可以得到如下结论： 如果不想打破双亲委派模型，那么只需要重写findClass方法即可。 如果想打破双亲委派模型，那么就重写整个loadClass方法。 defineClass方法可以把二进制流字节组成的文件转换为一个java.lang.Class—-只要二进制字节流的内容符合Class文件规范。 打破双亲委派机制案例案例一：tomcattomcat 通过 war 包进行应用的发布，它其实是违反了双亲委派机制原则的。简单看一下 tomcat 类加载器的层次结构。 对于一些需要加载的非基础类，会由一个叫作 WebAppClassLoader 的类加载器优先加载。等它加载不到的时候，再交给上层的 ClassLoader 进行加载。这个加载器用来隔绝不同应用的 .class 文件，比如你的两个应用，可能会依赖同一个第三方的不同版本，它们是相互没有影响的。 如何在同一个 JVM 里，运行着不兼容的两个版本，当然是需要自定义加载器才能完成的事。 那么 tomcat 是怎么打破双亲委派机制的呢？可以看图中的 WebAppClassLoader，它加载自己目录下的 .class 文件，并不会传递给父类的加载器。但是，它却可以使用 SharedClassLoader 所加载的类，实现了共享和分离的功能。 但是你自己写一个 ArrayList，放在应用目录里，tomcat 依然不会加载。它只是自定义的加载器顺序不同，但对于顶层来说，还是一样的。 案例二：SPIJava 中有一个 SPI 机制，全称是 Service Provider Interface，是 Java 提供的一套用来被第三方实现或者扩展的 API，它可以用来启用框架扩展和替换组件。 这个说法可能比较晦涩，但是拿我们常用的数据库驱动加载来说，就比较好理解了。在使用 JDBC 写程序之前，通常会调用下面这行代码，用于加载所需要的驱动类。 1Class.forName(&quot;com.mysql.jdbc.Driver&quot;) 这只是一种初始化模式，通过 static 代码块显式地声明了驱动对象，然后把这些信息，保存到底层的一个 List 中。这种方式我们不做过多的介绍，因为这明显就是一个接口编程的思路，没什么好奇怪的。 但是你会发现，即使删除了 Class.forName 这一行代码，也能加载到正确的驱动类，什么都不需要做，非常的神奇，它是怎么做到的呢？ 我们翻开 MySQL 的驱动代码，发现了一个奇怪的文件。之所以能够发生这样神奇的事情，就是在这里实现的。 1mysql-connector-java-8.0.15.jar!/META-INF/services/java.sql.Driver 里面的内容是： 1com.mysql.cj.jdbc.Driver 通过在 META-INF/services 目录下，创建一个以接口全限定名为命名的文件（内容为实现类的全限定名），即可自动加载这一种实现，这就是 SPI。 SPI 实际上是“基于接口的编程＋策略模式＋配置文件”组合实现的动态加载机制，主要使用 java.util.ServiceLoader 类进行动态装载。 这种方式，同样打破了双亲委派的机制。 DriverManager 类和 ServiceLoader 类都是属于 rt.jar 的。它们的类加载器是 Bootstrap ClassLoader，也就是最上层的那个。而具体的数据库驱动，却属于业务代码，这个启动类加载器是无法加载的。这就比较尴尬了，虽然凡事都要祖先过问，但祖先没有能力去做这件事情，怎么办？ 我们可以一步步跟踪代码，来看一下这个过程。 123456789101112//part1:DriverManager::loadInitialDrivers //jdk1.8 之后，变成了lazy的ensureDriversInitialized ... ServiceLoader &lt;Driver&gt; loadedDrivers = ServiceLoader.load(Driver.class); Iterator&lt;Driver&gt; driversIterator = loadedDrivers.iterator(); ... //part2:ServiceLoader::load public static &lt;T&gt; ServiceLoader&lt;T&gt; load(Class&lt;T&gt; service) { ClassLoader cl = Thread.currentThread().getContextClassLoader(); return ServiceLoader.load(service, cl); } 通过代码你可以发现 Java 玩了个魔术，它把当前的类加载器，设置成了线程的上下文类加载器。那么，对于一个刚刚启动的应用程序来说，它当前的加载器是谁呢？也就是说，启动 main 方法的那个加载器，到底是哪一个？ 所以我们继续跟踪代码。找到 Launcher 类，就是 jre 中用于启动入口函数 main 的类。我们在 Launcher 中找到以下代码。 12345678910111213141516public Launcher() { Launcher.ExtClassLoader var1; try { var1 = Launcher.ExtClassLoader.getExtClassLoader(); } catch (IOException var10) { throw new InternalError(&quot;Could not create extension class loader&quot;, var10); } try { this.loader = Launcher.AppClassLoader.getAppClassLoader(var1); } catch (IOException var9) { throw new InternalError(&quot;Could not create application class loader&quot;, var9); } Thread.currentThread().setContextClassLoader(this.loader); ... } 到此为止，事情就比较明朗了，当前线程上下文的类加载器，是应用程序类加载器。使用它来加载第三方驱动，是没有什么问题的。 我们之所以花大量的篇幅来介绍这个过程，第一，可以让你更好的看到一个打破规则的案例。第二，这个问题面试时出现的几率也是比较高的，你需要好好理解。 classloader 的应用场景类加载器是 java 语言的一项创新，也是 java 语言流行的重要原因这一。通过灵活定义 classloader 的加载机制，我们可以完成很多事情，例如解决类冲突问题，实现热加载以及热部署，甚至可以实现 jar 包的加密保护。接下来，我们会针对这些特殊场景进行逐一介绍。 依赖冲突做过多人协同开发的大型项目的同学可能深有感触。基于 maven 的 pom 进制可以方便的进行依赖管理，但是由于 maven 依赖的传递性，会导致我们的依赖错综复杂，这样就会导致引入类冲突的问题。最典型的就是 NoSuchMethodException 异常了。 在阿里平时的项目开发中是否也会遇到类似的问题吗，答案是肯定的。例如阿里内部也很多成熟的中间件，由不同的中间件团队来负责。那么当一个项目引入不同的中间件的时候，该如何避免依赖冲突的问题呢？首先我们用一个非常简单的场景来描述为什么会出现类冲突的问题。 某个业务引用了消息中间件 (例如 metaq) 和微服务中间件 (例如 dubbo)，这两个中间件也同时引用了 fastjson-2.0 和 fastjson-3.0 版本，而业务自己本身也引用了 fastjson-1.0 版本。这三个版本表现不同之处在于 classA 类中方法数目不相同，我们根据 maven 依赖处理的机制，引用路径最短的 fastjson-1.0 会真正作为应用最终的依赖，其它两个版本的 fastjson 则会被忽略，那么中间件在调用 method2() 方法的时候，则会抛出方法找不到异常。 或许你会说，将所有依赖 fastjson 的版本都升级到 3.0 不是就能解解决问题吗？确实这样能够解决问题，但是在实际操作中不太现实，首先，中间件团队和业务团队之间并不是一个团队，并不能做到高效协同，其次是中间件的稳定性是需要保障的，不可能因为包冲突问题，就升级版本，更何况一个中间件依赖的包可能有上百个，如果纯粹依赖包升级来解决，不仅稳定性难以保障，排包耗费的时间恐怕就让人窒息了。 那如何解决包冲突的问题呢？答案就是 pandora(潘多拉)，通过自定义类加载器，为每个中间件自定义一个加载器，这些加载器之间的关系是平行的，彼此没有依赖关系。这样每个中间件的 classloader 就可以加载各自版本的 fastjson。 因为一个类的全限定名以及加载该类的加载器两者共同形成了这个类在 JVM 中的惟一标识，这也是阿里 pandora 实现依赖隔离的基础。 可能到这里，你又会有新的疑惑，根据双亲委托模型，App Classloader 分别继承了 Custom Classloader. 那么业务包中的 fastjson 的 class 在加载的时候，会先委托到 Custom ClassLoader。这样不就会导致自身依赖的 fastjson 版本被忽略吗？确实如此，所以潘多拉又是如何做的呢？ 首先每个中间件对应的 ModuleClassLoader 在加载中间对应的 class 文件的同时，根据中间件配置的 export.index 负责将要需要透出的 class(主要是提供 api 接口的相关类) 索引到 exportedClassHashMap 中，然后应用程序的类加载器会持有这个 exportedClassHashMap，因此应用程序代码在 loadClass 的时候，会优先判断 exportedClassHashMap 是否存在当前类，如果存在，则直接返回，如果不存在，则再使用传统的双亲委托机制来进行类加载。 这样中间件 MoudleClassloader 不仅实现了中间件的加载，也实现了中间件关键服务类的透出。 我们可以大概看下应用程序类加载的过程： 热加载在开发项目的时候，我们需要频繁的重启应用进行程序调试，但是 java 项目的启动少则几十秒，多则几分钟。如此慢的启动速度极大地影响了程序开发的效率，那是否可以快速的进行启动，进而能够快速的进行开发验证呢？答案也是肯定的，通过 classloader 我们可以完成对变更内容的加载，然后快速的启动。 常用的热加载方案有好几个，接下来我们介绍下 spring 官方推荐的热加载方案，即 spring boot devtools。 首先我们需要思考下，为什么重新启动一个应用会比较慢，那是因为在启动应用的时候，JVM 虚拟机需要将所有的应用程序重新装载到整个虚拟机。可想而知，一个复杂的应用程序所包含的 jar 包可能有上百兆，每次微小的改动都是全量加载，那自然是很慢了。 那么我们是否可以做到，当我们修改了某个文件后，在 JVM 中替换到这个文件相关的部分而不全量的重新加载呢？而 spring boot devtools 正是基于这个思路进行处理的。 如上图所示，通常一个项目的代码由以上四部分组成，即基础类、扩展类、二方包 / 三方包、以及我们自己编写的业务代码组成。上面的一排是我们通常的类加载结构，其中业务代码和二方包 / 三方包是由应用加载器加载的。 而实际开发和调试的过程中，主要变化的是业务代码，并且业务代码相对二方包 / 三方包的内容来说会更少一些。因此我们可以将业务代码单独通过一个自定义的加载器 Custom Classloader 来进行加载，当监控发现业务代码发生改变后，我们重新加载启动，老的业务代码的相关类则由虚拟机的垃圾回收机制来自动回收。其工程流程大概如下。有兴趣的同学可以去看下源码，会更加清楚。 RestartClassLoader 为自定义的类加载器，其核心是 loadClass 的加载方式，我们发现其通过修改了双亲委托机制，默认优先从自己加载，如果自己没有加载到，从从 parent 进行加载。这样保证了业务代码可以优先被 RestartClassLoader 加载。进而通过重新加载 RestartClassLoader 即可完成应用代码部分的重新加载。 热部署热部署本质其实与热加载并没有太大的区别，通常我们说热加载是指在开发环境中进行的 classloader 加载，而热部署则更多是指在线上环境使用 classloader 的加载机制完成业务的部署。所以这二者使用的技术并没有本质的区别。 那热部署除了与热加载具有发布更快之外，还有更多的更大的优势就是具有更细的发布粒度。我们可以想像以下的一个业务场景。 假设某个营销投放平台涉及到 4 个业务方的开发，需要对会场业务进行投放。而这四个业务方的代码全部都在一个应用里面。因此某个业务方有代码变更则需要对整个应用进行发布，同时其它业务方也需要跟着回归。因此每个微小的发动，则需要走整个应用的全量发布。这种方式带来的稳定性风险估且不说，整个发布迭代的效率也可想而知了。这在整个互联网里，时间和效率就是金钱的理念下，显然是无法接受的。 那么我们完全可以通过类加载机制，将每个业务方通过一个 classloader 来加载。基于类的隔离机制，可以保障各个业务方的代码不会相互影响，同时也可以做到各个业务方进行独立的发布。其实在移动客户端，每个应用模块也可以基于类加载，实现插件化发布。本质上也是一个原理。 在阿里内部像阿拉丁投放平台，以及 crossbow 容器化平台，本质都是使用 classloader 的热加载技术，实现业务细粒度的开发部署以及多应用的合并部署。 加密保护众所周期，基于 java 开发编译产生的 jar 包是由. class 字节码组成，由于字节码的文件格式是有明确规范的。因此对于字节码进行反编译，就很容易知道其源码实现了。因此大致会存在如下两个方面的诉求。例如在服务端，我们向别人提供三方包实现的时候，不希望别人知道核心代码实现，我们可以考虑对 jar 包进行加密，在客户端则会比较普遍，那就是我们打包好的 apk 的安装包，不希望被人家反编译而被人家翻个底朝天，我们也可以对 apk 进行加密。 jar 包加密的本质，还是对字节码文件进行操作。但是 JVM 虚拟机加载 class 的规范是统一的，因此我们在最终加载 class 文件的时候，还是需要满足其 class 文件的格式规范，否则虚拟机是不能正常加载的。因此我们可以在打包的时候对 class 进行正向的加密操作，然后，在加载 class 文件之前通过自定义 classloader 先进行反向的解密操作，然后再按照标准的 class 文件标准进行加载，这样就完成了 class 文件正常的加载。因此这个加密的 jar 包只有能够实现解密方法的 classloader 才能正常加载。 我们可以贴一下简单的实现方案： 这样整个 jar 包的安全性就有一定程度的提高，至于更高安全的保障则取决于加密算法的安全性了以及如何保障加密算法的密钥不被泄露的问题了。这有种套娃的感觉，所谓安全基本都是相对的。并且这些方法也不是绝对的，例如可以通过对 classloader 进行插码，对解密后的 class 文件进行存储；另外大多数 JVM 本身并不安全，还可以修改 JVM，从 ClassLoader 之外获取解密后的代码并保存到磁盘，从而绕过上述加密所做的一切工作，当然这些操作的成本就比单纯的 class 反编译就高很多了。所以说安全保障只要做到使对方破解的成本高于收益即是安全，所以一定程度的安全性，足以减少很多低成本的攻击了。 总结本文对 classloader 的加载过程和加载原理进行了介绍，并结合类加载机制的特征，介绍了其相应的使用场景。由于篇幅限制，并没有对每种场景的具体实现细节进行介绍，而只是阐述了其基本实现思路。或许大家觉得 classloader 的应用有些复杂，但事实上只要大家对 class 从哪里加载，搞清楚 loadClass 的机制，就已经成功了一大半。正所谓万变不离其宗，抓住了本质，其它问题也就迎刃而解了。","link":"/posts/51998.html"},{"title":"MySQL：体系结构与存储引擎","text":"MySQL体系结构MySQL数据库的体系结构，如下图所示 MySQL体系结构由Client Connectors层、MySQL Server层及存储引擎层组成。 Client Connectors层 负责处理客户端的连接请求，与客户端创建连接。目前 MySQL几乎支持所有的连接类型，例如常见的 JDBC、Python、Go 等。 MySQL Server层 MySQL Server 层主要包括Connection Pool、Service &amp; utilities、SQL interface、Parser解析器、Optimizer 查询优化器、Caches 缓存等模块。 Connection Pool负责处理和存储数据库与客户端创建的连接，一个线程负责管理一个连接。Connection Pool 包括了用户认证模块，就是用户登录身份的认证和鉴权及安全管理，也就是用户执行操作权限校验。 Service &amp; utilities是管理服务&amp;工具集，包括备份恢复、安全管理、集群管理服务和工具。 SQL interface负责接收客户端发送的各种 SQL 语句，比如 DML、DDL 和存储过程等。 Parser解析器会对 SQL 语句进行语法解析生成解析树。 Optimizer查询优化器会根据解析树生成执行计划，并选择合适的索引，然后按照执行计划执行 SQL 语言并与各个存储引擎交互。 Caches缓存包括各个存储引擎的缓存部分，比如：InnoDB 存储的 Buffer Pool、MyISAM 存储引擎的 key buffer 等，Caches 中也会缓存一些权限，也包括一些 Session 级别的缓存。 存储引擎层 存储引擎包括MyISAM、InnoDB，以及支持归档的Archive和内存的Memory等。MySQL是插件式的存储引擎，只要正确定义与 MySQL Server交互的接口，任何引擎都可以访问MySQL，这也是 MySQL 流行的原因之一。 最常用的存储引擎是 InnoDB，从MySQL 5.5.5版本开始成为了默认存储引擎。 存储引擎底部是物理存储层，是文件的物理存储层，包括二进制日志、数据文件、错误日志、慢查询日志、全日志、redo/undo 日志等。 InnoDB架构下图显示了构成 InnoDB 存储引擎架构的内存和磁盘结构。 Buffer Pool缓存表数据与索引数据，把磁盘上的数据加载到缓冲池，避免每次访问都进行磁盘IO，起到加速访问的作用。 为啥不把所有数据都放到缓冲池里？ 抛开数据易失性，访问快速的反面是存储容量小，因此，只能把“最热”的数据放到“最近”的地方，以“最大限度”的降低磁盘访问。 预读磁盘读写，并不是按需读取，而是按页读取，一次至少读一页数据（一般是4K），如果未来要读取的数据就在页中，就能够省去后续的磁盘IO，提高效率。 数据访问，通常都遵循“集中读写”的原则，使用一些数据，大概率会使用附近的数据，这就是所谓的“局部性原理”，它表明提前加载是有效的，确实能够减少磁盘IO。 按页(4K)读取，和InnoDB的缓冲池设计有啥关系？ （1）磁盘访问按页读取能够提高性能，所以缓冲池一般也是按页缓存数据； （2）预读机制启示了我们，能把一些“可能要访问”的页提前加入缓冲池，避免未来的磁盘IO操作； 传统的LRU算法管理缓冲页最常见的玩法是，把入缓冲池的页放到LRU的头部，作为最近访问的元素，从而最晚被淘汰。这里又分两种情况： （1）页已经在缓冲池里，那就只做“移至”LRU头部的动作，而没有页被淘汰； （2）页不在缓冲池里，除了做“放入”LRU头部的动作，还要做“淘汰”LRU尾部页的动作； 如下图，假如管理缓冲池的LRU长度为10，缓冲了页号为1，3，5…，40，7的页。 假如，接下来要访问的数据在页号为4的页中： （1）页号为4的页，本来就在缓冲池里； （2）把页号为4的页，放到LRU的头部即可，没有页被淘汰； 假如，再接下来要访问的数据在页号为50的页中： （1）页号为50的页，原来不在缓冲池里； （2）把页号为50的页，放到LRU头部，同时淘汰尾部页号为7的页； MySQL InnoDB没有使用传统的LRU管理缓冲页，因为存在以下两个问题： （1）预读失效； （2）缓冲池污染； 预读失效由于预读(Read-Ahead)，提前把页放入了缓冲池，但最终MySQL并没有从页中读取数据，称为预读失效。 如何对预读失效进行优化？要优化预读失效，思路是： （1）让预读失败的页，停留在缓冲池LRU里的时间尽可能短； （2）让真正被读取的页，才挪到缓冲池LRU的头部； 以保证，真正被读取的热数据留在缓冲池里的时间尽可能长。 缓冲池污染当某一个SQL语句，要批量扫描大量数据时，可能导致把缓冲池的所有页都替换出去，导致大量热数据被换出，MySQL性能急剧下降，这种情况叫缓冲池污染。 例如，有一个数据量较大的用户表，当执行： 1select * from user where name like &quot;%shenjian%&quot;; 虽然结果集可能只有少量数据，但这类like不能命中索引，必须全表扫描，就需要访问大量的页。 InnoDB LRU算法 将LRU分为两个部分： 新生代(new sublist) 老生代(old sublist) 新老生代收尾相连，即：新生代的尾(tail)连接着老生代的头(head)； 新页（例如被预读的页）加入缓冲池时，只加入到老生代头部： 如果数据真正被读取（预读成功），才会加入到新生代的头部。 如果数据没有被读取，则会比新生代里的“热数据页”更早被淘汰出缓冲池。 另外，MySQL缓冲池加入了一个“老生代停留时间窗口”的机制： （1）假设T=老生代停留时间窗口； （2）插入老生代头部的页，即使立刻被访问，并不会立刻放入新生代头部； （3）只有满足“被访问”并且“在老生代停留时间”大于T，才会被放入新生代头部； 相关参数设置 innodb_buffer_pool_size：配置缓冲池的大小，在内存允许的情况下，DBA往往会建议调大这个参数，越多数据和索引放到内存里，数据库的性能会越好。 innodb_old_blocks_pct：老生代占整个LRU链长度的比例，默认是37，即整个LRU中新生代与老生代长度比例是63:37。 innodb_old_blocks_time：老生代停留时间窗口，单位是毫秒，默认是1000，即同时满足“被访问”与“在老生代停留时间超过1秒”两个条件，才会被插入到新生代头部。 写缓冲(change buffer)对于读请求，缓冲池能够减少磁盘IO，提升性能。那写请求呢？ 情况一：修改的索引页正好在缓冲池内。 （1）直接修改缓冲池中的页，一次内存操作； （2）写入redo log，一次磁盘顺序写操作； 情况二：修改的索引页正好不在缓冲池内。 （1） 先把需要修改的的索引页，从磁盘加载到缓冲池，一次磁盘随机读操作；（2）修改缓冲池中的页，一次内存操作；（3）写入redo log，一次磁盘顺序写操作； 概述写缓冲是一种应用在非唯一普通索引页(non-unique secondary index page)不在缓冲池中，对页进行了写操作，并不会立刻将磁盘页加载到缓冲池，而仅仅记录缓冲变更(buffer changes)，等未来数据被读取时，再将数据合并(merge)恢复到缓冲池中的技术。 写缓冲的目的是降低写操作的磁盘IO，提升数据库性能。 在MySQL5.5之前，叫插入缓冲(insert buffer)，只针对insert做了优化；现在对delete和update也有效，叫做写缓冲(change buffer)。 InnoDB加入写缓冲优化，上文“情况二”流程会有什么变化？ 写入流程 （1）在写缓冲中记录这个操作，一次内存操作； （2）写入redo log，一次磁盘顺序写操作； 读取流程 （1）载入索引页，缓冲池未命中，这次磁盘IO不可避免；（2）从写缓冲读取相关信息；（3）恢复索引页，放到缓冲池LRU里 为什么写缓冲优化，仅适用于非唯一普通索引页呢？如果索引设置了唯一(unique)属性，在进行修改操作时，InnoDB必须进行唯一性检查。也就是说，索引页即使不在缓冲池，磁盘上的页读取无法避免(否则怎么校验是否唯一)，此时就应该直接把相应的页放入缓冲池再进行修改。 哪些场景会触发刷写缓冲中的数据呢 数据页被访问 有一个后台线程，会认为数据库空闲时； 数据库缓冲池不够用时； 数据库正常关闭时； redo log写满时； 适用场景 数据库大部分是非唯一索引； 业务是写多读少，或者不是写后立刻读取； 相关参数设置 innodb_change_buffer_max_size：配置写缓冲的大小，占整个缓冲池的比例，默认值是25%，最大值是50%。 innodb_change_buffering：配置哪些写操作启用写缓冲，可以设置成all/none/inserts/deletes等。 double write bufferMySQL的buffer一页的大小是16K，文件系统一页的大小是4K，也就是说，MySQL将buffer中一页数据刷入磁盘，要写4个文件系统里的页。 那么，问题来了，这个操作并非原子，如果执行到一半断电，会不会出现问题呢？会，这就是所谓的“页数据损坏”。 很容易想到的方法是，能有一个“副本”，对原来的页进行还原，这个存储“副本”的地方，就是Double Write Buffer。Double Write Buffer，但它与传统的buffer又不同，它分为内存和磁盘的两层架构。 写入流程 页数据先mem copy到DWB的内存里； DWB的内存里，会先刷到DWB的磁盘上； DWB的内存里，再刷到数据磁盘存储上； DWB要写两次磁盘，会不会导致数据库性能急剧降低呢？ 步骤2属于顺序追加写； 128页（每页16K）2M的DWB，会分两次刷入磁盘，每次最多64页，即1M的数据，执行也是非常之快的。 相关参数设置 Innodb_dblwr_pages_written：记录写入DWB中页的数量。 Innodb_dblwr_writes：记录DWB写操作的次数。 SQL查询语句是如何执行？ 连接器当客户端工具与服务器端完成TCP握手后，连接器就要开始认证你的身份，这个时候用的就是你输入的用户名和密码。 如果用户名或密码不对，你就会收到一个”Access denied for user”的错误，然后客户端 程序结束执行。 如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。 这就意味着，一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。 连接完成后，如果你没有后续的动作，这个连接就处于空闲状态，可以通过show processlist命令查看。 其中的Command 列显示为“Sleep”的这一行，就表示现在系统里面有一个空闲连接。 客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数 wait_timeout 控制的，默认值是 8 小时。如果在连接被断开之后，客户端再次发送请求的话，就会收到一个错误提醒：Lost connection to MySQL server during query。这时候如果你要继续，就需要重连，然后再执行请求了。 建立连接的过程通常是比较复杂的，所以我建议你在使用中要尽量减少建立连接的动作，也就是尽量使用长连接。 但是全部使用长连接后，你可能会发现，有些时候 MySQL 占用内存涨得特别快，这是因为 MySQL在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断 开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉 (OOM)，从现象看就是 MySQL 异常重启了。 怎么解决这个问题呢?你可以考虑以下两种方案。 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后， 断开连接，之后要查询再重连。 如果你用的是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection来重新初始化连接资源。这个过程不需要重连和重新做权限验 证，但是会将连接恢复到刚刚创建完时的状态。 查询缓存MySQL 拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中。key 是查询的语句，value是查询的结果。如果你的查询能够直接在这个缓存中找到 key，那么这个 value 就会被直接返回给客户端。 但是大多数情况下我会建议你不要使用查询缓存，为什么呢?因为查询缓存往往弊大于利。 查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。 对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。 MySQL提供了“按需使用”的方式，你可以将参数query_cache_type设置成 DEMAND，这样对于默认的 SQL 语句都不使用查询缓存。 分析器分析器先会做“词法分析”。你输入的是由多个字符串和空格组成的一条 SQL 语句， MySQL 需要识别出里面的字符串分别是什么，代表什么。 MySQL 从你输入的”select”这个关键字识别出来，这是一个查询语句。它也要把字符 串“T”识别成“表名 T”，把字符串“ID”识别成“列 ID”。做完了这些识别以后，就要做“语法分析”。根据词法分析的结果，语法分析器会根据语法 规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。 优化器经过了分析器，MySQL就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联 (join)的时候，决定各个表的连接顺序。 执行器开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限，如果没有，就会返回没有权限的错误。如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。 假设表T中，ID字段没有索引，执行select * from T where id = 10。执行器的执行流程如下: 调用 InnoDB 引擎接口取这个表的第一行，判断ID值是不是10，如果不是则跳过，如果是则将这行存在结果集中; 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。 对于有索引的表，执行的逻辑也差不多。第一次调用的是“取满足条件的第一行”这个接口，之后循环取“满足条件的下一行”这个接口，这些接口都是引擎中已经定义好的。 你会在数据库的慢查询日志中看到一个 rows_examined 的字段，表示这个语句执行过程中 扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的。在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此引擎扫描行数跟 rows_examined 并不是完全相同的。 SQL更新语句是如何执行？与查询流程不一样的是，更新流程还涉及两个重要的日志模块：redo log(重做日志)和 binlog(归档日志)。 redo log不知道你还记不记得《孔乙己》这篇文章，酒店掌柜有一个粉板，专门用来记录客人的赊账记录。如果赊账的人不多，那么他可以把顾客名和账目写在板上。但如果赊账的人多了，粉板总会有记不下的时候，这个时候掌柜一定还有一个专门记录赊账的账本。 如果有人要赊账或者还账的话，掌柜一般有两种做法: 一种做法是直接把账本翻出来，把这次赊的账加上去或者扣除掉; 另一种做法是先在粉板上记下这次的账，等打烊以后再把账本翻出来核算 在生意红火柜台很忙时，掌柜一定会选择后者，因为前者操作实在是太麻烦了。首先，你得找到这个人的赊账总额那条记录。你想想，密密麻麻几十页，掌柜要找到那个名字，可能还得带上老花镜慢慢找，找到之后再拿出算盘计算，最后再将结果写回到账本上。 这整个过程想想都麻烦。相比之下，还是先在粉板上记一下方便。你想想，如果掌柜没有粉板的帮助，每次记账都得翻账本，效率是不是低得让人难以忍受? MySQL有存在这个问题，如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程 IO 成本、查找成本都很高。为了解决这个 问题，MySQL 的设计者就用了类似酒店掌柜粉板的思路来提升更新效率。MySQL使用WAL 技术，WAL 的全称 是 Write-Ahead Logging，它的关键点就是先写日志，再写磁盘，也就是先写粉板，等不忙的时候再写账本。 具体来说，当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log(粉 板)里面，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候， 将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做，这就像打烊以后掌柜做的事。 InnoDB 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，那么这块“粉板”总共就可以记录4GB 的操作。从头开始写，写到末尾就 又回到开头循环写，如下面这个图所示。 write pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件 开头。checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录 更新到数据文件。 write pos 和 checkpoint 之间的是“粉板”上还空着的部分，可以用来记录新的操作。如 果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下 来先擦掉一些记录，把 checkpoint 推进一下。 有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢 失，这个能力称为crash-safe binlogMySQL 整体来看，其实就有两块:一块是 Server 层，它主要做的是 MySQL 功能层面的事情；还有一块是引擎层，负责存储相关的具体事宜。 上面我们聊到的粉板 redo log 是 InnoDB 引擎特有的日志，而 Server 层也有自己的日志，称为 binlog(归档日志)。 为什么会有两份日志呢? 最开始 MySQL 里并没有 InnoDB 引擎。MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档。而 InnoDB 是另一个公司 以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用另外一套日志系统——也就是 redo log 来实现 crash-safe 能力。 这两种日志有以下三点不同。 redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。 redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日 志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。 redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 有了对这两个日志的概念性理解，我们再来看执行器和 InnoDB 引擎在执行这个简单的 update 语句时的内部流程。 执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器;否则，需要先从磁盘 读入内存，然后再返回。 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时redo log处于prepare状态。然后告知执行器执行完成了，随时可以提交事务。 执行器生成这个操作的binlog，并把binlog写入磁盘。 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交(commit)状态，更新完成。 两阶段提交如何让数据库恢复到指定的某一秒的状态？ 当需要恢复到指定的某一秒时，比如某天下午两点发现中午十二点有一次误删表，需要找回数据，那你可以这么做: 首先，找到最近的一次全量备份，如果你运气好，可能就是昨天晚上的一个备份，从这个备份恢复到临时库; 然后，从备份的时间点开始，将备份的 binlog 依次取出来，重放到中午误删表之前的那 个时刻。 这样你的临时库就跟误删之前的线上库一样了，然后你可以把表数据从临时库取出来，按需要恢复到线上库去。 为什么必须有“两阶段提交”呢？这是为了让两份日志之间的逻辑一致。下面通过反证法来进行解释。 由于 redo log 和 binlog 是两个独立的逻辑，如果不用两阶段提交，要么就是先写完 redo log 再写 binlog，或者采用反过来的顺序。仍然用前面的 update 语句来做例子。假设当前 ID=2 的行，字段 c 的值是 0，再假设执行 update 语句过程中在写完第一个日志后，第二个日志还没有写完期间发生了 crash，会出 现什么情况呢? 先写 redo log 后写 binlog 假设在 redo log写完，binlog还没有写完的时候， MySQL 进程异常重启。redo log写完之后，系统即使崩溃，仍然能够把数据恢复回来，恢复后这一行c的值是1。 但是由于binlog没写完就 crash 了，这时候binlog里面就没有记录这个语句。因此之后备份日志的时候，存起来的binlog里面就没有这条语句。 然后你会发现，如果需要用这个 binlog 来恢复临时库的话，由于这个语句的binlog丢 失，这个临时库就会少了这一次更新，恢复出来的这一行 c 的值就是 0，与原库的值不同。 先写 binlog 后写 redo log 如果在 binlog 写完之后 crash，由于redo log还没写， 崩溃恢复以后这个事务无效，所以这一行 c 的值是 0。但是binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。所以，在之后用 binlog 来恢复的时候就多了一个事务出来， 恢复出来的这一行 c 的值就是 1，与原库的值不同。 可以看到，如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。其实不是的，不只是误操作后需要用这个过程来恢复数据。当你需要扩容的时候，也就是需要再多搭建一些备库来增加系统的读能力的时候，现在常见的做法也是用全量备份加上应用 binlog 来实现的，这个“不一致”就会导致你的线上出现主从数据库不一致的情况。 简单说，redo log和binlog都可以用于表示事务的提交状态，而两阶段提交就是让这两 个状态保持逻辑上的一致。","link":"/posts/33457.html"},{"title":"Netty：ByteBuf与内存管理","text":"为什么需要堆外内存？在Java 中对象都是在堆内分配的，通常说的JVM 内存也就指的堆内内存，堆内内存完全被JVM 虚拟机所管理，JVM 有自己的垃圾回收算法，对于使用者来说不必关心对象的内存如何回收。 堆外内存与堆内内存相对应，对于整个机器内存而言，除堆内内存以外部分即为堆外内存。堆外内存不受 JVM 虚拟机管理，直接由操作系统管理。 堆外内存 VS 堆内内存 堆内内存由 JVM GC 自动回收内存，降低了 Java 用户的使用心智，但是GC是需要时间开销成本的，堆外内存由于不受 JVM 管理，所以在一定程度上可以降低 GC 对应用运行时带来的影响。 堆外内存需要手动释放，这一点跟 C/C++ 很像，稍有不慎就会造成应用程序内存泄漏，当出现内存泄漏问题时排查起来会相对困难。 当进行网络 I/O 操作、文件读写时，堆内内存都需要转换为堆外内存，然后再与底层设备进行交互，使用堆外内存可以减少一次内存拷贝。 堆外内存可以实现进程之间、JVM多实例之间的数据共享。 Java中堆外内存的分配方式ByteBuffer#allocateDirect12// 分配 10M 堆外内存ByteBuffer buffer = ByteBuffer.allocateDirect(10 * 1024 * 1024); 跟进 ByteBuffer.allocateDirect源码，发现其中直接调用的DirectByteBuffer 构造函数： 12345678910111213141516171819202122DirectByteBuffer(int cap) { super(-1, 0, cap, cap); boolean pa = VM.isDirectMemoryPageAligned(); int ps = Bits.pageSize(); long size = Math.max(1L, (long)cap + (pa ? ps : 0)); Bits.reserveMemory(size, cap); long base = 0; try { base = unsafe.allocateMemory(size); } catch (OutOfMemoryError x) { Bits.unreserveMemory(size, cap); throw x; } unsafe.setMemory(base, size, (byte) 0); if (pa &amp;&amp; (base % ps != 0)) { address = base + ps - (base &amp; (ps - 1)); } else { address = base; } cleaner = Cleaner.create(this, new Deallocator(base, size, cap)); att = null;} 如下图所示，描述了 DirectByteBuffer 的内存引用情况，方便你更好地理解上述源码的初始化过程。 在堆内存放的 DirectByteBuffer 对象并不大，仅仅包含堆外内存的地址、大小等属性，同时还会创建对应的 Cleaner 对象，通过 ByteBuffer 分配的堆外内存不需要手动回收，它可以被 JVM 自动回收。当堆内的 DirectByteBuffer 对象被 GC 回收时，Cleaner 就会用于回收对应的堆外内存。 Unsafe#allocateMemory从 DirectByteBuffer的构造函数中可以看出，真正分配堆外内存的逻辑还是通过unsafe.allocateMemory(size)。 Unsafe 是一个非常不安全的类，它用于执行内存访问、分配、修改等敏感操作，可以越过 JVM 限制的枷锁。Unsafe 最初并不是为开发者设计的，使用它时虽然可以获取对底层资源的控制权，但也失去了安全性的保证，所以使用 Unsafe 一定要慎重。 在 Java 中是不能直接使用 Unsafe 的，但是我们可以通过反射获取 Unsafe 实例。使用方式如下所示。 12345678910private static Unsafe unsafe = null;static { try { Field getUnsafe = Unsafe.class.getDeclaredField(&quot;theUnsafe&quot;); getUnsafe.setAccessible(true); unsafe = (Unsafe) getUnsafe.get(null); } catch (NoSuchFieldException | IllegalAccessException e) { e.printStackTrace(); }} 获得 Unsafe 实例后，我们可以通过 allocateMemory 方法分配堆外内存，allocateMemory 方法返回的是内存地址。 12// 分配 10M 堆外内存long address = unsafe.allocateMemory(10 * 1024 * 1024); 与 DirectByteBuffer 不同的是，Unsafe#allocateMemory 所分配的内存必须自己手动释放，否则会造成内存泄漏，这也是 Unsafe 不安全的体现。 1unsafe.freeMemory(address); 对于 Java 开发者而言，常用的是 ByteBuffer.allocateDirect 分配方式，我们平时常说的堆外内存泄漏都与该分配方式有关。 堆外内存的回收DirectByteBuffer 对象有可能长时间存在于堆内内存，所以它很可能晋升到 JVM 的老年代，所以这时候 DirectByteBuffer 对象的回收需要依赖 Old GC 或者 Full GC 才能触发清理。 如果长时间没有 Old GC 或者 Full GC 执行，那么堆外内存即使不再使用，也会一直在占用内存不释放，很容易将机器的物理内存耗尽，这是相当危险的。 使用 DirectByteBuffer 时如何避免物理内存被耗尽呢？由于JVM 并不知道堆外内存是不是已经不足了，所以最好通过 JVM 参数 -XX:MaxDirectMemorySize 指定堆外内存的上限大小，当堆外内存的大小超过该阈值时，就会触发一次 Full GC 进行清理回收，如果在 Full GC 之后还是无法满足堆外内存的分配，那么程序将会抛出 OOM 异常。 此外在 ByteBuffer.allocateDirect 分配的过程中，如果没有足够的空间分配堆外内存，在 Bits.reserveMemory 方法中也会主动调用 System.gc() 强制执行 Full GC，但是在生产环境一般都是设置了 -XX:+DisableExplicitGC，System.gc() 是不起作用的，所以依赖 System.gc() 并不是一个好办法。 Cleaner 是如何与 GC 关联起来的呢？Java 对象有四种引用方式：强引用 StrongReference、软引用 SoftReference、弱引用 WeakReference 和虚引用 PhantomReference。其中 PhantomReference 是最不常用的一种引用方式，Cleaner 就属于 PhantomReference 的子类。 123456789101112131415public class Cleaner extends java.lang.ref.PhantomReference&lt;java.lang.Object&gt; { private static final java.lang.ref.ReferenceQueue&lt;java.lang.Object&gt; dummyQueue; private static sun.misc.Cleaner first; private sun.misc.Cleaner next; private sun.misc.Cleaner prev; private final java.lang.Runnable thunk; public void clean() {}} PhantomReference不能被单独使用，需要与引用队列 ReferenceQueue 联合使用。 当初始化堆外内存时，内存中的对象引用情况如下图所示： first 是 Cleaner 类中的静态变量，Cleaner 对象在初始化时会加入 Cleaner 链表中。DirectByteBuffer 对象包含堆外内存的地址、大小以及 Cleaner 对象的引用，ReferenceQueue 用于保存需要回收的 Cleaner 对象。 当发生 GC 时，DirectByteBuffer 对象被回收，内存中的对象引用情况发生了如下变化： 此时 Cleaner 对象不再有任何引用关系，在下一次 GC 时，该 Cleaner 对象将被添加到 ReferenceQueue 中，并执行 clean() 方法。clean() 方法主要做两件事情： 将 Cleaner 对象从 Cleaner 链表中移除； 调用 unsafe.freeMemory 方法清理堆外内存。 ByteBuffer有哪些缺陷和痛点 ByteBuffer 包含以下四个基本属性： mark：为某个读取过的关键位置做标记，方便回退到该位置； position：当前读取的位置； limit：buffer 中有效的数据长度大小； capacity：初始化时的空间容量。 以上四个基本属性的关系是：mark &lt;= position &lt;= limit &lt;= capacity。 ByteBuffer具有以下的缺陷： ByteBuffer 分配的长度是固定的，无法动态扩缩容，所以很难控制需要分配多大的容量。在使用 ByteBuffer 时，为了避免容量不足问题，你必须每次在存放数据的时候对容量大小做校验，如果超出 ByteBuffer 最大容量，那么需要重新开辟一个更大容量的 ByteBuffer，将已有的数据迁移过去。整个过程相对烦琐，对开发者而言是非常不友好的。 ByteBuffer 只能通过 position 获取当前可操作的位置，因为读写共用的 position 指针，所以需要频繁调用 flip、rewind 方法切换读写状态，开发者必须很小心处理 ByteBuffer 的数据读写，稍不留意就会出错。 ByteBufByteBuffer 作为网络通信中高频使用的数据载体，显然不能够满足 Netty 的需求，Netty 重新实现了一个性能更高、易用性更强的 ByteBuf，相比于 ByteBuffer 它提供了很多非常酷的特性： 容量可以按需动态扩展，类似于 StringBuffer； 读写采用了不同的指针，读写模式可以随意切换，不需要调用 flip 方法； 通过内置的复合缓冲类型可以实现零拷贝； 支持引用计数； 支持缓存池 ByteBuf 内部结构 ByteBuf 包含三个指针：读指针 readerIndex、写指针 writeIndex、最大容量 maxCapacity，根据指针的位置又可以将 ByteBuf 内部结构可以分为四个部分： 第一部分是废弃字节，表示已经丢弃的无效字节数据。 第二部分是可读字节，表示 ByteBuf 中可以被读取的字节内容，可以通过 writeIndex - readerIndex 计算得出。从 ByteBuf 读取 N 个字节，readerIndex 就会自增 N，readerIndex 不会大于 writeIndex，当 readerIndex == writeIndex 时，表示 ByteBuf 已经不可读。 第三部分是可写字节，向 ByteBuf 中写入数据都会存储到可写字节区域。向 ByteBuf 写入 N 字节数据，writeIndex 就会自增 N，当 writeIndex 超过 capacity，表示 ByteBuf 容量不足，需要扩容。 第四部分是可扩容字节，表示 ByteBuf 最多还可以扩容多少字节，当 writeIndex 超过 capacity 时，会触发 ByteBuf 扩容，最多扩容到 maxCapacity 为止，超过 maxCapacity 再写入就会出错。 由此可见，Netty 重新设计的 ByteBuf 有效地区分了可读、可写以及可扩容数据，解决了 ByteBuffer 无法扩容以及读写模式切换烦琐的缺陷。 引用计数ByteBuf 是基于引用计数设计的，它实现了 ReferenceCounted 接口，ByteBuf 的生命周期是由引用计数所管理。 当新创建一个 ByteBuf 对象时，它的初始引用计数为 1，当 ByteBuf 调用 release() 后，引用计数减 1，所以不要误以为调用了 release() 就会保证 ByteBuf 对象一定会被回收。 1234ByteBuf buffer = ctx.alloc().directbuffer();assert buffer.refCnt() == 1;buffer.release();assert buffer.refCnt() == 0; 引用计数对于 Netty 设计缓存池化有非常大的帮助，当引用计数为 0，该 ByteBuf 可以被放入到对象池中，避免每次使用 ByteBuf 都重复创建，对于实现高性能的内存管理有着很大的意义。 此外 Netty 可以利用引用计数的特点实现内存泄漏检测工具。JVM 并不知道 Netty 的引用计数是如何实现的，当 ByteBuf 对象不可达时，一样会被 GC 回收掉，但是如果此时 ByteBuf 的引用计数不为 0，那么该对象就不会释放或者被放入对象池，从而发生了内存泄漏。Netty 会对分配的 ByteBuf 进行抽样分析，检测 ByteBuf 是否已经不可达且引用计数大于 0，判定内存泄漏的位置并输出到日志中，你需要关注日志中 LEAK 关键字。 ByteBuf 分类ByteBuf 可以划分为三个不同的维度：Heap/Direct、Pooled/Unpooled和Unsafe/非 Unsafe。 Heap/Direct 就是堆内和堆外内存。Heap 指的是在 JVM 堆内分配，底层依赖的是字节数据；Direct 则是堆外内存，不受 JVM 限制，分配方式依赖 JDK 底层的 ByteBuffer。 Pooled/Unpooled 表示池化还是非池化内存。Pooled 是从预先分配好的内存中取出，使用完可以放回 ByteBuf 内存池，等待下一次分配。而 Unpooled 是直接调用系统 API 去申请内存，确保能够被 JVM GC 管理回收。 Unsafe/非 Unsafe 的区别在于操作方式是否安全。 Unsafe 表示每次调用 JDK 的 Unsafe 对象操作物理内存，依赖 offset + index 的方式操作数据。非 Unsafe 则不需要依赖 JDK 的 Unsafe 对象，直接通过数组下标的方式操作数据。 ByteBuf核心API指针操作 API readerIndex() &amp; writeIndex() readerIndex() 返回的是当前的读指针的 readerIndex 位置，writeIndex() 返回的当前写指针 writeIndex 位置。 markReaderIndex() &amp; resetReaderIndex() markReaderIndex() 用于保存 readerIndex 的位置，resetReaderIndex() 则将当前 readerIndex 重置为之前保存的位置。 这对 API 在实现协议解码时最为常用，例如在上述自定义解码器的源码中，在读取协议内容长度字段之前，先使用 markReaderIndex() 保存了 readerIndex 的位置，如果 ByteBuf 中可读字节数小于长度字段的值，则表示 ByteBuf 还没有一个完整的数据包，此时直接使用 resetReaderIndex() 重置 readerIndex 的位置。 数据读写 API isReadable() isReadable() 用于判断 ByteBuf 是否可读，如果 writerIndex 大于 readerIndex，那么 ByteBuf 是可读的，否则是不可读状态。 readableBytes() readableBytes() 可以获取 ByteBuf 当前可读取的字节数，可以通过 writerIndex - readerIndex 计算得到。 readBytes(byte[] dst) &amp; writeBytes(byte[] src) readBytes() 和 writeBytes() 是两个最为常用的方法。readBytes() 是将 ByteBuf 的数据读取相应的字节到字节数组 dst 中，readBytes() 经常结合 readableBytes() 一起使用，dst 字节数组的大小通常等于 readableBytes() 的大小。 readByte() &amp; writeByte(int value) readByte() 是从 ByteBuf 中读取一个字节，相应的 readerIndex + 1；同理 writeByte 是向 ByteBuf 写入一个字节，相应的 writerIndex + 1。类似的 Netty 提供了 8 种基础数据类型的读取和写入，例如 readChar()、readShort()、readInt()、readLong()、writeChar()、writeShort()、writeInt()、writeLong() 等。 getByte(int index) &amp; setByte(int index, int value) 与 readByte() 和 writeByte() 相对应的还有 getByte() 和 setByte()，get/set 系列方法也提供了 8 种基础类型的读写，那么这两个系列的方法有什么区别呢？read/write 方法在读写时会改变readerIndex 和 writerIndex 指针，而 get/set 方法则不会改变指针位置。 内存管理 API release() &amp; retain() 之前已经介绍了引用计数的基本概念，每调用一次 release() 引用计数减 1，每调用一次 retain() 引用计数加 1。 slice() &amp; duplicate() slice() 等同于 slice(buffer.readerIndex(), buffer.readableBytes())，默认截取 readerIndex 到 writerIndex 之间的数据，最大容量 maxCapacity 为原始 ByteBuf 的可读取字节数，底层分配的内存、引用计数都与原始的 ByteBuf 共享。 duplicate() 与 slice() 不同的是，duplicate()截取的是整个原始 ByteBuf 信息，底层分配的内存、引用计数也是共享的。如果向 duplicate() 分配出来的 ByteBuf 写入数据，那么都会影响到原始的 ByteBuf 底层数据。 copy() copy() 会从原始的 ByteBuf 中拷贝所有信息，所有数据都是独立的，向 copy() 分配的 ByteBuf 中写数据不会影响原始的 ByteBuf。 ByteBuf API 使用时的注意点： write 系列方法会改变 writerIndex 位置，当 writerIndex 等于 capacity 的时候，Buffer 置为不可写状态； 向不可写 Buffer 写入数据时，Buffer 会尝试扩容，但是扩容后 capacity 最大不能超过 maxCapacity，如果写入的数据超过 maxCapacity，程序会直接抛出异常； read 系列方法会改变 readerIndex 位置，get/set 系列方法不会改变 readerIndex/writerIndex 位置。 Netty的零拷贝技术Netty 中的零拷贝技术除了操作系统级别的功能封装，更多的是面向用户态的数据操作优化，主要体现在以下 5 个方面： 堆外内存，避免 JVM 堆内存到堆外内存的数据拷贝。 CompositeByteBuf 类，可以组合多个 Buffer 对象合并成一个逻辑上的对象，避免通过传统内存拷贝的方式将几个 Buffer 合并成一个大的 Buffer。 通过 Unpooled.wrappedBuffer 可以将 byte 数组包装成 ByteBuf 对象，包装过程中不会产生内存拷贝。 ByteBuf.slice 操作与 Unpooled.wrappedBuffer 相反，slice 操作可以将一个 ByteBuf 对象切分成多个 ByteBuf 对象，切分过程中不会产生内存拷贝，底层共享一个 byte 数组的存储空间。 Netty 使用 FileRegion 实现文件传输，FileRegion 底层封装了 FileChannel#transferTo() 方法，可以将文件缓冲区的数据直接传输到目标 Channel，避免内核缓冲区和用户态缓冲区之间的数据拷贝，这属于操作系统级别的零拷贝。 堆外内存如果在 JVM 内部执行 I/O 操作时，必须将数据拷贝到堆外内存，才能执行系统调用。这是所有 VM 语言都会存在的问题。 那么为什么操作系统不能直接使用 JVM 堆内存进行 I/O 的读写呢？主要有两点原因： 第一，操作系统并不感知 JVM 的堆内存，而且 JVM 的内存布局与操作系统所分配的是不一样的，操作系统并不会按照 JVM 的行为来读写数据。 第二，同一个对象的内存地址随着 JVM GC 的执行可能会随时发生变化，例如 JVM GC 的过程中会通过压缩来减少内存碎片，这就涉及对象移动的问题了。 Netty 在进行 I/O 操作时都是使用的堆外内存，可以避免数据从 JVM 堆内存到堆外内存的拷贝。 CompositeByteBufCompositeByteBuf 是 Netty 中实现零拷贝机制非常重要的一个数据结构，CompositeByteBuf 可以理解为一个虚拟的 Buffer 对象，它是由多个 ByteBuf 组合而成，但是在 CompositeByteBuf 内部保存着每个 ByteBuf 的引用关系，从逻辑上构成一个整体。 比较常见的像 HTTP 协议数据可以分为头部信息 header和消息体数据 body，分别存在两个不同的 ByteBuf 中，通常我们需要将两个 ByteBuf 合并成一个完整的协议数据进行发送，可以使用如下方式完成： 123ByteBuf httpBuf = Unpooled.buffer(header.readableBytes() + body.readableBytes());httpBuf.writeBytes(header);httpBuf.writeBytes(body); 可以看出，如果想实现 header 和 body 这两个 ByteBuf 的合并，需要先初始化一个新的 httpBuf，然后再将 header 和 body 分别拷贝到新的 httpBuf。合并过程中涉及两次 CPU 拷贝，这非常浪费性能。 如果使用 CompositeByteBuf 如何实现类似的需求呢？如下所示： 12CompositeByteBuf httpBuf = Unpooled.compositeBuffer();httpBuf.addComponents(true, header, body); CompositeByteBuf 通过调用 addComponents() 方法来添加多个 ByteBuf，但是底层的 byte 数组是复用的，不会发生内存拷贝。但对于用户来说，它可以当作一个整体进行操作。 CompositeByteBuf 的内部结构如下图所示： CompositeByteBuf 内部维护了一个 Components 数组。在每个 Component 中存放着不同的 ByteBuf，各个 ByteBuf 独立维护自己的读写索引，而 CompositeByteBuf 自身也会单独维护一个读写索引。由此可见，Component 是实现 CompositeByteBuf 的关键所在。 CompositeByteBuf实现原理？ Component 结构定义如下： 123456789private static final class Component { final ByteBuf srcBuf; // 原始的 ByteBuf final ByteBuf buf; // srcBuf 去除包装之后的 ByteBuf int srcAdjustment; // CompositeByteBuf 的起始索引相对于 srcBuf 读索引的偏移 int adjustment; // CompositeByteBuf 的起始索引相对于 buf 的读索引的偏移 int offset; // Component 相对于 CompositeByteBuf 的起始索引位置 int endOffset; // Component 相对于 CompositeByteBuf 的结束索引位置 // 省略其他代码} Unpooled.wrappedBuffer 操作Unpooled.wrappedBuffer也是创建 CompositeByteBuf 对象的另一种推荐做法。Unpooled 提供了一系列用于包装数据源的 wrappedBuffer 方法，如下所示： Unpooled.wrappedBuffer 方法可以将不同的数据源的一个或者多个数据包装成一个大的 ByteBuf 对象，其中数据源的类型包括 byte[]、ByteBuf、ByteBuffer。包装的过程中不会发生数据拷贝操作，包装后生成的 ByteBuf 对象和原始 ByteBuf 对象是共享底层的 byte 数组。 ByteBuf.slice 操作ByteBuf.slice 和 Unpooled.wrappedBuffer 的逻辑正好相反，ByteBuf.slice 是将一个 ByteBuf 对象切分成多个共享同一个底层存储的 ByteBuf 对象。 ByteBuf 提供了两个 slice 切分方法: 12public ByteBuf slice();public ByteBuf slice(int index, int length); 假设我们已经有一份完整的 HTTP 数据，可以通过 slice 方法切分获得 header 和 body 两个 ByteBuf 对象，对应的内容分别为 “header” 和 “body”，实现方式如下： 123ByteBuf httpBuf = ...ByteBuf header = httpBuf.slice(0, 6);ByteBuf body = httpBuf.slice(6, 4); 通过 slice 切分后都会返回一个新的 ByteBuf 对象，而且新的对象有自己独立的 readerIndex、writerIndex 索引。由于新的 ByteBuf 对象与原始的 ByteBuf 对象数据是共享的，所以通过新的 ByteBuf 对象进行数据操作也会对原始 ByteBuf 对象生效。 文件传输 FileRegion在 Netty 源码的 example 包中，提供了 FileRegion 的使用示例，以下代码片段摘自 FileServerHandler.java。 12345678910111213141516171819202122232425@Overridepublic void channelRead0(ChannelHandlerContext ctx, String msg) throws Exception { RandomAccessFile raf = null; long length = -1; try { raf = new RandomAccessFile(msg, &quot;r&quot;); length = raf.length(); } catch (Exception e) { ctx.writeAndFlush(&quot;ERR: &quot; + e.getClass().getSimpleName() + &quot;: &quot; + e.getMessage() + '\\n'); return; } finally { if (length &lt; 0 &amp;&amp; raf != null) { raf.close(); } } ctx.write(&quot;OK: &quot; + raf.length() + '\\n'); if (ctx.pipeline().get(SslHandler.class) == null) { // SSL not enabled - can use zero-copy file transfer. ctx.write(new DefaultFileRegion(raf.getChannel(), 0, length)); } else { // SSL enabled - cannot use zero-copy file transfer. ctx.write(new ChunkedFile(raf)); } ctx.writeAndFlush(&quot;\\n&quot;);} 从 FileRegion 的使用示例可以看出，Netty 使用 FileRegion 实现文件传输的零拷贝。FileRegion 的默认实现类是 DefaultFileRegion，通过 DefaultFileRegion 将文件内容写入到 NioSocketChannel。 那么 FileRegion 是如何实现零拷贝的呢？ 123456789101112131415161718192021222324252627282930313233public class DefaultFileRegion extends AbstractReferenceCounted implements FileRegion { private final File f; // 传输的文件 private final long position; // 文件的起始位置 private final long count; // 传输的字节数 private long transferred; // 已经写入的字节数 private FileChannel file; // 文件对应的 FileChannel @Override public long transferTo(WritableByteChannel target, long position) throws IOException { long count = this.count - position; if (count &lt; 0 || position &lt; 0) { throw new IllegalArgumentException( &quot;position out of range: &quot; + position + &quot; (expected: 0 - &quot; + (this.count - 1) + ')'); } if (count == 0) { return 0L; } if (refCnt() == 0) { throw new IllegalReferenceCountException(0); } open(); long written = file.transferTo(this.position + position, count, target); if (written &gt; 0) { transferred += written; } else if (written == 0) { validate(this, position); } return written; } // 省略其他代码} 从源码可以看出，FileRegion 其实就是对 FileChannel 的包装，并没有什么特殊操作，底层使用的是 JDK NIO 中的 FileChannel#transferTo() 方法实现文件传输，所以 FileRegion 是操作系统级别的零拷贝，对于传输大文件会很有帮助。","link":"/posts/19955.html"},{"title":"Spring：Aop","text":"AOP concepts（AOP术语） Aspect/Advisors（切面） 一个关注点的模块化，这个关注点可能会横切多个对象。在Spring AOP中，切面可以使用基于模式或者基于@Aspect注解的方式来实现。 Join point（连接点）在程序执行期间的一点。在Spring AOP中，连接点总是表示方法执行。 Advice（通知）在切面的某个特定的连接点上执行的动作。许多AOP框架（包括Spring）都是以拦截器做通知模型，并维护一个以连接点为中心的拦截器链。 Pointcut（切入点）查找连接点的条件。通知和一个切入点表达式关联，并在满足这个切入点的连接点上运行。 Introduction（引入）给一个类型声明额外的方法或属性。Spring允许引入新的接口（以及一个对应的实现）到任何被代理的对象。 Target object（目标对象）被一个或者多个切面所通知的对象。也被称做被通知（advised）对象。 既然Spring AOP是通过运行时代理实现的，这个对象永远是一个被代理（proxied）对象。 AOP proxyAOP框架创建的对象，用来实现切面契约（例如通知方法执行等等）。在Spring中，AOP代理可以是JDK动态代理或者CGLIB代理。 Weaving（织入）织入是一个过程，是将切面应用到目标对象从而创建出AOP代理对象的过程，织入可以在编译期、类装载期、运行期进行。 通知类型 Before advice（前置通知）：在某连接点之前执行的通知，但这个通知不能阻止连接点之前的执行流程（除非它抛出一个异常）。 After returning advice（后置通知）：在某连接点正常完成后执行的通知：例如，一个方法没有抛出任何异常，正常返回。 After throwing advice（异常通知）：在方法抛出异常退出时执行的通知。 After (finally) advice（最终通知）：当某连接点退出的时候执行的通知（不论是正常返回还是异常退出）。 Around Advice（环绕通知）：包围一个连接点的通知，如方法调用。这是最强大的一种通知类型。环绕通知可以在方法调用前后完成自定义的行为。它也会选择是否继续执行连接点或直接返回它自己的返回值或抛出异常来结束执行。 Spring AOP Spring AOP使用纯Java实现，它不需要专门的编译过程。Spring AOP不需要控制类加载器层次结构，因此适用于Servlet容器或应用程序服务器。 Spring AOP目前仅支持方法执行连接点。 Spring实现AOP的方法跟其他的框架不同。Spring并不是要提供最完整的AOP实现（尽管Spring AOP有这个能力），相反的，它其实侧重于提供一种AOP实现和Spring IoC容器之间的整合，用于帮助解决在企业级开发中的常见问题。 Spring AOP从来没有打算通过提供一种全面的AOP解决方案来与AspectJ竞争。我们相信无论是基于代理（proxy-based）的框架如Spring AOP或者是成熟的框架如AspectJ都是很有价值的，他们之间应该是互补而不是竞争的关系。 Spring AOP基于XML的应用程序1、Jar包依赖 12345678910111213141516171819202122&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 2、定义切面和需要被拦截的对象 1234567891011121314151617181920212223242526public class Student { private Integer age; private String name; public void setAge(Integer age) { this.age = age; } public Integer getAge() { System.out.println(&quot;Age : &quot; + age ); return age; } public void setName(String name) { this.name = name; } public String getName() { System.out.println(&quot;Name : &quot; + name ); return name; } public void printThrowException(){ System.out.println(&quot;Exception raised&quot;); throw new IllegalArgumentException(); }} 3、实现通知 12345678910111213141516171819public class Logging { public void beforeAdvice(){ System.out.println(&quot;beforeAdvice.&quot;); } public void afterAdvice(){ System.out.println(&quot;afterAdvice&quot;); } public void afterReturningAdvice(Object retVal){ System.out.println(&quot;afterReturningAdvice:&quot; + retVal.toString() ); } public void afterThrowingAdvice(IllegalArgumentException ex){ System.out.println(&quot;afterThrowingAdvice: &quot; + ex.toString()); }} 3、配置XML 12345678910111213141516&lt;aop:config&gt; &lt;aop:aspect id=&quot;log&quot; ref=&quot;logging&quot;&gt; &lt;aop:pointcut id=&quot;all&quot; expression=&quot;execution(* com.codersm.study.spring.aop.*.*(..))&quot;/&gt; &lt;aop:before method=&quot;beforeAdvice&quot; pointcut-ref=&quot;all&quot;/&gt; &lt;aop:after method=&quot;afterAdvice&quot; pointcut-ref=&quot;all&quot;/&gt; &lt;aop:after-returning method=&quot;afterReturningAdvice&quot; pointcut-ref=&quot;all&quot; returning=&quot;retVal&quot;/&gt; &lt;aop:after-throwing method=&quot;afterThrowingAdvice&quot; pointcut-ref=&quot;all&quot; throwing=&quot;ex&quot;/&gt; &lt;/aop:aspect&gt;&lt;/aop:config&gt;&lt;bean id=&quot;student&quot; class=&quot;com.codersm.study.spring.aop.Student&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;zhangsan&quot;/&gt; &lt;property name=&quot;age&quot; value=&quot;21&quot;/&gt;&lt;/bean&gt;&lt;bean id=&quot;logging&quot; class=&quot;com.codersm.study.spring.aop.Logging&quot;/&gt; 4、测试 123ApplicationContext context = new ClassPathXmlApplicationContext(&quot;classpath:spring-aop.xml&quot;); Student student = (Student) context.getBean(&quot;student&quot;); student.getAge(); Spring AOP基于@Aspect的应用程序1、定义切面 123456789101112131415161718192021222324252627282930313233343536373839@Aspect@Componentpublic class LoggingAspect { /** * 单独定义切入点，可复用 */ @Pointcut(&quot;execution(* com.codersm.study.spring.aop.*.*(..))&quot;) public void pointcut() { } @Before(value = &quot;pointcut()&quot;) public void before() { System.out.println(&quot;Before advice&quot;); } @Around(value = &quot;pointcut()&quot;) public void around(ProceedingJoinPoint proceedingJoinPoint) throws Throwable { System.out.println(&quot;Around advice begin&quot;); Object ret = proceedingJoinPoint.proceed(); System.out.println(&quot;Around advice end,execute method result is &quot; + ret); } @After(value = &quot;pointcut()&quot;) public void after() { System.out.println(&quot;After advice&quot;); } @AfterThrowing(value = &quot;pointcut()&quot;, throwing = &quot;ex&quot;) public void afterThrowing(Throwable ex) { System.out.println(&quot;afterThrowing advice exception is &quot; + ex); } @AfterReturning(value = &quot;pointcut()&quot;, returning = &quot;ret&quot;) public void AfterReturning(Object ret) { System.out.println(&quot;AfterReturning advice result is :&quot; + ret); }} 2、配置xml文件开启@Aspect 12&lt;context:component-scan base-package=&quot;com.codersm.study.spring.*&quot;/&gt;&lt;aop:aspectj-autoproxy/&gt; 3、测试 123456789101112131415ApplicationContext applicationContext = null; @Before public void before() { applicationContext = new ClassPathXmlApplicationContext(&quot;classpath:spring-aop-annotation.xml&quot;); } @Test public void testAop() { Student student = (Student) applicationContext.getBean(&quot;student&quot;); student.setName(&quot;hello world&quot;); System.out.println(&quot;---------------------------------&quot;); student.printThrowException(); } 通知类型小结 通知 描述 前置通知 权限控制(少用) 后置通知 少用 环绕通知 权限控制/性能监控/缓存实现/事务管理 异常通知 发生异常后,记录错误日志 最终通知 释放资源 获取通知参数1、任何通知声明JoinPoint作为通知方法第一个参数，JoinPoint提供一些有用的方法。 around advice is required to declare a first parameter of type ProceedingJoinPoint, which is a subclass of JoinPoint. ProceedingJoinPoint is only supported for around advice. 2、传递参数给通知 To make argument values available to the advice body, you can use the binding form of args. 1234@Before(&quot;execution(* com.codersm.study.spring.aop.*.*(..)) &amp;&amp; args(name,..)&quot;)public void before(String name) { System.out.println(&quot;Before advice,name = &quot; + name);} 另外一种定义方式： 12345678@Pointcut(&quot;execution(* com.codersm.study.spring.aop.*.*(..)) &amp;&amp; args(name,..)&quot;)public void pointcut(String name) {}@Before(value = &quot;pointcut(name)&quot;)public void before(String name) { System.out.println(&quot;Before advice,name = &quot; + name);} AOP proxies4.1、 AOP介绍Spring AOP使用JDK动态代理或CGLIB创建目标类的代理对象，如果目标类实现了至少一个接口，则使用JDK动态代理；否则，使用CGLIB代理。如果强制使用CGLIB代理，需要考虑这些问题： final methods cannot be advised, as they cannot be overridden. As of Spring 3.2, it is no longer necessary to add CGLIB to your project classpath, as CGLIB classes are repackaged under org.springframework and included directly in the spring-core JAR. This means that CGLIB-based proxy support ‘just works’ in the same way that JDK dynamic proxies always have. As of Spring 4.0, the constructor of your proxied object will NOT be called twice anymore since the CGLIB proxy instance will be created via Objenesis. Only if your JVM does not allow for constructor bypassing, you might see double invocations and corresponding debug log entries from Spring’s AOP support. 4.2、理解AOP代理any method calls that it may make on itself, such as this.bar() or this.foo(), are going to be invoked against the this reference, and not the proxy. This has important implications. It means that self-invocation is not going to result in the advice associated with a method invocation getting a chance to execute. solution: refactor your code such that the self-invocation does not happen. You can (choke!) totally tie the logic within your class to Spring AOP by doing this 1234567891011public class SimplePojo implements Pojo { public void foo() { // this works, but... gah! ((Pojo) AopContext.currentProxy()).bar(); } public void bar() { // some logic... }} 123456789101112public static void main(String[] args) { ProxyFactory factory = new ProxyFactory(new SimplePojo()); factory.adddInterface(Pojo.class); factory.addAdvice(new RetryAdvice()); factory.setExposeProxy(true); Pojo pojo = (Pojo) factory.getProxy(); // this is a method call on the proxy! pojo.foo(); } AOP源码分析spring.handlers 1http\\://www.springframework.org/schema/aop=org.springframework.aop.config.AopNamespaceHandler 12345678910111213141516171819public class AopNamespaceHandler extends NamespaceHandlerSupport { /** * Register the {@link BeanDefinitionParser BeanDefinitionParsers} for the * '{@code config}', '{@code spring-configured}', '{@code aspectj-autoproxy}' * and '{@code scoped-proxy}' tags. */ @Override public void init() { // In 2.0 XSD as well as in 2.1 XSD. registerBeanDefinitionParser(&quot;config&quot;, new ConfigBeanDefinitionParser()); registerBeanDefinitionParser(&quot;aspectj-autoproxy&quot;, new AspectJAutoProxyBeanDefinitionParser()); registerBeanDefinitionDecorator(&quot;scoped-proxy&quot;, new ScopedProxyBeanDefinitionDecorator()); // Only in 2.0 XSD: moved to context namespace as of 2.1 registerBeanDefinitionParser(&quot;spring-configured&quot;, new SpringConfiguredBeanDefinitionParser()); }} ConfigBeanDefinitionParser.parse( )方法： 123456789101112131415161718192021222324public BeanDefinition parse(Element element, ParserContext parserContext) { CompositeComponentDefinition compositeDef = new CompositeComponentDefinition(element.getTagName(), parserContext.extractSource(element)); parserContext.pushContainingComponent(compositeDef); configureAutoProxyCreator(parserContext, element); List&lt;Element&gt; childElts = DomUtils.getChildElements(element); for (Element elt: childElts) { String localName = parserContext.getDelegate().getLocalName(elt); if (POINTCUT.equals(localName)) { parsePointcut(elt, parserContext); } else if (ADVISOR.equals(localName)) { parseAdvisor(elt, parserContext); } else if (ASPECT.equals(localName)) { parseAspect(elt, parserContext); } } parserContext.popAndRegisterContainingComponent(); return null; } **configureAutoProxyCreator(parserContext, element)** 12345678910111213141516private void configureAutoProxyCreator(ParserContext parserContext, Element element) { AopNamespaceUtils.registerAspectJAutoProxyCreatorIfNecessary(parserContext, element);}public static void registerAspectJAutoProxyCreatorIfNecessary( ParserContext parserContext, Element sourceElement) { BeanDefinition beanDefinition = AopConfigUtils.registerAspectJAutoProxyCreatorIfNecessary( parserContext.getRegistry(), parserContext.extractSource(sourceElement)); useClassProxyingIfNecessary(parserContext.getRegistry(), sourceElement); registerComponentIfNecessary(beanDefinition, parserContext);}public static BeanDefinition registerAspectJAutoProxyCreatorIfNecessary(BeanDefinitionRegistry registry, Object source) { return registerOrEscalateApcAsRequired(AspectJAwareAdvisorAutoProxyCreator.class, registry, source);} Spring对XML文件解析 12345678910111213141516171819202122232425/** * Parse the elements at the root level in the document: * &quot;import&quot;, &quot;alias&quot;, &quot;bean&quot;. * @param root the DOM root element of the document */ protected void parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate) { if (delegate.isDefaultNamespace(root)) { NodeList nl = root.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) { Node node = nl.item(i); if (node instanceof Element) { Element ele = (Element) node; if (delegate.isDefaultNamespace(ele)) { parseDefaultElement(ele, delegate); } else { delegate.parseCustomElement(ele); } } } } else { delegate.parseCustomElement(root); } } AspectJAwareAdvisorAutoProxyCreator 123456public interface BeanPostProcessor { Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException; Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException;} BeanPostProcessor接口定义回调方法，允许修改新的实例化Bean，例如检查标记接口或用代理进行包装。 postProcessBeforeInitialization 在任何bean初始化回调（如InitializingBean的afterPropertiesSet或自定义init方法）之前，将此BeanPostProcessor应用于给定的新bean实例。 postProcessAfterInitialization 在任何bean初始化回调之后，将此BeanPostProcessor应用于给定的新Bean实例（如InitializingBean的afterPropertiesSet或自定义init方法）。 ApplicationContext 会自动检测由 BeanPostProcessor 接口的实现定义的 bean，注册这些 bean 为后置处理器，然后通过在容器中创建 bean，在适当的时候调用它。 AspectJAwareAdvisorAutoProxyCreator这两个方法的实现： 123456789101112131415@Overridepublic Object postProcessBeforeInitialization(Object bean, String beanName) { return bean;}@Overridepublic Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException { if (bean != null) { Object cacheKey = getCacheKey(bean.getClass(), beanName); if (!this.earlyProxyReferences.contains(cacheKey)) { return wrapIfNecessary(bean, beanName, cacheKey); } } return bean;} 继续跟踪源码，发现了createProxy方法： 12345678910111213141516171819202122232425protected Object wrapIfNecessary(Object bean, String beanName, Object cacheKey) { if (beanName != null &amp;&amp; this.targetSourcedBeans.contains(beanName)) { return bean; } if (Boolean.FALSE.equals(this.advisedBeans.get(cacheKey))) { return bean; } if (isInfrastructureClass(bean.getClass()) || shouldSkip(bean.getClass(), beanName)) { this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean; } // Create proxy if we have advice. Object[] specificInterceptors = getAdvicesAndAdvisorsForBean(bean.getClass(), beanName, null); if (specificInterceptors != DO_NOT_PROXY) { this.advisedBeans.put(cacheKey, Boolean.TRUE); Object proxy = createProxy( bean.getClass(), beanName, specificInterceptors, new SingletonTargetSource(bean)); this.proxyTypes.put(cacheKey, proxy.getClass()); return proxy; } this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean;} createProxy方法: 12345678910111213141516171819202122232425262728293031323334protected Object createProxy( Class&lt;?&gt; beanClass, String beanName, Object[] specificInterceptors, TargetSource targetSource) { if (this.beanFactory instanceof ConfigurableListableBeanFactory) { AutoProxyUtils.exposeTargetClass((ConfigurableListableBeanFactory) this.beanFactory, beanName, beanClass); } ProxyFactory proxyFactory = new ProxyFactory(); proxyFactory.copyFrom(this); if (!proxyFactory.isProxyTargetClass()) { if (shouldProxyTargetClass(beanClass, beanName)) { proxyFactory.setProxyTargetClass(true); } else { evaluateProxyInterfaces(beanClass, proxyFactory); } } Advisor[] advisors = buildAdvisors(beanName, specificInterceptors); for (Advisor advisor : advisors) { proxyFactory.addAdvisor(advisor); } proxyFactory.setTargetSource(targetSource); customizeProxyFactory(proxyFactory); proxyFactory.setFrozen(this.freezeProxy); if (advisorsPreFiltered()) { proxyFactory.setPreFiltered(true); } return proxyFactory.getProxy(getProxyClassLoader()); } 创建AopProxy代理对象，具体流程： 12345678910111213141516public AopProxy createAopProxy(AdvisedSupport config) throws AopConfigException { if (config.isOptimize() || config.isProxyTargetClass() || hasNoUserSuppliedProxyInterfaces(config)) { Class&lt;?&gt; targetClass = config.getTargetClass(); if (targetClass == null) { throw new AopConfigException(&quot;TargetSource cannot determine target class: &quot; + &quot;Either an interface or a target is required for proxy creation.&quot;); } if (targetClass.isInterface() || Proxy.isProxyClass(targetClass)) { return new JdkDynamicAopProxy(config); } return new ObjenesisCglibAopProxy(config); } else { return new JdkDynamicAopProxy(config); } } ObjenesisCglibAopProxy继承CglibAopProxy。方法调用原理可以查看CglibAopProxy和JdkDynamicAopProxy。","link":"/posts/18571.html"},{"title":"微服务事务一致性解决方案","text":"微服务事务一致性解决方案2PC2PC即两阶段提交协议，是将整个事务流程分为两个阶段，准备阶段（Prepare phase）、提交阶段（commit phase）。 整体流程阶段一：准备阶段（投票阶段） 协调者节点向所有参与者节点询问是否可以执行提交操作，并开始等待各参与者节点的响应。 参与者节点执行事务操作，并将Undo和Redo信息记入事务日志中。 各参与者响应协调者发起的询问。如果参与者节点的事务操作实际执行成功，则它返回一个”同意”消息；如果参与者节点的事务操作实际执行失败，则它返回一个”中止”消息。 阶段二：提交阶段（完成）成功当协调者节点从所有参与者节点获得的响应消息都为”同意”时： 协调者节点向所有参与者节点发出”正式提交”的请求。 参与者节点正式完成操作，并释放在整个事务期间内占用的资源。 参与者节点向协调者节点发送”完成”消息。 协调者节点收到所有参与者节点反馈的”完成”消息后，完成事务。 失败如果任一参与者节点在第一阶段返回的响应消息为”终止”，或者协调者节点在第一阶段的询问超时之前无法获取所有参与者节点的响应消息时： 协调者节点向所有参与者节点发出”回滚操作”的请求。 参与者节点利用之前写入的Undo信息执行回滚，并释放在整个事务期间内占用的资源。 参与者节点向协调者节点发送”回滚完成”消息。 协调者节点收到所有参与者节点反馈的”回滚完成”消息后，取消事务。 缺点同步阻塞在执行过程中，所有参与该事务操作的逻辑都处理于阻塞状态。 太过保守协调者节点指示参与者节点进行提交等操作时，如有参与者节点出现了崩溃等情况而导致协调者始终无法获取所有参与者的响应信息，这时协调者将只能依赖协调者自身的超时机制来生效。但往往超时机制生效时，协调者都会指示参与者进行回滚操作。这样的策略显得比较保守。 实现XA协议 TCC模式数据库以及分布式的两阶段提交都提供了ACID 的保证。 由于隔离性互斥的要求，在事务执行过程中，所有的资源都是被锁定的，这种情况只适合执行时间确定的短事务。 后续大家开始通过业务逻辑将互斥锁操作从资源层面上移到业务层面，这并不是完全放弃了 ACID，而是通过放宽一致性要求，借助本地事务来实现最终分布式事务一致性的同时也保证系统的吞吐。 概述TCC 名字的由来是其中包含了 try, confirm, cancel 三个操作。 Try：资源的检测和预留； Confirm：执行的业务操作提交；要求 Try 成功 Confirm 一定要能成功； Cancel：预留资源释放； 服务要求TCC设计 - 业务模型分2阶段设计用户接入TCC，最重要的是考虑如何将自己的业务模型拆成两阶段来实现。 以“扣钱”场景为例，在接入TCC前，对 A 账户的扣钱，只需一条更新账户余额的SQL便能完成；但是在接入TCC之后，用户就需要考虑如何将原来一步就能完成的扣钱操作，拆成两阶段，实现成三个方法，并且保证一阶段Try成功，二阶段Confirm一定能成功。 如上图所示，Try 方法作为一阶段准备方法，需要做资源的检查和预留。在扣钱场景下，Try 要做的事情是就是检查账户余额是否充足，预留转账资金，预留的方式就是冻结 A 账户的 转账资金。Try 方法执行之后，账号 A 余额虽然还是 100，但是其中 30 元已经被冻结了，不能被其他事务使用。 二阶段 Confirm 方法执行真正的扣钱操作。Confirm 会使用 Try 阶段冻结的资金，执行账号扣款。Confirm 方法执行之后，账号 A 在一阶段中冻结的 30 元已经被扣除，账号 A 余额变成 70 元 。 如果二阶段是回滚的话，就需要在 Cancel 方法内释放一阶段 Try 冻结的 30 元，使账号 A 的回到初始状态，100 元全部可用。 用户接入 TCC 模式，最重要的事情就是考虑如何将业务模型拆成 2 阶段，实现成 TCC 的 3 个方法，并且保证 Try 成功 Confirm 一定能成功。相对于 AT 模式，TCC 模式对业务代码有一定的侵入性，但是TCC 模式无AT模式的全局行锁，TCC性能会比 AT模式高很多。 TCC设计 - 允许空回滚 Cancel接口设计时需要允许空回滚。在 Try 接口因为丢包时没有收到，事务管理器会触发回滚，这时会触发Cancel接口，这时Cancel执行时发现没有对应的事务 xid 或主键时，需要返回回滚成功。让事务服务管理器认为已回滚，否则会不断重试，而Cancel又没有对应的业务数据可以进行回滚。 TCC设计 - 防悬挂控制 悬挂的意思是：Cancel比Try 接口先执行，出现的原因是 Try 由于网络拥堵而超时，事务管理器生成回滚，触发 Cancel 接口，而最终又收到了 Try 接口调用，但是 Cancel 比 Try 先到。按照前面允许空回滚的逻辑，回滚会返回成功，事务管理器认为事务已回滚成功，则此时的 Try 接口不应该执行，否则会产生数据不一致，所以我们在 Cancel 空回滚返回成功之前先记录该条事务 xid 或业务主键，标识这条记录已经回滚过，Try接口先检查这条事务xid或业务主键如果已经标记为回滚成功过，则不执行 Try 的业务操作。 TCC设计 - 幂等控制 幂等性的意思是：对同一个系统，使用同样的条件，一次请求和重复的多次请求对系统资源的影响是一致的。因为网络抖动或拥堵可能会超时，事务管理器会对资源进行重试操作，所以很可能一个业务操作会被重复调用，为了不因为重复调用而多次占用资源，需要对服务设计时进行幂等控制，通常我们可以用事务 xid 或业务主键判重来控制。 Saga模式概述Saga 是一种补偿协议，在 Saga 模式下，分布式事务内有多个参与者，每一个参与者都是一个冲正补偿服务，需要用户根据业务场景实现其正向操作和逆向回滚操作。 分布式事务执行过程中，依次执行各参与者的正向操作，如果所有正向操作均执行成功，那么分布式事务提交。如果任何一个正向操作执行失败，那么分布式事务会退回去执行前面各参与者的逆向回滚操作，回滚已提交的参与者，使分布式事务回到初始状态。 Saga 模式下分布式事务通常是由事件驱动的，各个参与者之间是异步执行的，Saga 模式是一种长事务解决方案。 服务要求与TCC实践经验相同的是，Saga 模式中，每个事务参与者的冲正、逆向操作，需要支持： 空补偿：逆向操作早于正向操作时； 防悬挂控制：空补偿后要拒绝正向操作 幂等 适应场景 Saga模式适用于业务流程长且需要保证事务最终一致性的业务系统，Saga模式一阶段就会提交本地事务，无锁、长流程情况下可以保证性能。 事务参与者可能是其它公司的服务或者是遗留系统的服务，无法进行改造和提供TCC要求的接口，可以使用Saga 模式。 优缺点优势 一阶段提交本地数据库事务，无锁，高性能； 参与者可以采用事务驱动异步执行，高吞吐； 补偿服务即正向服务的“反向”，易于理解，易于实现； 缺点Saga 模式由于一阶段已经提交本地数据库事务，且没有进行“预留”动作，所以不能保证隔离性。 事件驱动架构概述和TCC一样，在事件驱动的架构中，长活事务涉及的每个服务都需要支持额外的待处理状态。接收到事务请求的服务会在其数据库中插入一条新的记录， 将该记录状态设为待处理并发送一个新的事件给事务序列中的下一个服务。 因为在插入记录后服务可能崩溃，我们无法确定是否新事件已发送，所以每个服务还需要额外的事件表来跟踪当前长活事务处于哪一步。 一旦长活事务中的最后一个服务完成其子事务，它将通知它在事务中的前一个服务。接收到完成事件的服务将其在数据库中的记录状态设为完成。 如果仔细比较，事件驱动的架构就像非集中式的基于事件的TCC实现。如果去掉待处理状态而直接把服务记录设为最终状态，这个架构就像非集中式的基于事件的saga实现。 去中心化能达到服务自治，但也造成了服务之间更紧密的的耦合。假设新的业务需求在服务B和C之间的增加了新的流程D。在事件驱动架构下，服务B和C必须改动代码以适应新的流程D。 Saga则正好相反，所有这些耦合都在saga系统中，当在长活事务中添加新流程时，现有服务不需要任何改动。 本地消息表本地消息表这个方案最初是 ebay 架构师 Dan Pritchett 在 2008 年发表给 ACM 的文章。该方案中会有消息生产者与消费者两个角色，假设系统 A 是消息生产者，系统 B 是消息消费者，其大致流程如下： 当系统 A 被其他系统调用发生数据库表更操作，首先会更新数据库的业务表，其次会往相同数据库的消息表中插入一条数据，两个操作发生在同一个事务中 系统 A 的脚本定期轮询本地消息往 mq 中写入一条消息，如果消息发送失败会进行重试 系统 B 消费 mq 中的消息，并处理业务逻辑。如果本地事务处理失败，会在继续消费 mq 中的消息进行重试，如果业务上的失败，可以通知系统 A 进行回滚操作 本地消息表实现的条件： 消费者与生成者的接口都要支持幂等 生产者需要额外的创建消息表 需要提供补偿逻辑，如果消费者业务失败，需要生产者支持回滚操作 容错机制： 步骤 1 失败时，事务直接回滚 步骤 2、3 写 mq 与消费 mq 失败会进行重试 步骤 3 业务失败系统 B 向系统 A 发起事务回滚操作 事务消息 A 系统先向 mq 发送一条 prepare 消息，如果 prepare 消息发送失败，则直接取消操作 如果消息发送成功，则执行本地事务 如果本地事务执行成功，则想 mq 发送一条 confirm 消息，如果发送失败，则发送回滚消息 B 系统定期消费 mq 中的 confirm 消息，执行本地事务，并发送 ack 消息。如果 B 系统中的本地事务失败，会一直不断重试，如果是业务失败，会向 A 系统发起回滚请求 mq 会定期轮询所有 prepared 消息调用系统 A 提供的接口查询消息的处理情况，如果该 prepare 消息本地事务处理成功，则重新发送 confirm 消息，否则直接回滚该消息 该方案与本地消息最大的不同是去掉了本地消息表，其次本地消息表依赖消息表重试写入 mq 这一步由本方案中的轮询 prepare 消息状态来重试或者回滚该消息替代。其实现条件与余容错方案基本一致。目前市面上实现该方案的只有阿里的 RocketMq。 Seata术语Seata中有3个基本组件： Transaction Coordinator(TC): Maintain status of global and branch transactions, drive the global commit or rollback. Transaction Manager(TM): Define the scope of global transaction: begin a global transaction, commit or rollback a global transaction. Resource Manager(RM): Manage resources that branch transactions working on, talk to TC for registering branch transactions and reporting status of branch transactions, and drive the branch transaction commit or rollback. 整体流程 TM asks TC to begin a new global transaction. TC generates an XID representing the global transaction. XID is propagated through microservices’ invoke chain. RM registers local transaction as a branch of the corresponding global transaction of XID to TC. TM asks TC for committing or rollbacking the corresponding global transaction of XID. TC drives all branch transactions under the corresponding global transaction of XID to finish branch committing or rollbacking. AT模式AT模式是一种无侵入的分布式事务解决方案。在AT模式下，用户只需关注自己的“业务 SQL”，用户的 “业务 SQL” 作为一阶段，Seata框架会自动生成事务的二阶段提交和回滚操作。 整体机制两阶段提交协议的演变： 阶段一 业务数据和回滚日志记录在同一个本地事务中提交，释放本地锁和连接资源。 阶段二： 提交异步化，非常快速地完成。 回滚通过一阶段的回滚日志进行反向补偿。 阶段一：详细流程 Seata 会拦截“业务 SQL”，首先解析 SQL 语义，找到“业务 SQL”要更新的业务数据，在业务数据被更新前，将其保存成“before image”，然后执行“业务 SQL”更新业务数据，在业务数据更新之后，再将其保存成“after image”，最后生成行锁。以上操作全部在一个数据库事务内完成，这样保证了一阶段操作的原子性。 阶段二：提交“业务SQL”在阶段一已经提交至数据库， 所以Seata 框架只需将阶段一保存的快照数据和行锁删掉，完成数据清理即可。 阶段二：回滚Seata需要回滚阶段一已经执行的“业务 SQL”，还原业务数据。回滚方式便是用“before image”还原业务数据；但在还原前要首先要校验脏写，对比“数据库当前业务数据”和 “after image”，如果两份数据完全一致就说明没有脏写，可以还原业务数据，如果不一致就说明有脏写，出现脏写就需要转人工处理。 适应场景 分布式事务的业务逻辑中仅仅纯数据库操作，不包含其他中间件事务逻辑。 优缺点优点改动及代码侵入最小。由Seata来负责Commit和Rollback的自动化触发或回滚操作。 缺点 如果事务中包含缓存存储或发送消息等不适合。 为了保证镜像sql的可靠性，需要用户对sql尽量做简化， 建议做法：将多条SQL语句分解为多个事务中的原子步骤（对应SeataAt模式的分支Branch概念），如果单条SQL语句跨表，也分解成为多个事务中的原子步骤（尽量降低Seata存储镜前SQL结果时的风险）。 多次对DB的操作，以及全局行锁的存在对并发处理性能有影响。 微服务事务一致性建议与两阶段提交相比，TCC 位于业务服务层， 没有单独的准备阶段，Try 操作可以灵活选择业务资源锁的粒度。TCC 是通过最终一致性来解决系统性能问题的这个设计，对我们设计抉择有很大的启发。 有些时候 系统的技术问题是可以通过业务建模的方式来解决的。 有关领域建模，这里给大家推荐两本书，一个是《领域驱动设计》，还有一个是《实现领域驱动设计》。 微服务架构是一个在限定界限上下文内的松耦合的服务架构。微服务事务一致性的建议是什么呢？就是内刚外柔。在限定上下文内容范围内可以借助数据库提供事务一致性来做强一致。在限定上下文之间依靠最终一致性方案来解决服务间协同问题。 参考资料[]: https://mp.weixin.qq.com/s?__biz=MzI4MTY5NTk4Ng==&amp;mid=2247489026&amp;idx=1&amp;sn=67184a64653164d1c48255e0e87373c8&amp;chksm=eba4159fdcd39c895524eb8430237dd3c6fa3d47fb3dd15766a67588aaccfbaee4e4a17c5398&amp;scene=27#wechat_redirect “Saga分布式事务解决方案与实践”[]: https://www.jianshu.com/p/f2caa8737b7b “分布式事务 Seata(二) 理解什么是AT、TCC、Saga”[]: https://www.cnblogs.com/anhaogoon/p/13033611.html “Seata解决方案整体介绍” []: https://xiaomi-info.github.io/2020/01/02/distributed-transaction/ “分布式事务，这一篇就够了”","link":"/posts/10608.html"},{"title":"分布式缓存：7大缓存经典问题","text":"缓存失效 问题描述 服务系统查数据，首先会查缓存，如果缓存数据不存在，就进一步查 DB，最后查到数据后回种到缓存并返回。缓存的性能比 DB 高 50~100 倍以上，所以我们希望数据查询尽可能命中缓存，这样系统负荷最小，性能最佳。缓存里的数据存储基本上都是以 key 为索引进行存储和获取的。 业务访问时，如果大量的 key 同时过期，很多缓存数据访问都会 miss，进而穿透到 DB，DB 的压力就会明显上升，由于 DB 的性能较差，只在缓存的 1%~2% 以下，这样请求的慢查率会明显上升。这就是缓存失效的问题。 原因分析 导致缓存失效，特别是很多 key 一起失效的原因，跟我们日常写缓存的过期时间息息相关。 在写缓存时，我们一般会根据业务的访问特点，给每种业务数据预置一个过期时间，在写缓存时把这个过期时间带上，让缓存数据在这个固定的过期时间后被淘汰。一般情况下，因为缓存数据是逐步写入的，所以也是逐步过期被淘汰的。 但在某些场景，一大批数据会被系统主动或被动从 DB 批量加载，然后写入缓存。这些数据写入缓存时，由于使用相同的过期时间，在经历这个过期时间之后，这批数据就会一起到期，从而被缓存淘汰。此时，对这批数据的所有请求，都会出现缓存失效，从而都穿透到 DB，DB 由于查询量太大，就很容易压力大增，请求变慢。 业务场景 很多业务场景，稍不注意，就出现大量的缓存失效，进而导致系统 DB 压力大、请求变慢的情况。比如同一批火车票、飞机票，当可以售卖时，系统会一次性加载到缓存，如果缓存写入时，过期时间按照预先设置的过期值，那过期时间到期后，系统就会因缓存失效出现变慢的问题。类似的业务场景还有很多，比如微博业务，会有后台离线系统，持续计算热门微博，每当计算结束，会将这批热门微博批量写入对应的缓存。还比如，很多业务，在部署新 IDC 或新业务上线时，会进行缓存预热，也会一次性加载大批热数据。 解决方案 对于批量 key 缓存失效的问题，原因既然是预置的固定过期时间，那解决方案也从这里入手。设计缓存的过期时间时，使用公式：过期时间=baes 时间+随机时间。即相同业务数据写缓存时，在基础过期时间之上，再加一个随机的过期时间，让数据在未来一段时间内慢慢过期，避免瞬时全部过期，对 DB 造成过大压力，如下图所示。 缓存穿透 问题描述 缓存穿透是一个很有意思的问题。因为缓存穿透发生的概率很低，所以一般很难被发现。但是，一旦你发现了，而且量还不小，你可能立即就会经历一个忙碌的夜晚。因为对于正常访问，访问的数据即便不在缓存，也可以通过 DB 加载回种到缓存。而缓存穿透，则意味着有特殊访客在查询一个不存在的 key，导致每次查询都会穿透到 DB，如果这个特殊访客再控制一批肉鸡机器，持续访问你系统里不存在的 key，就会对 DB 产生很大的压力，从而影响正常服务。 原因分析 缓存穿透存在的原因，就是因为我们在系统设计时，更多考虑的是正常访问路径，对特殊访问路径、异常访问路径考虑相对欠缺。 缓存访问设计的正常路径，是先访问 cache，cache miss 后查 DB，DB 查询到结果后，回种缓存返回。这对于正常的 key 访问是没有问题的，但是如果用户访问的是一个不存在的 key，查 DB 返回空（即一个 NULL），那就不会把这个空写回cache。那以后不管查询多少次这个不存在的 key，都会 cache miss，都会查询 DB。整个系统就会退化成一个“前端+DB“的系统，由于 DB 的吞吐只在 cache 的 1%~2% 以下，如果有特殊访客，大量访问这些不存在的 key，就会导致系统的性能严重退化，影响正常用户的访问。 业务场景 缓存穿透的业务场景很多，比如通过不存在的UID访问用户，通过不存在的车次 ID 查看购票信息。用户输入错误，偶尔几个这种请求问题不大，但如果是大量这种请求，就会对系统影响非常大。 解决方案 第一种方案就是，查询这些不存在的数据时，第一次查 DB，虽然没查到结果返回 NULL，仍然记录这个 key 到缓存，只是这个 key 对应的 value 是一个特殊设置的值。 第二种方案是，构建一个 BloomFilter 缓存过滤器，记录全量数据，这样访问数据时，可以直接通过 BloomFilter 判断这个 key 是否存在，如果不存在直接返回即可，根本无需查缓存和 DB。 不过这两种方案在设计时仍然有一些要注意的坑。 对于方案一，如果特殊访客持续访问大量的不存在的 key，这些 key 即便只存一个简单的默认值，也会占用大量的缓存空间，导致正常 key 的命中率下降。所以进一步的改进措施是，对这些不存在的 key 只存较短的时间，让它们尽快过期；或者将这些不存在的 key 存在一个独立的公共缓存，从缓存查找时，先查正常的缓存组件，如果 miss，则查一下公共的非法 key 的缓存，如果后者命中，直接返回，否则穿透 DB，如果查出来是空，则回种到非法 key 缓存，否则回种到正常缓存。 对于方案二，BloomFilter 要缓存全量的 key，这就要求全量的 key 数量不大，10亿 条数据以内最佳，因为 10亿 条数据大概要占用 1.2GB 的内存。也可以用 BloomFilter 缓存非法 key，每次发现一个 key 是不存在的非法 key，就记录到 BloomFilter 中，这种记录方案，会导致 BloomFilter 存储的 key 持续高速增长，为了避免记录 key 太多而导致误判率增大，需要定期清零处理。 缓存雪崩 问题描述 系统运行过程中，缓存雪崩是一个非常严重的问题。缓存雪崩是指部分缓存节点不可用，导致整个缓存体系甚至甚至服务系统不可用的情况。缓存雪崩按照缓存是否rehash（即是否漂移）分两种情况： 缓存不支持 rehash 导致的系统雪崩不可用 缓存支持 rehash 导致的缓存雪崩不可用 原因分析 在上述两种情况中，缓存不进行 rehash 时产生的雪崩，一般是由于较多缓存节点不可用，请求穿透导致 DB 也过载不可用，最终整个系统雪崩不可用的。而缓存支持 rehash 时产生的雪崩，则大多跟流量洪峰有关，流量洪峰到达，引发部分缓存节点过载 Crash，然后因 rehash 扩散到其他缓存节点，最终整个缓存体系异常。 第一种情况比较容易理解，缓存节点不支持 rehash，较多缓存节点不可用时，大量 Cache 访问会失败，根据缓存读写模型，这些请求会进一步访问 DB，而且 DB 可承载的访问量要远比缓存小的多，请求量过大，就很容易造成 DB 过载，大量慢查询，最终阻塞甚至 Crash，从而导致服务异常。 第二种情况是怎么回事呢？这是因为缓存分布设计时，很多同学会选择一致性 Hash 分布方式，同时在部分节点异常时，采用 rehash 策略，即把异常节点请求平均分散到其他缓存节点。在一般情况下，一致性 Hash 分布+rehash 策略可以很好得运行，但在较大的流量洪峰到临之时，如果大流量 key 比较集中，正好在某 1～2 个缓存节点，很容易将这些缓存节点的内存、网卡过载，缓存节点异常 Crash，然后这些异常节点下线，这些大流量 key 请求又被 rehash 到其他缓存节点，进而导致其他缓存节点也被过载 Crash，缓存异常持续扩散，最终导致整个缓存体系异常，无法对外提供服务。 业务场景 s缓存雪崩的业务场景并不少见，微博、Twitter 等系统在运行的最初若干年都遇到过很多次。比如，微博最初很多业务缓存采用一致性 Hash+rehash 策略，在突发洪水流量来临时，部分缓存节点过载 Crash 甚至宕机，然后这些异常节点的请求转到其他缓存节点，又导致其他缓存节点过载异常，最终整个缓存池过载。另外，机架断电，导致业务缓存多个节点宕机，大量请求直接打到 DB，也导致 DB 过载而阻塞，整个系统异常。最后缓存机器复电后，DB 重启，数据逐步加热后，系统才逐步恢复正常。 解决方案 预防缓存雪崩，这里给出 3 个解决方案。 方案一，对业务 DB 的访问增加读写开关，当发现 DB 请求变慢、阻塞，慢请求超过阀值时，就会关闭读开关，部分或所有读 DB 的请求进行 failfast 立即返回，待 DB 恢复后再打开读开关，如下图。 方案二，对缓存增加多个副本，缓存异常或请求 miss 后，再读取其他缓存副本，而且多个缓存副本尽量部署在不同机架，从而确保在任何情况下，缓存系统都会正常对外提供服务。 方案三，对缓存体系进行实时监控，当请求访问的慢速比超过阀值时，及时报警，通过机器替换、服务替换进行及时恢复；也可以通过各种自动故障转移策略，自动关闭异常接口、停止边缘服务、停止部分非核心功能措施，确保在极端场景下，核心功能的正常运行。 数据不一致 问题描述 同一份数据，可能会同时存在 DB 和缓存之中。那就有可能发生，DB 和缓存的数据不一致。如果缓存有多个副本，多个缓存副本里的数据也可能会发生不一致现象。 原因分析 不一致的问题大多跟缓存更新异常有关。比如更新 DB 后，写缓存失败，从而导致缓存中存的是老数据。另外，如果系统采用一致性 Hash 分布，同时采用 rehash 自动漂移策略，在节点多次上下线之后，也会产生脏数据。缓存有多个副本时，更新某个副本失败，也会导致这个副本的数据是老数据。 业务场景 导致数据不一致的场景也不少。在缓存机器的带宽被打满，或者机房网络出现波动时，缓存更新失败，新数据没有写入缓存，就会导致缓存和 DB 的数据不一致。缓存 rehash 时，某个缓存机器反复异常，多次上下线，更新请求多次 rehash。这样，一份数据存在多个节点，且每次 rehash 只更新某个节点，导致一些缓存节点产生脏数据。 解决方案 要尽量保证数据的一致性。这里也给出了 3 个方案，可以根据实际情况进行选择。 第一个方案，cache 更新失败后，可以进行重试，如果重试失败，则将s失败的 key 写入队列机服务，待缓存访问恢复后，将这些 key 从缓存删除。这些 key 在再次被查询时，重新从 DB 加载，从而保证数据的一致性。 第二个方案，缓存时间适当调短，让缓存数据及早过期后，然后从 DB 重新加载，确保数据的最终一致性。 第三个方案，不采用 rehash 漂移策略，而采用缓存分层策略，尽量避免脏数据产生。 数据并发竞争 问题描述 互联网系统，线上流量较大，缓存访问中很容易出现数据并发竞争的现象。数据并发竞争，是指在高并发访问场景，一旦缓存访问没有找到数据，大量请求就会并发查询 DB，导致 DB 压力大增的现象。 数据并发竞争，主要是由于多个进程/线程中，有大量并发请求获取相同的数据，而这个数据 key 因为正好过期、被剔除等各种原因在缓存中不存在，这些进程/线程之间没有任何协调，然后一起并发查询 DB，请求那个相同的 key，最终导致 DB 压力大增，如下图。 ​ 业务场景 数据并发竞争在大流量系统也比较常见，比如车票系统，如果某个火车车次缓存信息过期，但仍然有大量用户在查询该车次信息。又比如微博系统中，如果某条微博正好被缓存淘汰，但这条微博仍然有大量的转发、评论、赞。上述情况都会造成该车次信息、该条微博存在并发竞争读取的问题。 解决方案 要解决并发竞争，有 2 种方案。 方案一是使用全局锁。如下图所示，即当缓存请求 miss 后，先尝试加全局锁，只有加全局锁成功的线程，才可以到 DB 去加载数据。其他进程/线程在读取缓存数据 miss 时，如果发现这个 key 有全局锁，就进行等待，待之前的线程将数据从 DB 回种到缓存后，再从缓存获取。 方案二是，对缓存数据保持多个备份，即便其中一个备份中的数据过期或被剔除了，还可以访问其他备份，从而减少数据并发竞争的情况，如下图。 ​ Hot key 问题描述 对于大多数互联网系统，数据是分冷热的。比如最近的新闻、新发表的微博被访问的频率最高，而比较久远的之前的新闻、微博被访问的频率就会小很多。而在突发事件发生时，大量用户同时去访问这个突发热点信息，访问这个 Hot key，这个突发热点信息所在的缓存节点就很容易出现过载和卡顿现象，甚至会被 Crash。 原因分析 Hot key 引发缓存系统异常，主要是因为突发热门事件发生时，超大量的请求访问热点事件对应的 key，比如微博中数十万、数百万的用户同时去吃一个新瓜。数十万的访问请求同一个 key，流量集中打在一个缓存节点机器，这个缓存机器很容易被打到物理网卡、带宽、CPU 的极限，从而导致缓存访问变慢、卡顿。 业务场景 引发 Hot key 的业务场景很多，比如明星结婚、离婚、出轨这种特殊突发事件，比如奥运、春节这些重大活动或节日，还比如秒杀、双12、618 等线上促销活动，都很容易出现 Hot key 的情况。 解决方案 要解决这种极热 key 的问题，首先要找出这些 Hot key 来。对于重要节假日、线上促销活动、集中推送这些提前已知的事情，可以提前评估出可能的热 key 来。而对于突发事件，无法提前评估，可以通过 Spark，对应流任务进行实时分析，及时发现新发布的热点 key。而对于之前已发出的事情，逐步发酵成为热 key 的，则可以通过 Hadoop 对批处理任务离线计算，找出最近历史数据中的高频热 key。 找到热 key 后，就有很多解决办法了。首先可以将这些热 key 进行分散处理，比如一个热 key 名字叫 hotkey，可以被分散为 hotkey#1、hotkey#2、hotkey#3，……hotkey#n，这 n 个 key 分散存在多个缓存节点，然后 client 端请求时，随机访问其中某个后缀的 hotkey，这样就可以把热 key 的请求打散，避免一个缓存节点过载。 其次，也可以 key 的名字不变，对缓存提前进行多副本+多级结合的缓存架构设计。 再次，如果热 key 较多，还可以通过监控体系对缓存的 SLA 实时监控，通过快速扩容来减少热 key 的冲击。 最后，业务端还可以使用本地缓存，将这些热 key 记录在本地缓存，来减少对远程缓存的冲击。 Big key 问题描述 大 key，是指在缓存访问时，部分 Key 的 Value 过大，读写、加载易超时的现象。 原因分析 如果业务中这种大 key 很多，而这种 key 被大量访问，缓存组件的网卡、带宽很容易被打满，也会导致较多的大 key 慢查询。另外，如果大 key 缓存的字段较多，每个字段的变更都会引发对这个缓存数据的变更，同时这些 key 也会被频繁地读取，读写相互影响，也会导致慢查现象。最后，大 key 一旦被缓存淘汰，DB 加载可能需要花费很多时间，这也会导致大 key 查询慢的问题。 业务场景 大 key 的业务场景也比较常见。比如互联网系统中需要保存用户最新 1万 个粉丝的业务，比如一个用户个人信息缓存，包括基本资料、关系图谱计数、发 feed 统计等。微博的 feed 内容缓存也很容易出现，一般用户微博在 140 字以内，但很多用户也会发表 1千 字甚至更长的微博内容，这些长微博也就成了大 key。 解决方案 第一种方案，如果数据存在 Mc 中，可以设计一个缓存阀值，当 value 的长度超过阀值，则对内容启用压缩，让 KV 尽量保持小的 size，其次评估大 key 所占的比例，在 Mc 启动之初，就立即预写足够数据的大 key，让 Mc 预先分配足够多的 trunk size 较大的 slab。确保后面系统运行时，大 key 有足够的空间来进行缓存。 第二种方案，如果数据存在 Redis 中，比如业务数据存 set 格式，大 key 对应的 set 结构有几千几万个元素，这种写入 Redis 时会消耗很长的时间，导致 Redis 卡顿。此时，可以扩展新的数据结构，同时让 client 在这些大 key 写缓存之前，进行序列化构建，然后通过 restore 一次性写入。 第三种方案，将大 key 分拆为多个 key，尽量减少大 key 的存在。同时由于大 key 一旦穿透到 DB，加载耗时很大，所以可以对这些大 key 进行特殊照顾，比如设置较长的过期时间，比如缓存内部在淘汰 key 时，同等条件下，尽量不淘汰这些大 key。","link":"/posts/9261.html"},{"title":"设计模式：单例模式","text":"一个类只允许创建一个对象（或者实例），那这个类就是一个单例类，这种设计模式就叫作单例设计模式，简称单例模式。 为什么我们需要单例这种设计模式？它能解决哪些问题？实战案例一：处理资源访问冲突在这个例子中，自定义实现了一个往文件中打印日志的 Logger 类。具体的代码实现如下所示： 123456789101112131415161718192021222324252627282930public class Logger { private FileWriter writer; public Logger() { File file = new File(&quot;/Users/wangzheng/log.txt&quot;); writer = new FileWriter(file, true); //true表示追加写入 } public void log(String message) { writer.write(mesasge); }}// Logger类的应用示例：public class UserController { private Logger logger = new Logger(); public void login(String username, String password) { // ...省略业务逻辑代码... logger.log(username + &quot; logined!&quot;); }}public class OrderController { private Logger logger = new Logger(); public void create(OrderVo order) { // ...省略业务逻辑代码... logger.log(&quot;Created an order: &quot; + order.toString()); }} 在上面的代码中，所有的日志都写入到同一个文件 /Users/wangzheng/log.txt 中。在 UserController 和 OrderController 中，我们分别创建两个 Logger 对象。 在 Web 容器的 Servlet 多线程环境下，如果两个 Servlet 线程同时分别执行 login() 和 create() 两个函数，并且同时写日志到 log.txt 文件中，那就有可能存在日志信息互相覆盖的情况。 为什么会出现互相覆盖呢？ log.txt 文件是竞争资源，两个线程同时往里面写数据，就有可能存在互相覆盖的情况。 那如何来解决这个问题呢？ 最先想到的就是通过加锁的方式：给log()函数加互斥锁（Java 中可以通过 synchronized 的关键字），同一时刻只允许一个线程调用执行log()函数。除此之外，还可以使用单例模式的解决思路。单例模式相对于之前类级别锁的好处是，不用创建那么多 Logger 对象，一方面节省内存空间，另一方面节省系统文件句柄（对于操作系统来说，文件句柄也是一种资源，不能随便浪费）。 将 Logger 设计成一个单例类，程序中只允许创建一个 Logger 对象，所有的线程共享使用的这一个 Logger 对象，共享一个 FileWriter 对象，而 FileWriter 本身是对象级别线程安全的，也就避免了多线程情况下写日志会互相覆盖的问题。代码如下： 1234567891011121314151617181920212223242526272829303132public class Logger { private FileWriter writer; private static final Logger instance = new Logger(); private Logger() { File file = new File(&quot;/Users/wangzheng/log.txt&quot;); writer = new FileWriter(file, true); //true表示追加写入 } public static Logger getInstance() { return instance; } public void log(String message) { writer.write(mesasge); }}// Logger类的应用示例：public class UserController { public void login(String username, String password) { // ...省略业务逻辑代码... Logger.getInstance().log(username + &quot; logined!&quot;); }}public class OrderController { private Logger logger = new Logger(); public void create(OrderVo order) { // ...省略业务逻辑代码... Logger.getInstance().log(&quot;Created a order: &quot; + order.toString()); }} 实战案例二：表示全局唯一类从业务概念上，如果有些数据在系统中只应保存一份，那就比较适合设计为单例类。 比如，配置信息类。在系统中，我们只有一个配置文件，当配置文件被加载到内存之后，以对象的形式存在，也理所应当只有一份。再比如，唯一递增 ID 号码生成器，如果程序中有两个对象，那就会存在生成重复 ID 的情况，所以，我们应该将 ID 生成器类设计为单例。 123456789101112131415161718import java.util.concurrent.atomic.AtomicLong;public class IdGenerator { // AtomicLong是一个Java并发库中提供的一个原子变量类型, // 它将一些线程不安全需要加锁的复合操作封装为了线程安全的原子操作， // 比如下面会用到的incrementAndGet(). private AtomicLong id = new AtomicLong(0); private static final IdGenerator instance = new IdGenerator(); private IdGenerator() {} public static IdGenerator getInstance() { return instance; } public long getId() { return id.incrementAndGet(); }}// IdGenerator使用举例long id = IdGenerator.getInstance().getId(); 如何实现一个单例？概括起来，要实现一个单例，我们需要关注的点无外乎下面几个： 构造函数需要是 private 访问权限的，这样才能避免外部通过 new 创建实例； 考虑对象创建时的线程安全问题； 考虑是否支持延迟加载； 考虑 getInstance() 性能是否高（是否加锁）。 饿汉式饿汉式的实现方式比较简单。在类加载的时候，instance静态实例就已经创建并初始化好了，所以，instance实例的创建过程是线程安全的。不过，这样的实现方式不支持延迟加载。具体的代码实现如下所示： 1234567891011public class IdGenerator { private AtomicLong id = new AtomicLong(0); private static final IdGenerator instance = new IdGenerator(); private IdGenerator() {} public static IdGenerator getInstance() { return instance; } public long getId() { return id.incrementAndGet(); }} 因为不支持延迟加载，如果实例占用资源多（比如占用内存多）或初始化耗时长（比如需要加载各种配置文件），提前初始化实例是一种浪费资源的行为。最好的方法应该在用到的时候再去初始化。不过，我个人并不认同这样的观点。 如果初始化耗时长，那我们最好不要等到真正要用它的时候，才去执行这个耗时长的初始化过程，这会影响到系统的性能（比如，在响应客户端接口请求的时候，做这个初始化操作，会导致此请求的响应时间变长，甚至超时）。采用饿汉式实现方式，将耗时的初始化操作，提前到程序启动的时候完成，这样就能避免在程序运行的时候，再去初始化导致的性能问题。 如果实例占用资源多，按照 fail-fast 的设计原则（有问题及早暴露），那我们也希望在程序启动时就将这个实例初始化好。如果资源不够，就会在程序启动的时候触发报错（比如 Java 中的 PermGen Space OOM），我们可以立即去修复。这样也能避免在程序运行一段时间后，突然因为初始化这个实例占用资源过多，导致系统崩溃，影响系统的可用性。 懒汉式懒汉式相对于饿汉式的优势是支持延迟加载。具体的代码实现如下所示： 12345678910111213141516public class IdGenerator { private AtomicLong id = new AtomicLong(0); private static IdGenerator instance; private IdGenerator() {} public static synchronized IdGenerator getInstance() { if (instance == null) { instance = new IdGenerator(); } return instance; } public long getId() { return id.incrementAndGet(); }} 不过懒汉式的缺点也很明显，我们给 getInstance() 这个方法加了一把大锁（synchronzed），导致这个函数的并发度很低。量化一下的话，并发度是 1，也就相当于串行操作了。而这个函数是在单例使用期间，一直会被调用。 如果这个单例类偶尔会被用到，那这种实现方式还可以接受。但是，如果频繁地用到，那频繁加锁、释放锁及并发度低等问题，会导致性能瓶颈，这种实现方式就不可取了。 双重检测饿汉式不支持延迟加载，懒汉式有性能问题，不支持高并发。那我们再来看一种既支持延迟加载、又支持高并发的单例实现方式，也就是双重检测实现方式。 在这种实现方式中，只要 instance 被创建之后，即便再调用 getInstance() 函数也不会再进入到加锁逻辑中了。所以，这种实现方式解决了懒汉式并发度低的问题。具体的代码实现如下所示： 1234567891011121314151617181920public class IdGenerator { private AtomicLong id = new AtomicLong(0); private static IdGenerator instance; private IdGenerator() {} public static IdGenerator getInstance() { if (instance == null) { synchronized(IdGenerator.class) { // 此处为类级别的锁 if (instance == null) { instance = new IdGenerator(); } } } return instance; } public long getId() { return id.incrementAndGet(); }} 网上有人说，这种实现方式有些问题。因为指令重排序，可能会导致IdGenerator对象被new出来，并且赋值给 instance之后，还没来得及初始化（执行构造函数中的代码逻辑），就被另一个线程使用了。 要解决这个问题，我们需要给instance成员变量加上volatile关键字，禁止指令重排序才行。实际上，只有很低版本的Java才会有这个问题。我们现在用的高版本的 Java 已经在 JDK 内部实现中解决了这个问题（解决的方法很简单，只要把对象 new 操作和初始化操作设计为原子操作，就自然能禁止重排序）。 静态内部类利用Java的静态内部类，它有点类似饿汉式，但又能做到了延迟加载。具体是怎么做到的呢？代码实现如下： 12345678910111213141516public class IdGenerator { private AtomicLong id = new AtomicLong(0); private IdGenerator() {} private static class SingletonHolder{ private static final IdGenerator instance = new IdGenerator(); } public static IdGenerator getInstance() { return SingletonHolder.instance; } public long getId() { return id.incrementAndGet(); }} SingletonHolder 是一个静态内部类，当外部类 IdGenerator 被加载的时候，并不会创建 SingletonHolder 实例对象。只有当调用 getInstance() 方法时，SingletonHolder 才会被加载，这个时候才会创建 instance。 instance 的唯一性、创建过程的线程安全性，都由 JVM 来保证。所以，这种实现方法既保证了线程安全，又能做到延迟加载。 枚举利用Java 枚举类型本身的特性，保证了实例创建的线程安全性和实例的唯一性。具体的代码如下所示： 12345678public enum IdGenerator { INSTANCE; private AtomicLong id = new AtomicLong(0); public long getId() { return id.incrementAndGet(); }} 单例存在哪些问题?单例对 OOP 特性的支持不友好OOP 的四大特性是封装、抽象、继承、多态。单例这种设计模式对于其中的抽象、继承、多态都支持得不好。为什么这么说呢？我们还是通过 IdGenerator 这个例子来讲解。 1234567891011121314public class Order { public void create(...) { //... long id = IdGenerator.getInstance().getId(); //... }}public class User { public void create(...) { // ... long id = IdGenerator.getInstance().getId(); //... }} IdGenerator 的使用方式违背了基于接口而非实现的设计原则，也就违背了广义上理解的 OOP 的抽象特性。如果未来某一天，我们希望针对不同的业务采用不同的 ID 生成算法。比如，订单 ID 和用户 ID 采用不同的 ID 生成器来生成。为了应对这个需求变化，我们需要修改所有用到 IdGenerator 类的地方，这样代码的改动就会比较大。 单例会隐藏类之间的依赖关系通过构造函数、参数传递等方式声明的类之间的依赖关系，我们通过查看函数的定义，就能很容易识别出来。但是，单例类不需要显示创建、不需要依赖参数传递，在函数中直接调用就可以了。如果代码比较复杂，这种调用关系就会非常隐蔽。 在阅读代码的时候，我们就需要仔细查看每个函数的代码实现，才能知道这个类到底依赖了哪些单例类。 单例对代码的扩展性不友好单例类只能有一个对象实例。如果未来某一天，我们需要在代码中创建两个实例或多个实例，那就要对代码有比较大的改动。你可能会说，会有这样的需求吗？既然单例类大部分情况下都用来表示全局类，怎么会需要两个或者多个实例呢？ 实际上，这样的需求并不少见。我们拿数据库连接池来举例解释一下。 在系统设计初期，我们觉得系统中只应该有一个数据库连接池，这样能方便我们控制对数据库连接资源的消耗。所以，我们把数据库连接池类设计成了单例类。但之后我们发现，系统中有些 SQL 语句运行得非常慢。这些 SQL 语句在执行的时候，长时间占用数据库连接资源，导致其他 SQL 请求无法响应。为了解决这个问题，我们希望将慢 SQL 与其他 SQL 隔离开来执行。 为了实现这样的目的，我们可以在系统中创建两个数据库连接池，慢 SQL 独享一个数据库连接池，其他 SQL 独享另外一个数据库连接池，这样就能避免慢 SQL 影响到其他 SQL 的执行。 如果我们将数据库连接池设计成单例类，显然就无法适应这样的需求变更，也就是说，单例类在某些情况下会影响代码的扩展性、灵活性。所以，数据库连接池、线程池这类的资源池，最好还是不要设计成单例类。 单例对代码的可测试性不友好单例模式的使用会影响到代码的可测试性。如果单例类依赖比较重的外部资源，比如 DB，我们在写单元测试的时候，希望能通过 mock 的方式将它替换掉。而单例类这种硬编码式的使用方式，导致无法实现 mock 替换。 除此之外，如果单例类持有成员变量（比如 IdGenerator 中的 id 成员变量），那它实际上相当于一种全局变量，被所有的代码共享。如果这个全局变量是一个可变全局变量，也就是说，它的成员变量是可以被修改的，那我们在编写单元测试的时候，还需要注意不同测试用例之间，修改了单例类中的同一个成员变量的值，从而导致测试结果互相影响的问题。 单例不支持有参数的构造函数单例不支持有参数的构造函数，比如我们创建一个连接池的单例对象，我们没法通过参数来指定连接池的大小。针对这个问题，我们来看下都有哪些解决方案。 思路一： 创建完实例之后，再调用 init() 函数传递参数。需要注意的是，我们在使用这个单例类的时候，要先调用 init() 方法，然后才能调用 getInstance() 方法，否则代码会抛出异常。具体的代码实现如下所示： 123456789101112131415161718192021222324public class Singleton { private static Singleton instance = null; private final int paramA; private final int paramB; private Singleton(int paramA, int paramB) { this.paramA = paramA; this.paramB = paramB; } public static Singleton getInstance() { if (instance == null) { throw new RuntimeException(&quot;Run init() first.&quot;); } return instance; } public synchronized static Singleton init(int paramA, int paramB) { if (instance != null){ throw new RuntimeException(&quot;Singleton has been created!&quot;); } instance = new Singleton(paramA, paramB); return instance; }}Singleton.init(10, 50); // 先init，再使用Singleton singleton = Singleton.getInstance(); 思路二： 将参数放到 getIntance() 方法中。具体的代码实现如下所示： 12345678910111213141516public class Singleton { private static Singleton instance = null; private final int paramA; private final int paramB; private Singleton(int paramA, int paramB) { this.paramA = paramA; this.paramB = paramB; } public synchronized static Singleton getInstance(int paramA, int paramB) { if (instance == null) { instance = new Singleton(paramA, paramB); } return instance; }}Singleton singleton = Singleton.getInstance(10, 50); 思路三： 将参数放到另外一个全局变量中。具体的代码实现如下。Config 是一个存储了 paramA 和 paramB 值的全局变量。里面的值既可以像下面的代码那样通过静态常量来定义，也可以从配置文件中加载得到。实际上，这种方式是最值得推荐的。 12345678910111213141516171819public class Config { public static final int PARAM_A = 123; public static final int PARAM_B = 245;}public class Singleton { private static Singleton instance = null; private final int paramA; private final int paramB; private Singleton() { this.paramA = Config.PARAM_A; this.paramB = Config.PARAM_B; } public synchronized static Singleton getInstance() { if (instance == null) { instance = new Singleton(); } return instance; }} 有何替代解决方案？即便单例有这么多问题，但我不用不行啊。我业务上有表示全局唯一类的需求，如果不用单例，我怎么才能保证这个类的对象全局唯一呢？ 为了保证全局唯一，除了使用单例，我们还可以用静态方法来实现。这也是项目开发中经常用到的一种实现思路。比如，上一节课中讲的 ID 唯一递增生成器的例子，用静态方法实现一下，就是下面这个样子： 12345678910// 静态方法实现方式public class IdGenerator { private static AtomicLong id = new AtomicLong(0); public static long getId() { return id.incrementAndGet(); }}// 使用举例long id = IdGenerator.getId(); 不过，静态方法这种实现思路，并不能解决我们之前提到的问题。实际上，它比单例更加不灵活，比如，它无法支持延迟加载。我们再来看看有没有其他办法。实际上，单例除了我们之前讲到的使用方法之外，还有另外一种使用方法。具体的代码如下所示： 1234567891011121314// 1. 老的使用方式public demofunction() { //... long id = IdGenerator.getInstance().getId(); //...}// 2. 新的使用方式：依赖注入public demofunction(IdGenerator idGenerator) { long id = idGenerator.getId();}// 外部调用demofunction()的时候，传入idGeneratorIdGenerator idGenerator = IdGenerator.getInsance();demofunction(idGenerator); 基于新的使用方式，我们将单例生成的对象，作为参数传递给函数（也可以通过构造函数传递给类的成员变量），可以解决单例隐藏类之间依赖关系的问题。不过，对于单例存在的其他问题，比如对 OOP 特性、扩展性、可测性不友好等问题，还是无法解决。 所以，如果要完全解决这些问题，我们可能要从根上，寻找其他方式来实现全局唯一类。实际上，类对象的全局唯一性可以通过多种不同的方式来保证。我们既可以通过单例模式来强制保证，也可以通过工厂模式、IOC 容器（比如 Spring IOC 容器）来保证，还可以通过程序员自己来保证（自己在编写代码的时候自己保证不要创建两个类对象）。这就类似 Java 中内存对象的释放由 JVM 来负责，而 C++ 中由程序员自己负责，道理是一样的。","link":"/posts/57358.html"},{"title":"设计模式：工厂模式","text":"重点需要搞清楚应用场景：什么时候该用工厂模式？相对于直接new来创建对象，用工厂模式来创建究竟有什么好处呢？ 简单工厂（Simple Factory）举例说明：根据配置文件的后缀（json、xml、yaml、properties），选择不同的解析器（JsonRuleConfigParser、XmlRuleConfigParser……），将存储在文件中的配置解析成内存对象 RuleConfig。 12345678910111213141516171819202122232425262728public class RuleConfigSource { public RuleConfig load(String ruleConfigFilePath) { String ruleConfigFileExtension = getFileExtension(ruleConfigFilePath); IRuleConfigParser parser = null; if (&quot;json&quot;.equalsIgnoreCase(ruleConfigFileExtension)) { parser = new JsonRuleConfigParser(); } else if (&quot;xml&quot;.equalsIgnoreCase(ruleConfigFileExtension)) { parser = new XmlRuleConfigParser(); } else if (&quot;yaml&quot;.equalsIgnoreCase(ruleConfigFileExtension)) { parser = new YamlRuleConfigParser(); } else if (&quot;properties&quot;.equalsIgnoreCase(ruleConfigFileExtension)) { parser = new PropertiesRuleConfigParser(); } else { throw new InvalidRuleConfigException( &quot;Rule config file format is not supported: &quot; + ruleConfigFilePath); } String configText = &quot;&quot;; //从ruleConfigFilePath文件中读取配置文本到configText中 RuleConfig ruleConfig = parser.parse(configText); return ruleConfig; } private String getFileExtension(String filePath) { //...解析文件名获取扩展名，比如rule.json，返回json return &quot;json&quot;; }} 为了让代码逻辑更加清晰，可读性更好，我们应该将功能独立的代码块封装成函数。按照这个设计思路，我们可以将代码中涉及 parser 创建的部分逻辑剥离出来，抽象成 createParser() 函数。重构之后的代码如下所示： 1234567891011121314151617181920212223242526272829303132 public RuleConfig load(String ruleConfigFilePath) { String ruleConfigFileExtension = getFileExtension(ruleConfigFilePath); IRuleConfigParser parser = createParser(ruleConfigFileExtension); if (parser == null) { throw new InvalidRuleConfigException( &quot;Rule config file format is not supported: &quot; + ruleConfigFilePath); } String configText = &quot;&quot;; //从ruleConfigFilePath文件中读取配置文本到configText中 RuleConfig ruleConfig = parser.parse(configText); return ruleConfig; } private String getFileExtension(String filePath) { //...解析文件名获取扩展名，比如rule.json，返回json return &quot;json&quot;; } private IRuleConfigParser createParser(String configFormat) { IRuleConfigParser parser = null; if (&quot;json&quot;.equalsIgnoreCase(configFormat)) { parser = new JsonRuleConfigParser(); } else if (&quot;xml&quot;.equalsIgnoreCase(configFormat)) { parser = new XmlRuleConfigParser(); } else if (&quot;yaml&quot;.equalsIgnoreCase(configFormat)) { parser = new YamlRuleConfigParser(); } else if (&quot;properties&quot;.equalsIgnoreCase(configFormat)) { parser = new PropertiesRuleConfigParser(); } return parser; }} 为了让类的职责更加单一、代码更加清晰，我们还可以进一步将 createParser() 函数剥离到一个独立的类中，让这个类只负责对象的创建，而这个类就是我们现在要讲的简单工厂模式类。具体的代码如下所示： 1234567891011121314151617181920212223242526272829303132333435public class RuleConfigSource { public RuleConfig load(String ruleConfigFilePath) { String ruleConfigFileExtension = getFileExtension(ruleConfigFilePath); IRuleConfigParser parser = RuleConfigParserFactory.createParser(ruleConfigFileExtension); if (parser == null) { throw new InvalidRuleConfigException( &quot;Rule config file format is not supported: &quot; + ruleConfigFilePath); } String configText = &quot;&quot;; //从ruleConfigFilePath文件中读取配置文本到configText中 RuleConfig ruleConfig = parser.parse(configText); return ruleConfig; } private String getFileExtension(String filePath) { //...解析文件名获取扩展名，比如rule.json，返回json return &quot;json&quot;; }}public class RuleConfigParserFactory { public static IRuleConfigParser createParser(String configFormat) { IRuleConfigParser parser = null; if (&quot;json&quot;.equalsIgnoreCase(configFormat)) { parser = new JsonRuleConfigParser(); } else if (&quot;xml&quot;.equalsIgnoreCase(configFormat)) { parser = new XmlRuleConfigParser(); } else if (&quot;yaml&quot;.equalsIgnoreCase(configFormat)) { parser = new YamlRuleConfigParser(); } else if (&quot;properties&quot;.equalsIgnoreCase(configFormat)) { parser = new PropertiesRuleConfigParser(); } return parser; }} 如果我们要添加新的 parser，那势必要改动到 RuleConfigParserFactory 的代码，那这是不是违反开闭原则呢？ 实际上，如果不是需要频繁地添加新的 parser，只是偶尔修改一下 RuleConfigParserFactory 代码，稍微不符合开闭原则，也是完全可以接受的。 除此之外，在 RuleConfigParserFactory 的代码实现中，有一组 if 分支判断逻辑，是不是应该用多态或其他设计模式来替代呢？ 实际上，如果 if 分支并不是很多，代码中有 if 分支也是完全可以接受的。应用多态或设计模式来替代 if 分支判断逻辑，也并不是没有任何缺点的，它虽然提高了代码的扩展性，更加符合开闭原则，但也增加了类的个数，牺牲了代码的可读性。 工厂方法（Factory Method）如果我们非得要将 if 分支逻辑去掉，那该怎么办呢？比较经典处理方法就是利用多态。 按照多态的实现思路，对上面的代码进行重构。重构之后的代码如下所示： 123456789101112131415161718192021222324252627public interface IRuleConfigParserFactory { IRuleConfigParser createParser();}public class JsonRuleConfigParserFactory implements IRuleConfigParserFactory { @Override public IRuleConfigParser createParser() { return new JsonRuleConfigParser(); }}public class XmlRuleConfigParserFactory implements IRuleConfigParserFactory { @Override public IRuleConfigParser createParser() { return new XmlRuleConfigParser(); }}public class YamlRuleConfigParserFactory implements IRuleConfigParserFactory { @Override public IRuleConfigParser createParser() { return new YamlRuleConfigParser(); }}public class PropertiesRuleConfigParserFactory implements IRuleConfigParserFactory { @Override public IRuleConfigParser createParser() { return new PropertiesRuleConfigParser(); }} 实际上，这就是工厂方法模式的典型代码实现。这样当我们新增一种 parser 的时候，只需要新增一个实现了 IRuleConfigParserFactory 接口的 Factory 类即可。所以，工厂方法模式比起简单工厂模式更加符合开闭原则。 从上面的工厂方法的实现来看，一切都很完美，但是实际上存在挺大的问题，问题存在于这些工厂类的使用上。 接下来，我们看一下，如何用这些工厂类来实现 RuleConfigSource 的 load() 函数。具体的代码如下所示： 12345678910111213141516171819202122232425262728public class RuleConfigSource { public RuleConfig load(String ruleConfigFilePath) { String ruleConfigFileExtension = getFileExtension(ruleConfigFilePath); IRuleConfigParserFactory parserFactory = null; if (&quot;json&quot;.equalsIgnoreCase(ruleConfigFileExtension)) { parserFactory = new JsonRuleConfigParserFactory(); } else if (&quot;xml&quot;.equalsIgnoreCase(ruleConfigFileExtension)) { parserFactory = new XmlRuleConfigParserFactory(); } else if (&quot;yaml&quot;.equalsIgnoreCase(ruleConfigFileExtension)) { parserFactory = new YamlRuleConfigParserFactory(); } else if (&quot;properties&quot;.equalsIgnoreCase(ruleConfigFileExtension)) { parserFactory = new PropertiesRuleConfigParserFactory(); } else { throw new InvalidRuleConfigException(&quot;Rule config file format is not supported: &quot; + ruleConfigFilePath); } IRuleConfigParser parser = parserFactory.createParser(); String configText = &quot;&quot;; //从ruleConfigFilePath文件中读取配置文本到configText中 RuleConfig ruleConfig = parser.parse(configText); return ruleConfig; } private String getFileExtension(String filePath) { //...解析文件名获取扩展名，比如rule.json，返回json return &quot;json&quot;; }} 工厂类对象的创建逻辑又耦合进了 load() 函数中，跟我们最初的代码版本非常相似，引入工厂方法非但没有解决问题，反倒让设计变得更加复杂了。 那怎么来解决这个问题呢？ 我们可以为工厂类再创建一个简单工厂，也就是工厂的工厂，用来创建工厂类对象。代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142public class RuleConfigSource { public RuleConfig load(String ruleConfigFilePath) { String ruleConfigFileExtension = getFileExtension(ruleConfigFilePath); IRuleConfigParserFactory parserFactory = RuleConfigParserFactoryMap.getParserFactory(ruleConfigFileExtension); if (parserFactory == null) { throw new InvalidRuleConfigException(&quot;Rule config file format is not supported: &quot; + ruleConfigFilePath); } IRuleConfigParser parser = parserFactory.createParser(); String configText = &quot;&quot;; //从ruleConfigFilePath文件中读取配置文本到configText中 RuleConfig ruleConfig = parser.parse(configText); return ruleConfig; } private String getFileExtension(String filePath) { //...解析文件名获取扩展名，比如rule.json，返回json return &quot;json&quot;; }}//因为工厂类只包含方法，不包含成员变量，完全可以复用，//不需要每次都创建新的工厂类对象，所以，简单工厂模式的第二种实现思路更加合适。public class RuleConfigParserFactoryMap { //工厂的工厂 private static final Map&lt;String, IRuleConfigParserFactory&gt; cachedFactories = new HashMap&lt;&gt;(); static { cachedFactories.put(&quot;json&quot;, new JsonRuleConfigParserFactory()); cachedFactories.put(&quot;xml&quot;, new XmlRuleConfigParserFactory()); cachedFactories.put(&quot;yaml&quot;, new YamlRuleConfigParserFactory()); cachedFactories.put(&quot;properties&quot;, new PropertiesRuleConfigParserFactory()); } public static IRuleConfigParserFactory getParserFactory(String type) { if (type == null || type.isEmpty()) { return null; } IRuleConfigParserFactory parserFactory = cachedFactories.get(type.toLowerCase()); return parserFactory; }} 当我们需要添加新的规则配置解析器的时候，我们只需要创建新的parser类和parserFactory 类，将新的 parserfactory 对象添加到RuleConfigParserFactoryMap类的cachedFactories属性中即可。代码的改动非常少，基本上符合开闭原则。 抽象工厂（Abstract Factory）抽象工厂模式的应用场景比较特殊，没有前两种常用。在简单工厂和工厂方法中，类只有一种分类方式。比如，在规则配置解析那个例子中，解析器类只会根据配置文件格式（Json、Xml、Yaml……）来分类。 但是，如果类有两种分类方式，比如，我们既可以按照配置文件格式来分类，也可以按照解析的对象（Rule 规则配置还是 System 系统配置）来分类，那就会对应下面这 8 个 parser 类。 12345678910针对规则配置的解析器：基于接口IRuleConfigParserJsonRuleConfigParserXmlRuleConfigParserYamlRuleConfigParserPropertiesRuleConfigParser针对系统配置的解析器：基于接口ISystemConfigParserJsonSystemConfigParserXmlSystemConfigParserYamlSystemConfigParserPropertiesSystemConfigParser 针对这种特殊的场景，如果还是继续用工厂方法来实现的话，我们要针对每个 parser 都编写一个工厂类，也就是要编写 8 个工厂类。如果我们未来还需要增加针对业务配置的解析器（比如 IBizConfigParser），那就要再对应地增加 4 个工厂类。而我们知道，过多的类也会让系统难维护。这个问题该怎么解决呢？ 抽象工厂就是针对这种非常特殊的场景而诞生的。我们可以让一个工厂负责创建多个不同类型的对象（IRuleConfigParser、ISystemConfigParser 等），而不是只创建一种 parser 对象。这样就可以有效地减少工厂类的个数。具体的代码实现如下所示 123456789101112131415161718192021222324252627public interface IConfigParserFactory { IRuleConfigParser createRuleParser(); ISystemConfigParser createSystemParser(); //此处可以扩展新的parser类型，比如IBizConfigParser}public class JsonConfigParserFactory implements IConfigParserFactory { @Override public IRuleConfigParser createRuleParser() { return new JsonRuleConfigParser(); } @Override public ISystemConfigParser createSystemParser() { return new JsonSystemConfigParser(); }}public class XmlConfigParserFactory implements IConfigParserFactory { @Override public IRuleConfigParser createRuleParser() { return new XmlRuleConfigParser(); } @Override public ISystemConfigParser createSystemParser() { return new XmlSystemConfigParser(); }}// 省略YamlConfigParserFactory和PropertiesConfigParserFactory代码 扩展工厂模式和 DI 容器有何区别？实际上，DI 容器底层最基本的设计思路就是基于工厂模式的。DI 容器相当于一个大的工厂类，在程序启动的时候，根据配置（要创建哪些类对象，每个类对象的创建需要依赖哪些其他类对象）事先创建好对象。当应用程序需要使用某个类对象的时候，直接从容器中获取即可。正是因为它持有一堆对象，所以这个框架才被称为“容器”。 在工厂模式中，一个工厂类只负责某个类对象或者某一组相关类对象（继承自同一抽象类或者接口的子类）的创建，而 DI 容器负责的是整个应用中所有类对象的创建。 除此之外，DI 容器负责的事情要比单纯的工厂模式要多。比如，它还包括配置的解析、对象生命周期的管理。 DI 容器的核心功能有哪些？ 配置解析 作为一个通用的框架来说，框架代码跟应用代码应该是高度解耦的，DI 容器事先并不知道应用会创建哪些对象，不可能把某个应用要创建的对象写死在框架代码中。所以，我们需要通过一种形式，让应用告知 DI 容器要创建哪些对象。 需要由 DI 容器来创建的类对象和创建类对象的必要信息（使用哪个构造函数以及对应的构造函数参数都是什么等等），放到配置文件中。容器读取配置文件，根据配置文件提供的信息来创建对象。 对象创建 在 DI 容器中，如果我们给每个类都对应创建一个工厂类，那项目中类的个数会成倍增加，这会增加代码的维护成本。要解决这个问题并不难。我们只需要将所有类对象的创建都放到一个工厂类中完成就可以了，比如 BeansFactory。 通过“反射”这种机制，它能在程序运行的过程中，动态地加载类、创建对象，不需要事先在代码中写死要创建哪些对象。所以，不管是创建一个对象还是十个对象，BeansFactory 工厂类代码都是一样的。 对象生命周期管理 简单工厂模式有两种实现方式，一种是每次都返回新创建的对象，另一种是每次都返回同一个事先创建好的对象，也就是所谓的单例对象。在 Spring 框架中，我们可以通过配置 scope 属性，来区分这两种不同类型的对象。scope=prototype 表示返回新创建的对象，scope=singleton 表示返回单例对象。 除此之外，我们还可以配置对象是否支持懒加载。如果 lazy-init=true，对象在真正被使用到的时候（比如：BeansFactory.getBean(“userService”)）才被被创建；如果 lazy-init=false，对象在应用启动的时候就事先创建好。 不仅如此，我们还可以配置对象的 init-method 和 destroy-method 方法，比如 init-method=loadProperties()，destroy-method=updateConfigFile()。DI 容器在创建好对象之后，会主动调用 init-method 属性指定的方法来初始化对象。在对象被最终销毁之前，DI 容器会主动调用 destroy-method 属性指定的方法来做一些清理工作，比如释放数据库连接池、关闭文件。 使用场景当创建逻辑比较复杂，是一个“大工程”的时候，我们就考虑使用工厂模式，封装对象的创建过程，将对象的创建和使用相分离。何为创建逻辑比较复杂呢？我总结了下面两种情况。 第一种情况：类似规则配置解析的例子，代码中存在 if-else 分支判断，动态地根据不同的类型创建不同的对象。针对这种情况，我们就考虑使用工厂模式，将这一大坨 if-else 创建对象的代码抽离出来，放到工厂类中。 第二种情况，尽管我们不需要根据不同的类型创建不同的对象，但是，单个对象本身的创建过程比较复杂，比如要组合其他类对象，做各种初始化操作。在这种情况下，我们也可以考虑使用工厂模式，将对象的创建过程封装到工厂类中。 对于第一种情况，当每个对象的创建逻辑都比较简单的时候，我推荐使用简单工厂模式，将多个对象的创建逻辑放到一个工厂类中。当每个对象的创建逻辑都比较复杂的时候，为了避免设计一个过于庞大的简单工厂类，我推荐使用工厂方法模式，将创建逻辑拆分得更细，每个对象的创建逻辑独立到各自的工厂类中。 同理，对于第二种情况，因为单个对象本身的创建逻辑就比较复杂，所以，我建议使用工厂方法模式。 除了刚刚提到的这几种情况之外，如果创建对象的逻辑并不复杂，那我们就直接通过 new 来创建对象就可以了，不需要使用工厂模式。 判断要不要使用工厂模式的最本质的参考标准，如下： 封装变化：创建逻辑有可能变化，封装成工厂类之后，创建逻辑的变更对调用者透明。 代码复用：\b创建代码抽离到独立的工厂类之后可以复用。 隔离复杂性：封装复杂的创建逻辑，调用者无需了解如何创建对象。 控制复杂度：将创建代码抽离出来，让原本的函数或类职责更单一，代码更简洁。S","link":"/posts/17223.html"},{"title":"Dubbo：接口调用过程","text":"服务调用主流程在进行源码分析之前，先通过一张图了解Dubbo服务调用过程。 首先服务消费者通过代理对象Proxy 发起远程调用，接着通过网络客户端Client 将编码后的请求发送给服务提供方的网络层上，也就是 Server。 Server 在收到请求后，首先要做的事情是对数据包进行解码。然后将解码后的请求发送至分发器 Dispatcher，再由分发器将请求派发到指定的线程池上，最后由线程池调用具体的服务。这就是一个远程调用请求的发送与接收过程。 服务消费方调用服务调用方式Dubbo 支持同步和异步两种调用方式，其中异步调用还可细分为“有返回值”的异步调用和“无返回值”的异步调用。所谓“无返回值”异步调用是指服务消费方只管调用，但不关心调用结果，此时 Dubbo 会直接返回一个空的 RpcResult。若要使用异步特性，需要服务消费方手动进行配置。默认情况下，Dubbo 使用同步调用方式。 Dubbo 默认使用 Javassist 框架为服务接口生成动态代理类，因此我们需要先将代理类进行反编译才能看到源码。代码如下： 12345678910111213141516171819202122232425262728public class proxy0 implements ClassGenerator.DC, EchoService, DemoService { // 方法数组 public static Method[] methods; private InvocationHandler handler; public proxy0(InvocationHandler invocationHandler) { this.handler = invocationHandler; } public proxy0() { } public String sayHello(String string) { // 将参数存储到 Object 数组中 Object[] arrobject = new Object[]{string}; // 调用 InvocationHandler 实现类的 invoke 方法得到调用结果 Object object = this.handler.invoke(this, methods[0], arrobject); // 返回调用结果 return (String)object; } /** 回声测试方法 */ public Object $echo(Object object) { Object[] arrobject = new Object[]{object}; Object object2 = this.handler.invoke(this, methods[1], arrobject); return object2; }} 代理类的逻辑比较简单。首先将运行时参数存储到数组中，然后调用 InvocationHandler 接口实现类的 invoke 方法，得到调用结果，最后将结果转型并返回给调用方。关于代理类的逻辑就说这么多，继续向下分析。 123456789101112131415161718192021222324252627282930313233public class InvokerInvocationHandler implements InvocationHandler { private final Invoker&lt;?&gt; invoker; public InvokerInvocationHandler(Invoker&lt;?&gt; handler) { this.invoker = handler; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { String methodName = method.getName(); Class&lt;?&gt;[] parameterTypes = method.getParameterTypes(); // 拦截定义在 Object 类中的方法（未被子类重写），比如 wait/notify if (method.getDeclaringClass() == Object.class) { return method.invoke(invoker, args); } // 如果 toString、hashCode 和 equals 等方法被子类重写了，这里也直接调用 if (&quot;toString&quot;.equals(methodName) &amp;&amp; parameterTypes.length == 0) { return invoker.toString(); } if (&quot;hashCode&quot;.equals(methodName) &amp;&amp; parameterTypes.length == 0) { return invoker.hashCode(); } if (&quot;equals&quot;.equals(methodName) &amp;&amp; parameterTypes.length == 1) { return invoker.equals(args[0]); } // 将 method 和 args 封装到 RpcInvocation 中，并执行后续的调用 return invoker.invoke(new RpcInvocation(method, args)).recreate(); }} 由于本文重点是分析服务调用，因此省略中间环节（服务降级、集群容错），直接分析DubboInvoker。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class DubboInvoker&lt;T&gt; extends AbstractInvoker&lt;T&gt; { private final ExchangeClient[] clients; protected Result doInvoke(final Invocation invocation) throws Throwable { RpcInvocation inv = (RpcInvocation) invocation; final String methodName = RpcUtils.getMethodName(invocation); // 设置 path 和 version 到 attachment 中 inv.setAttachment(Constants.PATH_KEY, getUrl().getPath()); inv.setAttachment(Constants.VERSION_KEY, version); ExchangeClient currentClient; if (clients.length == 1) { // 从 clients 数组中获取 ExchangeClient currentClient = clients[0]; } else { currentClient = clients[index.getAndIncrement() % clients.length]; } try { // 获取异步配置 boolean isAsync = RpcUtils.isAsync(getUrl(), invocation); // isOneway 为 true，表示“单向”通信 boolean isOneway = RpcUtils.isOneway(getUrl(), invocation); int timeout = getUrl().getMethodParameter(methodName, Constants.TIMEOUT_KEY, Constants.DEFAULT_TIMEOUT); // 异步无返回值 if (isOneway) { boolean isSent = getUrl().getMethodParameter(methodName, Constants.SENT_KEY, false); // 发送请求 currentClient.send(inv, isSent); // 设置上下文中的 future 字段为 null RpcContext.getContext().setFuture(null); // 返回一个空的 RpcResult return new RpcResult(); } // 异步有返回值 else if (isAsync) { // 发送请求，并得到一个 ResponseFuture 实例 ResponseFuture future = currentClient.request(inv, timeout); // 设置 future 到上下文中 RpcContext.getContext().setFuture(new FutureAdapter&lt;Object&gt;(future)); // 暂时返回一个空结果 return new RpcResult(); } // 同步调用 else { RpcContext.getContext().setFuture(null); // 发送请求，得到一个 ResponseFuture 实例，并调用该实例的 get 方法进行等待 return (Result) currentClient.request(inv, timeout).get(); } } catch (TimeoutException e) { throw new RpcException(..., &quot;Invoke remote method timeout....&quot;); } catch (RemotingException e) { throw new RpcException(..., &quot;Failed to invoke remote method: ...&quot;); } } // 省略其他方法} 上面的代码包含了 Dubbo 对同步和异步调用的处理逻辑，搞懂了上面的代码，会对 Dubbo 的同步和异步调用方式有更深入的了解。Dubbo 实现同步和异步调用比较关键的一点就在于由谁调用 ResponseFuture 的 get 方法。同步调用模式下，由框架自身调用 ResponseFuture 的 get 方法。异步调用模式下，则由用户调用该方法。ResponseFuture 是一个接口，下面我们来看一下它的默认实现类 DefaultFuture 的源码。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495public class DefaultFuture implements ResponseFuture { private static final Map&lt;Long, Channel&gt; CHANNELS = new ConcurrentHashMap&lt;Long, Channel&gt;(); private static final Map&lt;Long, DefaultFuture&gt; FUTURES = new ConcurrentHashMap&lt;Long, DefaultFuture&gt;(); private final long id; private final Channel channel; private final Request request; private final int timeout; private final Lock lock = new ReentrantLock(); private final Condition done = lock.newCondition(); private volatile Response response; public DefaultFuture(Channel channel, Request request, int timeout) { this.channel = channel; this.request = request; // 获取请求 id，这个 id 很重要，后面还会见到 this.id = request.getId(); this.timeout = timeout &gt; 0 ? timeout : channel.getUrl().getPositiveParameter(Constants.TIMEOUT_KEY, Constants.DEFAULT_TIMEOUT); // 存储 &lt;requestId, DefaultFuture&gt; 映射关系到 FUTURES 中 FUTURES.put(id, this); CHANNELS.put(id, channel); } @Override public Object get() throws RemotingException { return get(timeout); } @Override public Object get(int timeout) throws RemotingException { if (timeout &lt;= 0) { timeout = Constants.DEFAULT_TIMEOUT; } // 检测服务提供方是否成功返回了调用结果 if (!isDone()) { long start = System.currentTimeMillis(); lock.lock(); try { // 循环检测服务提供方是否成功返回了调用结果 while (!isDone()) { // 如果调用结果尚未返回，这里等待一段时间 done.await(timeout, TimeUnit.MILLISECONDS); // 如果调用结果成功返回，或等待超时，此时跳出 while 循环，执行后续的逻辑 if (isDone() || System.currentTimeMillis() - start &gt; timeout) { break; } } } catch (InterruptedException e) { throw new RuntimeException(e); } finally { lock.unlock(); } // 如果调用结果仍未返回，则抛出超时异常 if (!isDone()) { throw new TimeoutException(sent &gt; 0, channel, getTimeoutMessage(false)); } } // 返回调用结果 return returnFromResponse(); } @Override public boolean isDone() { // 通过检测 response 字段为空与否，判断是否收到了调用结果 return response != null; } private Object returnFromResponse() throws RemotingException { Response res = response; if (res == null) { throw new IllegalStateException(&quot;response cannot be null&quot;); } // 如果调用结果的状态为 Response.OK，则表示调用过程正常，服务提供方成功返回了调用结果 if (res.getStatus() == Response.OK) { return res.getResult(); } // 抛出异常 if (res.getStatus() == Response.CLIENT_TIMEOUT || res.getStatus() == Response.SERVER_TIMEOUT) { throw new TimeoutException(res.getStatus() == Response.SERVER_TIMEOUT, channel, res.getErrorMessage()); } throw new RemotingException(channel, res.getErrorMessage()); } // 省略其他方法} 当服务消费者还未接收到调用结果时，用户线程调用 get 方法会被阻塞住。同步调用模式下，框架获得 DefaultFuture 对象后，会立即调用 get 方法进行等待。而异步模式下则是将该对象封装到 FutureAdapter 实例中，并将 FutureAdapter 实例设置到 RpcContext 中，供用户使用。FutureAdapter 是一个适配器，用于将 Dubbo 中的 ResponseFuture 与 JDK 中的 Future 进行适配。这样当用户线程调用 Future 的 get 方法时，经过 FutureAdapter 适配，最终会调用 ResponseFuture 实现类对象的 get 方法，也就是 DefaultFuture 的 get 方法。 发送请求在深入分析源码前，我们先来看一张图（DubboInvoke实例化）。 首先分析 ReferenceCountExchangeClient 的源码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748final class ReferenceCountExchangeClient implements ExchangeClient { private final URL url; private final AtomicInteger referenceCount = new AtomicInteger(0); public ReferenceCountExchangeClient(ExchangeClient client, ConcurrentMap&lt;String, LazyConnectExchangeClient&gt; ghostClientMap) { this.client = client; // 引用计数自增 referenceCount.incrementAndGet(); this.url = client.getUrl(); // ... } @Override public ResponseFuture request(Object request) throws RemotingException { // 直接调用被装饰对象的同签名方法 return client.request(request); } @Override public ResponseFuture request(Object request, int timeout) throws RemotingException { // 直接调用被装饰对象的同签名方法 return client.request(request, timeout); } /** 引用计数自增，该方法由外部调用 */ public void incrementAndGetCount() { // referenceCount 自增 referenceCount.incrementAndGet(); } @Override public void close(int timeout) { // referenceCount 自减 if (referenceCount.decrementAndGet() &lt;= 0) { if (timeout == 0) { client.close(); } else { client.close(timeout); } client = replaceWithLazyClient(); } } // 省略部分方法} ReferenceCountExchangeClient 内部定义了一个引用计数变量 referenceCount，每当该对象被引用一次 referenceCount 都会进行自增。每当 close 方法被调用时，referenceCount 进行自减。ReferenceCountExchangeClient 内部仅实现了一个引用计数的功能，其他方法并无复杂逻辑，均是直接调用被装饰对象的相关方法。继续向下分析，这次是 HeaderExchangeClient。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384public class HeaderExchangeClient implements ExchangeClient { private static final ScheduledThreadPoolExecutor scheduled = new ScheduledThreadPoolExecutor(2, new NamedThreadFactory(&quot;dubbo-remoting-client-heartbeat&quot;, true)); private final Client client; private final ExchangeChannel channel; private ScheduledFuture&lt;?&gt; heartbeatTimer; private int heartbeat; private int heartbeatTimeout; public HeaderExchangeClient(Client client, boolean needHeartbeat) { if (client == null) { throw new IllegalArgumentException(&quot;client == null&quot;); } this.client = client; // 创建 HeaderExchangeChannel 对象 this.channel = new HeaderExchangeChannel(client); // 以下代码均与心跳检测逻辑有关 String dubbo = client.getUrl().getParameter(Constants.DUBBO_VERSION_KEY); this.heartbeat = client.getUrl().getParameter(Constants.HEARTBEAT_KEY, dubbo != null &amp;&amp; dubbo.startsWith(&quot;1.0.&quot;) ? Constants.DEFAULT_HEARTBEAT : 0); this.heartbeatTimeout = client.getUrl().getParameter(Constants.HEARTBEAT_TIMEOUT_KEY, heartbeat * 3); if (heartbeatTimeout &lt; heartbeat * 2) { throw new IllegalStateException(&quot;heartbeatTimeout &lt; heartbeatInterval * 2&quot;); } if (needHeartbeat) { // 开启心跳检测定时器 startHeartbeatTimer(); } } @Override public ResponseFuture request(Object request) throws RemotingException { // 直接 HeaderExchangeChannel 对象的同签名方法 return channel.request(request); } @Override public ResponseFuture request(Object request, int timeout) throws RemotingException { // 直接 HeaderExchangeChannel 对象的同签名方法 return channel.request(request, timeout); } @Override public void close() { doClose(); channel.close(); } private void doClose() { // 停止心跳检测定时器 stopHeartbeatTimer(); } private void startHeartbeatTimer() { stopHeartbeatTimer(); if (heartbeat &gt; 0) { heartbeatTimer = scheduled.scheduleWithFixedDelay( new HeartBeatTask(new HeartBeatTask.ChannelProvider() { @Override public Collection&lt;Channel&gt; getChannels() { return Collections.&lt;Channel&gt;singletonList(HeaderExchangeClient.this); } }, heartbeat, heartbeatTimeout), heartbeat, heartbeat, TimeUnit.MILLISECONDS); } } private void stopHeartbeatTimer() { if (heartbeatTimer != null &amp;&amp; !heartbeatTimer.isCancelled()) { try { heartbeatTimer.cancel(true); scheduled.purge(); } catch (Throwable e) { if (logger.isWarnEnabled()) { logger.warn(e.getMessage(), e); } } } heartbeatTimer = null; } // 省略部分方法} HeaderExchangeClient仅仅封装了一些关于心跳检测的逻辑，具体功能是由HeaderExchangeChannel完成的。因此就不多说了，继续向下看 1234567891011121314151617181920212223242526272829303132333435363738394041424344final class HeaderExchangeChannel implements ExchangeChannel { private final Channel channel; HeaderExchangeChannel(Channel channel) { if (channel == null) { throw new IllegalArgumentException(&quot;channel == null&quot;); } // 这里的 channel 指向的是 NettyClient this.channel = channel; } @Override public ResponseFuture request(Object request) throws RemotingException { return request(request, channel.getUrl().getPositiveParameter(Constants.TIMEOUT_KEY, Constants.DEFAULT_TIMEOUT)); } @Override public ResponseFuture request(Object request, int timeout) throws RemotingException { if (closed) { throw new RemotingException(..., &quot;Failed to send request ...); } // 创建 Request 对象 Request req = new Request(); req.setVersion(Version.getProtocolVersion()); // 设置双向通信标志为 true req.setTwoWay(true); // 这里的 request 变量类型为 RpcInvocation req.setData(request); // 创建 DefaultFuture 对象 DefaultFuture future = new DefaultFuture(channel, req, timeout); try { // 调用 NettyClient 的 send 方法发送请求 channel.send(req); } catch (RemotingException e) { future.cancel(); throw e; } // 返回 DefaultFuture 对象 return future; }} 上面的方法首先定义了一个Request对象，然后再将该对象传给 NettyClient 的 send 方法，进行后续的调用。需要说明的是，NettyClient 中并未实现 send 方法，该方法继承自父类 AbstractPeer，下面直接分析 AbstractPeer 的代码。 123456789101112131415161718192021222324252627282930313233public abstract class AbstractPeer implements Endpoint, ChannelHandler { @Override public void send(Object message) throws RemotingException { // 该方法由 AbstractClient 类实现 send(message, url.getParameter(Constants.SENT_KEY, false)); } // 省略其他方法}public abstract class AbstractClient extends AbstractEndpoint implements Client { @Override public void send(Object message, boolean sent) throws RemotingException { if (send_reconnect &amp;&amp; !isConnected()) { connect(); } // 获取 Channel，getChannel 是一个抽象方法，具体由子类实现 Channel channel = getChannel(); if (channel == null || !channel.isConnected()) { throw new RemotingException(this, &quot;message can not send ...&quot;); } // 继续向下调用 channel.send(message, sent); } protected abstract Channel getChannel(); // 省略其他方法} 获取到 NettyChannel 实例后，即可进行后续的调用。下面看一下 NettyChannel的send方法。 1234567891011121314151617181920212223242526272829303132public void send(Object message, boolean sent) throws RemotingException { super.send(message, sent); boolean success = true; int timeout = 0; try { // 发送消息(包含请求和响应消息) ChannelFuture future = channel.write(message); // sent 的值源于 &lt;dubbo:method sent=&quot;true/false&quot; /&gt; 中 sent 的配置值，有两种配置值： // 1. true: 等待消息发出，消息发送失败将抛出异常 // 2. false: 不等待消息发出，将消息放入 IO 队列，即刻返回 // 默认情况下 sent = false； if (sent) { timeout = getUrl().getPositiveParameter(Constants.TIMEOUT_KEY, Constants.DEFAULT_TIMEOUT); // 等待消息发出，若在规定时间没能发出，success 会被置为 false success = future.await(timeout); } Throwable cause = future.getCause(); if (cause != null) { throw cause; } } catch (Throwable e) { throw new RemotingException(this, &quot;Failed to send message ...&quot;); } // 若 success 为 false，这里抛出异常 if (!success) { throw new RemotingException(this, &quot;Failed to send message ...&quot;); }} 2.3 NettyClientNettyClient类继承结构图如下： NettyClient实例化 通过Debug，nettyClientHandler实例如下： 服务提供方处理请求NettyServer通过之前的文章我们知道Server是在服务暴露的过程中创建的，在默认情况下Server为NettyServer。其类继承结构如下： NettyServer的构造方法如下： 熟悉Netty都应该知道Server的业务处理是由NettyServerHandler类完成的，因此相当重要。通过Debug其实例如下： 调用服务解码器将数据包解析成 Request 对象后，nettyServerHandler的 channelRead 方法紧接着会收到这个对象，并将这个对象继续向下传递。这期间该对象会被依次传递给 NettyServer、MultiMessageHandler、HeartbeatHandler 以及 AllChannelHandler。最后由 AllChannelHandler 将该对象封装到 Runnable 实现类对象中，并将 Runnable 放入线程池中执行后续的调用逻辑。整个调用栈如下： 123456nettyServerHandler#channelRead(ChannelHandlerContext, MessageEvent) —&gt; AbstractPeer#received(Channel, Object) —&gt; MultiMessageHandler#received(Channel, Object) —&gt; HeartbeatHandler#received(Channel, Object) —&gt; AllChannelHandler#received(Channel, Object) —&gt; ExecutorService#execute(Runnable) // 由线程池执行后续的调用逻辑 Dubbo 将底层通信框架中接收请求的线程称为 IO 线程。如果一些事件处理逻辑可以很快执行完，比如只在内存打一个标记，此时直接在 IO 线程上执行该段逻辑即可。但如果事件的处理逻辑比较耗时，比如该段逻辑会发起数据库查询或者 HTTP 请求。此时我们就不应该让事件处理逻辑在 IO 线程上执行，而是应该派发到线程池中去执行。原因也很简单，IO 线程主要用于接收请求，如果 IO 线程被占满，将导致它不能接收新的请求。以上就是线程派发的背景，下面我们再来通过 Dubbo 调用图，看一下线程派发器所处的位置。 如上图，红框中的 Dispatcher 就是线程派发器。需要说明的是，Dispatcher 真实的职责创建具有线程派发能力的 ChannelHandler，比如 AllChannelHandler、MessageOnlyChannelHandler 和 ExecutionChannelHandler 等，其本身并不具备线程派发能力。Dubbo 支持 5 种不同的线程派发策略，下面通过一个表格列举一下。 策略 用途 all 所有消息都派发到线程池，包括请求，响应，连接事件，断开事件等 direct 所有消息都不派发到线程池，全部在 IO 线程上直接执行 message 只有请求响应**消息派发到线程池，其它消息均在 IO 线程上执行 execution 只有请求消息派发到线程池，不含响应。其它消息均在 IO 线程上执行 connection 在 IO 线程上，将连接断开事件放入队列，有序逐个执行，其它消息派发到线程池 默认配置下，Dubbo 使用all派发策略，即将所有的消息都派发到线程池中。下面我们来分析一下 AllChannelHandler 的代码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class AllChannelHandler extends WrappedChannelHandler { public AllChannelHandler(ChannelHandler handler, URL url) { super(handler, url); } /** 处理连接事件 */ @Override public void connected(Channel channel) throws RemotingException { // 获取线程池 ExecutorService cexecutor = getExecutorService(); try { // 将连接事件派发到线程池中处理 cexecutor.execute(new ChannelEventRunnable(channel, handler, ChannelState.CONNECTED)); } catch (Throwable t) { throw new ExecutionException(..., &quot; error when process connected event .&quot;, t); } } /** 处理断开事件 */ @Override public void disconnected(Channel channel) throws RemotingException { ExecutorService cexecutor = getExecutorService(); try { cexecutor.execute(new ChannelEventRunnable(channel, handler, ChannelState.DISCONNECTED)); } catch (Throwable t) { throw new ExecutionException(..., &quot;error when process disconnected event .&quot;, t); } } /** 处理请求和响应消息，这里的 message 变量类型可能是 Request，也可能是 Response */ @Override public void received(Channel channel, Object message) throws RemotingException { ExecutorService cexecutor = getExecutorService(); try { // 将请求和响应消息派发到线程池中处理 cexecutor.execute(new ChannelEventRunnable(channel, handler, ChannelState.RECEIVED, message)); } catch (Throwable t) { if(message instanceof Request &amp;&amp; t instanceof RejectedExecutionException){ Request request = (Request)message; // 如果通信方式为双向通信，此时将 Server side ... threadpool is exhausted // 错误信息封装到 Response 中，并返回给服务消费方。 if(request.isTwoWay()){ String msg = &quot;Server side(&quot; + url.getIp() + &quot;,&quot; + url.getPort() + &quot;) threadpool is exhausted ,detail msg:&quot; + t.getMessage(); Response response = new Response(request.getId(), request.getVersion()); response.setStatus(Response.SERVER_THREADPOOL_EXHAUSTED_ERROR); response.setErrorMessage(msg); // 返回包含错误信息的 Response 对象 channel.send(response); return; } } throw new ExecutionException(..., &quot; error when process received event .&quot;, t); } } /** 处理异常信息 */ @Override public void caught(Channel channel, Throwable exception) throws RemotingException { ExecutorService cexecutor = getExecutorService(); try { cexecutor.execute(new ChannelEventRunnable(channel, handler, ChannelState.CAUGHT, exception)); } catch (Throwable t) { throw new ExecutionException(..., &quot;error when process caught event ...&quot;); } }} 如上，请求对象会被封装 ChannelEventRunnable 中，ChannelEventRunnable 将会是服务调用过程的新起点。所以接下来我们以 ChannelEventRunnable 为起点向下探索，该类的主要代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576public class ChannelEventRunnable implements Runnable { private static final Logger logger = LoggerFactory.getLogger(ChannelEventRunnable.class); private final ChannelHandler handler; private final Channel channel; private final ChannelState state; private final Throwable exception; private final Object message; public ChannelEventRunnable(Channel channel, ChannelHandler handler, ChannelState state) { this(channel, handler, state, null); } public ChannelEventRunnable(Channel channel, ChannelHandler handler, ChannelState state, Object message) { this(channel, handler, state, message, null); } public ChannelEventRunnable(Channel channel, ChannelHandler handler, ChannelState state, Throwable t) { this(channel, handler, state, null, t); } public ChannelEventRunnable(Channel channel, ChannelHandler handler, ChannelState state, Object message, Throwable exception) { this.channel = channel; this.handler = handler; this.state = state; this.message = message; this.exception = exception; } @Override public void run() { if (state == ChannelState.RECEIVED) { try { handler.received(channel, message); } catch (Exception e) { logger.warn(&quot;ChannelEventRunnable handle &quot; + state + &quot; operation error, channel is &quot; + channel + &quot;, message is &quot; + message, e); } } else { switch (state) { case CONNECTED: try { handler.connected(channel); } catch (Exception e) { logger.warn(&quot;ChannelEventRunnable handle &quot; + state + &quot; operation error, channel is &quot; + channel, e); } break; case DISCONNECTED: try { handler.disconnected(channel); } catch (Exception e) { logger.warn(&quot;ChannelEventRunnable handle &quot; + state + &quot; operation error, channel is &quot; + channel, e); } break; case SENT: try { handler.sent(channel, message); } catch (Exception e) { logger.warn(&quot;ChannelEventRunnable handle &quot; + state + &quot; operation error, channel is &quot; + channel + &quot;, message is &quot; + message, e); } case CAUGHT: try { handler.caught(channel, exception); } catch (Exception e) { logger.warn(&quot;ChannelEventRunnable handle &quot; + state + &quot; operation error, channel is &quot; + channel + &quot;, message is: &quot; + message + &quot;, exception is &quot; + exception, e); } break; default: logger.warn(&quot;unknown state: &quot; + state + &quot;, message is &quot; + message); } } }} 编解码NettyCodecAdapter Codec2接口定义编解码方法，如下所示： 12345678910111213141516@SPIpublic interface Codec2 { @Adaptive({Constants.CODEC_KEY}) void encode(Channel channel, ChannelBuffer buffer, Object message) throws IOException; @Adaptive({Constants.CODEC_KEY}) Object decode(Channel channel, ChannelBuffer buffer) throws IOException; enum DecodeResult { NEED_MORE_INPUT, SKIP_SOME_INPUT }} 类继承结构 Dubbo数据包结构 Dubbo 数据包分为消息头和消息体，消息头用于存储一些元信息，比如魔数（Magic），数据包类型（Request/Response），消息体长度（Data Length）等。消息体中用于存储具体的调用消息，比如方法名称，参数列表等。下面简单列举一下消息头的内容。 偏移量(Bit) 字段 取值 0 ~ 7 魔数高位 0xda00 8 ~ 15 魔数低位 0xbb 16 数据包类型 0 - Response, 1 - Request 17 调用方式 仅在第16位被设为1的情况下有效，0 - 单向调用，1 - 双向调用 18 事件标识 0 - 当前数据包是请求或响应包，1 - 当前数据包是心跳包 19 ~ 23 序列化器编号 2 - Hessian2Serialization 3 - JavaSerialization 4 - CompactedJavaSerialization 6 - FastJsonSerialization 7 - NativeJavaSerialization 8 - KryoSerialization 9 - FstSerialization 24 ~ 31 状态 20 - OK 30 - CLIENT_TIMEOUT 31 - SERVER_TIMEOUT 40 - BAD_REQUEST 50 - BAD_RESPONSE …… 32 ~ 95 请求编号 共8字节，运行时生成 96 ~ 127 消息体长度 运行时计算 服务消费方接收调用结果响应数据解码完成后，Dubbo 会将响应对象派发到线程池上。要注意的是，线程池中的线程并非用户的调用线程，所以要想办法将响应对象从线程池线程传递到用户线程上。前面已分析过用户线程在发送完请求后的动作，即调用 DefaultFuture 的 get 方法等待响应对象的到来。当响应对象到来后，用户线程会被唤醒，并通过调用编号获取属于自己的响应对象。代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class HeaderExchangeHandler implements ChannelHandlerDelegate { @Override public void received(Channel channel, Object message) throws RemotingException { channel.setAttribute(KEY_READ_TIMESTAMP, System.currentTimeMillis()); ExchangeChannel exchangeChannel = HeaderExchangeChannel.getOrAddChannel(channel); try { if (message instanceof Request) { // 处理请求，前面已分析过，省略 } else if (message instanceof Response) { // 处理响应 handleResponse(channel, (Response) message); } else if (message instanceof String) { // telnet 相关，忽略 } else { handler.received(exchangeChannel, message); } } finally { HeaderExchangeChannel.removeChannelIfDisconnected(channel); } } static void handleResponse(Channel channel, Response response) throws RemotingException { if (response != null &amp;&amp; !response.isHeartbeat()) { // 继续向下调用 DefaultFuture.received(channel, response); } }}public class DefaultFuture implements ResponseFuture { private final Lock lock = new ReentrantLock(); private final Condition done = lock.newCondition(); private volatile Response response; public static void received(Channel channel, Response response) { try { // 根据调用编号从 FUTURES 集合中查找指定的 DefaultFuture 对象 DefaultFuture future = FUTURES.remove(response.getId()); if (future != null) { // 继续向下调用 future.doReceived(response); } else { logger.warn(&quot;The timeout response finally returned at ...&quot;); } } finally { CHANNELS.remove(response.getId()); } } private void doReceived(Response res) { lock.lock(); try { // 保存响应对象 response = res; if (done != null) { // 唤醒用户线程 done.signal(); } } finally { lock.unlock(); } if (callback != null) { invokeCallback(callback); } }} 以上逻辑是将响应对象保存到相应的 DefaultFuture 实例中，然后再唤醒用户线程，随后用户线程即可从 DefaultFuture 实例中获取到相应结果。","link":"/posts/25919.html"},{"title":"JVM：垃圾回收","text":"虽然 Java 不用“手动管理”内存回收，代码写起来很顺畅。但是你有没有想过，这些内存是怎么被回收的？ 其实，JVM 是有专门的线程在做这件事情。当我们的内存空间达到一定条件时，会自动触发。这个过程就叫作 GC，负责 GC 的组件，就叫作垃圾回收器。 JVM 规范并没有规定垃圾回收器怎么实现，它只需要保证不要把正在使用的对象给回收掉就可以。在现在的服务器环境中，经常被使用的垃圾回收器有 CMS 和 G1，但 JVM 还有其他几个常见的垃圾回收器。 按照语义上的意思，垃圾回收，首先就需要找到这些垃圾，然后回收掉。但是 GC 过程正好相反，它是先找到活跃的对象，然后把其他不活跃的对象判定为垃圾，然后删除。所以垃圾回收只与活跃的对象有关，和堆的大小无关。这个概念是我们一直在强调的，你一定要牢记。 JVM 是如何判断哪些对象应该被回收？ 引用计数器算法 给对象中添加一个引用计数器，每当有一个地方引用它时，计数器值就加1；当引用失效时，计数器值就减1；任何时刻计数器为0的对象就是不可能再被使用的。 主流的Java虚拟机都没有选用引用计数算法来管理内存，其中最主要的原因是它很难解决对象之间相互循环引用的问题。 可达性分析算法 通过一系列的称为“GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链（Reference Chain），当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的，将会被判定为可回收的对象。 如图所示，Obj5、Obj6、Obj7，由于不能和 GC Root 产生关联，发生 GC 时，就会被摧毁。 GC Roots 有哪些GC Roots 是一组必须活跃的引用。用通俗的话来说，就是程序接下来通过直接引用或者间接引用，能够访问到的潜在被使用的对象。 GC Roots 包括： Java 线程中，当前所有正在被调用的方法的引用类型参数、局部变量、临时值等。也就是与我们栈帧相关的各种引用。 所有当前被加载的 Java 类。 Java 类的引用类型静态变量。 运行时常量池里的引用类型常量（String 或 Class 类型）。 JVM 内部数据结构的一些引用，比如 sun.jvm.hotspot.memory.Universe 类。 用于同步的监控对象，比如调用了对象的 wait() 方法。 JNI handles，包括 global handles 和 local handles。 这些 GC Roots 大体可以分为三大类，下面这种说法更加好记一些： 活动线程相关的各种引用。 类的静态变量的引用。 JNI 引用。 引用级别能够找到 Reference Chain 的对象，就一定会存活么？ 对象对于另外一个对象的引用，要看关系牢靠不牢靠，可能在链条的其中一环，就断掉了。 根据发生 GC 时，这条链条的表现，可以对这个引用关系进行更加细致的划分。它们的关系，可以分为强引用、软引用、弱引用、虚引用等。 强引用 Strong references当内存空间不足，系统撑不住了，JVM 就会抛出 OutOfMemoryError 错误。即使程序会异常终止，这种对象也不会被回收。这种引用属于最普通最强硬的一种存在，只有在和 GC Roots 断绝关系时，才会被消灭掉。 这种引用，你每天的编码都在用。例如：new 一个普通的对象。 1Object obj = new Object() 这种方式可能是有问题的。假如你的系统被大量用户（User）访问，你需要记录这个 User 访问的时间。可惜的是，User 对象里并没有这个字段，所以我们决定将这些信息额外开辟一个空间进行存放。 123static Map&lt;User,Long&gt; userVisitMap = new HashMap&lt;&gt;();...userVisitMap.put(user, time); 当你用完了 User 对象，其实你是期望它被回收掉的。但是，由于它被 userVisitMap 引用，我们没有其他手段 remove 掉它。这个时候，就发生了内存泄漏（memory leak）。 这种情况还通常发生在一个没有设定上限的 Cache 系统，由于设置了不正确的引用方式，加上不正确的容量，很容易造成 OOM。 软引用 Soft references软引用用于维护一些可有可无的对象。在内存足够的时候，软引用对象不会被回收，只有在内存不足时，系统则会回收软引用对象，如果回收了软引用对象之后仍然没有足够的内存，才会抛出内存溢出异常。 可以看到，这种特性非常适合用在缓存技术上。比如网页缓存、图片缓存等。 软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收，Java 虚拟机就会把这个软引用加入到与之关联的引用队列中。 软引用需要显式的声明，使用泛型来实现。 123// 伪代码Object object = new Object();SoftReference&lt;Object&gt; softRef = new SoftReference(object); 这里有一个相关的 JVM 参数-XX:SoftRefLRUPolicyMSPerMB=&lt;N&gt;。它的意思是：每 MB 堆空闲空间中 SoftReference 的存活时间。这个值的默认时间是1秒（1000）。 弱引用 Weak references弱引用对象相比较软引用，要更加无用一些，它拥有更短的生命周期。 当 JVM 进行垃圾回收时，无论内存是否充足，都会回收被弱引用关联的对象。弱引用拥有更短的生命周期，在 Java 中，用 java.lang.ref.WeakReference 类来表示。 它的应用场景和软引用类似，可以在一些对内存更加敏感的系统里采用。它的使用方式类似于这段的代码： 123// 伪代码Object object = new Object();WeakReference&lt;Object&gt; softRef = new WeakReference(object); 虚引用 Phantom References这是一种形同虚设的引用，在现实场景中用的不是很多。虚引用必须和引用队列（ReferenceQueue）联合使用。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收。 实际上，虚引用的 get，总是返回 null。 1234Object object = new Object();ReferenceQueue queue = new ReferenceQueue();// 虚引用，必须与一个引用队列关联PhantomReference pr = new PhantomReference(object, queue); 虚引用主要用来跟踪对象被垃圾回收的活动。 当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象之前，把这个虚引用加入到与之关联的引用队列中。 程序如果发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。 回收算法标记（Mark）垃圾回收的第一步，就是找出活跃的对象。反复强调 GC 过程是逆向的。 根据 GC Roots 遍历所有的可达对象，这个过程，就叫作标记。 圆圈代表的是对象。绿色的代表 GC Roots，红色的代表可以追溯到的对象。可以看到标记之后，仍然有多个灰色的圆圈，它们都是被回收的对象。 清除（Sweep）清除阶段就是把未被标记的对象回收掉。 但是这种简单的清除方式，有一个明显的弊端，那就是碎片问题。 比如我申请了 1k、2k、3k、4k、5k 的内存。 由于某种原因 ，2k 和 4k 的内存，我不再使用，就需要交给垃圾回收器回收。 这个时候，我应该有足足 6k 的空闲空间。接下来，我打算申请另外一个 5k 的空间，结果系统告诉我内存不足了。系统运行时间越长，这种碎片就越多。 在很久之前使用 Windows 系统时，有一个非常有用的功能，就是内存整理和磁盘整理，运行之后有可能会显著提高系统性能。这个出发点是一样的。 复制（Copy）解决碎片问题没有银弹，只有老老实实的进行内存整理。有一个比较好的思路可以完成这个整理过程，就是提供一个对等的内存空间，将存活的对象复制过去，然后清除原内存空间。 在程序设计中，一般遇到扩缩容或者碎片整理问题时，复制算法都是非常有效的。比如：HashMap 的扩容也是使用同样的思路，Redis 的 rehash 也是类似的。 整个过程如图所示： 这种方式看似非常完美的，解决了碎片问题。但是，它的弊端也非常明显。它浪费了几乎一半的内存空间来做这个事情，如果资源本来就很有限，这就是一种无法容忍的浪费。 整理（Compact）其实，不用分配一个对等的额外空间，也是可以完成内存的整理工作。你可以把内存想象成一个非常大的数组，根据随机的 index 删除了一些数据。那么对整个数组的清理，其实是不需要另外一个数组来进行支持的，使用程序就可以实现。 它的主要思路，就是移动所有存活的对象，且按照内存地址顺序依次排列，然后将末端内存地址以后的内存全部回收。 目前，JVM 的垃圾回收器，都是对几种朴素算法的发扬光大。简单看一下它们的特点： 复制算法（Copy） 复制算法是所有算法里面效率最高的，缺点是会造成一定的空间浪费。 标记-清除（Mark-Sweep） 效率一般，缺点是会造成内存碎片问题。 标记-整理（Mark-Compact） 效率比前两者要差，但没有空间浪费，也消除了内存碎片问题。 所以，没有最优的算法，只有最合适的算法。 分代JVM 是计算节点，而不是存储节点。最理想的情况，就是对象在用完之后，它的生命周期立马就结束了。而那些被频繁访问的资源，希望它能够常驻在内存里。 研究表明，大部分对象，可以分为两类： 大部分对象的生命周期都很短； 其他对象则很可能会存活很长时间。 大部分死的快，其他的活的长。这个假设我们称之为弱代假设（weak generational hypothesis）。 现在的垃圾回收器，都会在物理上或者逻辑上，把这两类对象进行区分。我们把死的快的对象所占的区域，叫作年轻代（Young generation）。把其他活的长的对象所占的区域，叫作老年代（Old generation/Tenured Generation）。 年轻代年轻代使用的垃圾回收算法是复制算法。因为年轻代发生 GC 后，只会有非常少的对象存活，复制这部分对象是非常高效的。 如上图所示，年轻代分为：一个伊甸园空间（Eden ），两个幸存者空间（Survivor ）。 当年轻代中的 Eden 区分配满的时候，就会触发年轻代的 GC（Minor GC）。具体过程如下： 在 Eden 区执行了第一次 GC 之后，存活的对象会被移动到其中一个 Survivor 分区（以下简称from）； Eden 区再次 GC，这时会采用复制算法，将 Eden 和 from 区一起清理。存活的对象会被复制到 to 区；接下来，只需要清空 from 区就可以了。 所以在这个过程中，总会有一个 Survivor 分区是空置的。Eden、from、to 的默认比例是 8:1:1，所以只会造成 10% 的空间浪费。这个比例，是由参数 -XX:SurvivorRatio 进行配置的（默认为 8）。 老年代老年代一般使用“标记-清除”、“标记-整理”算法，因为老年代的对象存活率一般是比较高的，空间又比较大，拷贝起来并不划算，还不如采取就地收集的方式。 对象是怎么进入老年代的呢？有多种途径。 提升（Promotion） 如果对象够老，会通过“提升”进入老年代。 关于对象老不老，是通过它的年龄（age）来判断的。每当发生一次 Minor GC，存活下来的对象年龄都会加 1。直到达到一定的阈值，就会把这些“老顽固”给提升到老年代。 这些对象如果变的不可达，直到老年代发生 GC 的时候，才会被清理掉。 这个阈值，可以通过参数 ‐XX:+MaxTenuringThreshold 进行配置，最大值是 15，因为它是用 4bit 存储的。 分配担保 看一下年轻代的图，每次存活的对象，都会放入其中一个幸存区，这个区域默认的比例是 10%。但是我们无法保证每次存活的对象都小于 10%，当 Survivor 空间不够，就需要依赖其他内存（指老年代）进行分配担保。这个时候，对象也会直接在老年代上分配。 大对象直接在老年代分配 超出某个大小的对象将直接在老年代分配。这个值是通过参数 -XX:PretenureSizeThreshold 进行配置的。默认为 0，意思是全部首选 Eden 区进行分配。 动态对象年龄判定 有的垃圾回收算法，并不要求 age 必须达到 15 才能晋升到老年代，它会使用一些动态的计算方法。比如，如果幸存区中相同年龄对象大小的和，大于幸存区的一半，大于或等于 age 的对象将会直接进入老年代。 这些动态判定一般不受外部控制，我们知道有这么回事就可以了。通过下图可以看一下一个对象的分配逻辑。 卡片标记（card marking）对象的引用关系是一个巨大的网状。有的对象可能在 Eden 区，有的可能在老年代，那么这种跨代的引用是如何处理的呢？由于 Minor GC 是单独发生的，如果一个老年代的对象引用了它，如何确保能够让年轻代的对象存活呢？ 对于是、否的判断，我们通常都会用 Bitmap（位图）和布隆过滤器来加快搜索的速度。JVM 也是用了类似的方法。其实，老年代是被分成众多的卡页（card page）的（一般数量是 2 的次幂）。 卡表（Card Table）就是用于标记卡页状态的一个集合，每个卡表项对应一个卡页。 如果年轻代有对象分配，而且老年代有对象指向这个新对象， 那么这个老年代对象所对应内存的卡页，就会标识为 dirty，卡表只需要非常小的存储空间就可以保留这些状态。 垃圾回收时，就可以先读这个卡表，进行快速判断。 HotSpot 垃圾回收器每种回收器都有各自的特点。我们在平常的 GC 优化时，一定要搞清楚现在用的是哪种垃圾回收器。 年轻代垃圾回收器 Serial 垃圾收集器 处理 GC 的只有一条线程，并且在垃圾回收的过程中暂停一切用户线程。 可以说是最简单的垃圾回收器，但千万别以为它没有用武之地。因为简单，所以高效，它通常用在客户端应用上。因为客户端应用不会频繁创建很多对象，用户也不会感觉出明显的卡顿。相反，它使用的资源更少，也更轻量级。 ParNew 垃圾收集器 ParNew 是 Serial 的多线程版本。由多条 GC 线程并行地进行垃圾清理。清理过程依然要停止用户线程。 ParNew 追求“低停顿时间”，与 Serial 唯一区别就是使用了多线程进行垃圾收集，在多 CPU 环境下性能比 Serial 会有一定程度的提升；但线程切换需要额外的开销，因此在单 CPU 环境中表现不如 Serial。 3.Parallel Scavenge 垃圾收集器 另一个多线程版本的垃圾回收器。它与 ParNew 的主要区别是： Parallel Scavenge：追求 CPU 吞吐量，能够在较短时间内完成指定任务，适合没有交互的后台计算。弱交互强计算。 ParNew：追求降低用户停顿时间，适合交互式应用。强交互弱计算。 老年代垃圾收集器 Serial Old 垃圾收集器 与年轻代的 Serial 垃圾收集器对应，都是单线程版本，同样适合客户端使用。 年轻代的 Serial，使用复制算法。 老年代的 Old Serial，使用标记-整理算法。 Parallel Old Parallel Old 收集器是 Parallel Scavenge 的老年代版本，追求 CPU 吞吐量。 CMS 垃圾收集器 CMS（Concurrent Mark Sweep）收集器是以获取最短 GC 停顿时间为目标的收集器，它在垃圾收集时使得用户线程和 GC 线程能够并发执行，因此在垃圾收集过程中用户也不会感到明显的卡顿。 配置参数除了上面几个垃圾回收器，还有 G1、ZGC 等更加高级的垃圾回收器，它们都有专门的配置参数来使其生效。 通过 -XX:+PrintCommandLineFlags 参数，可以查看当前 Java 版本默认使用的垃圾回收器。 以下是一些配置参数： -XX:+UseSerialGC 年轻代和老年代都用串行收集器 -XX:+UseParNewGC 年轻代使用 ParNew，老年代使用 Serial Old -XX:+UseParallelGC 年轻代使用 ParallerGC，老年代使用 Serial Old -XX:+UseParallelOldGC 新生代和老年代都使用并行收集器 -XX:+UseConcMarkSweepGC，表示年轻代使用 ParNew，老年代的用 CMS -XX:+UseG1GC 使用 G1垃圾回收器 -XX:+UseZGC 使用 ZGC 垃圾回收器 注意 -XX:+UseParNewGC 这个参数，已经在 Java9 中就被抛弃了。很多程序（比如 ES）会报这个错误，不要感到奇怪。 CMSCMS 的全称是 Mostly Concurrent Mark and Sweep Garbage Collector（主要并发­标记­清除­垃圾收集器），它在年轻代使用复制算法，而对老年代使用标记-清除算法。你可以看到，在老年代阶段，比起 Mark-Sweep，它多了一个并发字样。 CMS 的设计目标，是避免在老年代 GC 时出现长时间的卡顿（但它并不是一个老年代回收器）。如果你不希望有长时间的停顿，同时你的 CPU 资源也比较丰富，使用 CMS 是比较合适的。 CMS垃圾回收过程初始标记（Initial Mark）初始标记阶段，只标记直接关联 GC root 的对象，不用向下追溯。因为最耗时的就在 tracing 阶段，这样就极大地缩短了初始标记时间。 这个过程是 STW 的，但由于只是标记第一层，所以速度是很快的。 注意，这里除了要标记相关的 GC Roots 之外，还要标记年轻代中对象的引用，这也是 CMS 老年代回收，依然要扫描新生代的原因。 并发标记（Concurrent Mark）在初始标记的基础上，进行并发标记。这一步骤主要是 tracinng 的过程，用于标记所有可达的对象。 这个过程会持续比较长的时间，但却可以和用户线程并行。在这个阶段的执行过程中，可能会产生很多变化： 有些对象，从新生代晋升到了老年代； 有些对象，直接分配到了老年代； 老年代或者新生代的对象引用发生了变化。 在这个阶段受到影响的老年代对象所对应的卡页，会被标记为 dirty，用于后续重新标记阶段的扫描。 并发预清理（Concurrent Preclean）并发预清理也是不需要 STW 的，目的是为了让重新标记阶段的 STW 尽可能短。这个时候，老年代中被标记为 dirty 的卡页中的对象，就会被重新标记，然后清除掉 dirty 的状态。 由于这个阶段也是可以并发的，在执行过程中引用关系依然会发生一些变化。我们可以假定这个清理动作是第一次清理。 所以重新标记阶段，有可能还会有处于 dirty 状态的卡页。 并发可取消的预清理（Concurrent Abortable Preclean）因为重新标记是需要 STW 的，所以会有很多次预清理动作。并发可取消的预清理，顾名思义，在满足某些条件的时候，可以终止，比如迭代次数、有用工作量、消耗的系统时间等。 这个阶段是可选的。换句话说，这个阶段是“并发预清理”阶段的一种优化。 这个阶段的第一个意图，是避免回扫年轻代的大量对象；另外一个意图，就是当满足最终标记的条件时，自动退出。 标记动作是需要扫描年轻代的。如果年轻代的对象太多，肯定会严重影响标记的时间。如果在此之前能够进行一次 Minor GC，情况会不会变得好了许多？ CMS 提供了参数 CMSScavengeBeforeRemark，可以在进入重新标记之前强制进行一次 Minor GC。 但请你记住一件事情，GC 的停顿是不分什么年轻代老年代的。设置了上面的参数，可能会在一个比较长的 Minor GC 之后，紧跟着一个 CMS 的 Remark，它们都是 STW 的。 最终标记（Final Remark）通常 CMS 会尝试在年轻代尽可能空的情况下运行 Final Remark 阶段，以免接连多次发生 STW 事件。 这是 CMS 垃圾回收阶段的第二次 STW 阶段，目标是完成老年代中所有存活对象的标记。前面多轮的 preclean 阶段，一直在和应用线程玩追赶游戏，有可能跟不上引用的变化速度。本轮的标记动作就需要 STW 来处理这些情况。 如果预处理阶段做的不够好，会显著增加本阶段的 STW 时间。你可以看到，CMS 垃圾回收器把回收过程分了多个部分，而影响最大的不是 STW 阶段本身，而是它之前的预处理动作。 并发清除（Concurrent Sweep）此阶段用户线程被重新激活，目标是删掉不可达的对象，并回收它们的空间。 由于 CMS 并发清理阶段用户线程还在运行中，伴随程序运行自然就还会有新的垃圾不断产生，这一部分垃圾出现在标记过程之后，CMS 无法在当次 GC 中处理掉它们，只好留待下一次 GC 时再清理掉。这一部分垃圾就称为“浮动垃圾”。 内存碎片由于 CMS 在执行过程中，用户线程还需要运行，那就需要保证有充足的内存空间供用户使用。如果等到老年代空间快满了，再开启这个回收过程，用户线程可能会产生“Concurrent Mode Failure”的错误，这时会临时启用 Serial Old 收集器来重新进行老年代的垃圾收集，这样停顿时间就很长了（STW）。 这部分空间预留，一般在 30% 左右即可，那么能用的大概只有 70%。参数 -XX:CMSInitiatingOccupancyFraction 用来配置这个比例（记得要首先开启参数UseCMSInitiatingOccupancyOnly）。也就是说，当老年代的使用率达到 70%，就会触发 GC 了。如果你的系统老年代增长不是太快，可以调高这个参数，降低内存回收的次数。这个比率非常不好设置。一般在堆大小小于 2GB 的时候，都不会考虑 CMS 垃圾回收器。 另外，CMS 对老年代回收的时候，并没有内存的整理阶段。这就造成程序在长时间运行之后，碎片太多。如果你申请一个稍大的对象，就会引起分配失败。 CMS 提供了两个参数来解决这个问题： （1） UseCMSCompactAtFullCollection（默认开启），表示在要进行 Full GC 的时候，进行内存碎片整理。内存整理的过程是无法并发的，所以停顿时间会变长。 （2）CMSFullGCsBeforeCompaction，每隔多少次不压缩的 Full GC 后，执行一次带压缩的 Full GC。默认值为 0，表示每次进入 Full GC 时都进行碎片整理。 所以，预留空间加上内存的碎片，使用 CMS 垃圾回收器的老年代，留给我们的空间就不是太多，这也是 CMS 的一个弱点。 G1G1 的全称是 Garbage­First GC， G1分代其他的回收器，都是对某个年代的整体收集，收集时间上自然不好控制。G1 把堆切成了很多份，把每一份当作一个小目标，部分上目标很容易达成。 如图所示，G1 也是有 Eden 区和 Survivor 区的概念的，只不过它们在内存上不是连续的，而是由一小份一小份组成的。 这一小份区域的大小是固定的，名字叫作小堆区（Region）。小堆区可以是 Eden 区，也可以是 Survivor 区，还可以是 Old 区。所以 G1 的年轻代和老年代的概念都是逻辑上的。每一块 Region，大小都是一致的，它的数值是在 1M 到 32M 字节之间的一个 2 的幂值数。 但假如我的对象太大，一个 Region 放不下了怎么办？注意图中有一块面积很大的黄色区域，它的名字叫作 Humongous Region，大小超过 Region 50% 的对象，将会在这里分配。 Region 的大小，可以通过参数进行设置： -XX:G1HeapRegionSize=&lt;N&gt;M 那么，回收的时候，到底回收哪些小堆区呢？是随机的么？ 这当然不是。事实上，垃圾最多的小堆区，会被优先收集。这就是 G1 名字的由来。 G1垃圾回收过程在逻辑上，G1 分为年轻代和老年代，但它的年轻代和老年代比例，并不是那么“固定”，为了达到 MaxGCPauseMillis 所规定的效果，G1 会自动调整两者之间的比例。 如果你强行使用 -Xmn 或者 -XX:NewRatio 去设定它们的比例的话，我们给 G1 设定的这个目标将会失效。 G1 的回收过程主要分为 3 类： （1）G1“年轻代”的垃圾回收，同样叫 Minor GC，发生时机就是 Eden 区满的时候。 （2）老年代的垃圾收集，严格上来说其实不算是收集，它是一个“并发标记”的过程，顺便清理了一点点对象。 （3）真正的清理，发生在“混合模式”，它不止清理年轻代，还会将老年代的一部分区域进行清理。 在 GC 日志里，这个过程描述特别有意思，（1）的过程，叫作 [GC pause (G1 Evacuation Pause) (young)，而（2）的过程，叫作 [GC pause (G1 Evacuation Pause) (mixed)。Evacuation 是转移的意思，和 Copy 的意思有点类似。 这三种模式之间的间隔也是不固定的。比如，1 次 Minor GC 后，发生了一次并发标记，接着发生了 9 次 Mixed GC。 RSetRSet 全称是 Remembered Set，是一个空间换时间的数据结构（与Card Table功能类似），用于记录和维护 Region 之间的对象引用关系。 但 RSet 与 Card Table 有些不同的地方。Card Table 是一种 points-out（我引用了谁的对象）的结构。而 RSet 记录了其他 Region 中的对象引用本 Region 中对象的关系，属于 points-into 结构（谁引用了我的对象），有点倒排索引的味道。 你可以把 RSet 理解成一个 Hash，key 是引用的 Region 地址，value 是引用它的对象的卡页集合。 对于年轻代的 Region，它的 RSet 只保存了来自老年代的引用，这是因为年轻代的回收是针对所有年轻代 Region 的，没必要画蛇添足。所以说年轻代 Region 的 RSet 有可能是空的。 而对于老年代的 Region 来说，它的 RSet 也只会保存老年代对它的引用。这是因为老年代回收之前，会先对年轻代进行回收。这时，Eden 区变空了，而在回收过程中会扫描 Survivor 分区，所以也没必要保存来自年轻代的引用。 RSet 通常会占用很大的空间，大约 5% 或者更高。不仅仅是空间方面，很多计算开销也是比较大的。 事实上，为了维护 RSet，程序运行的过程中，写入某个字段就会产生一个 post-write barrier 。为了减少这个开销，将内容放入 RSet 的过程是异步的，而且经过了很多的优化：Write Barrier 把脏卡信息存放到本地缓冲区（local buffer），有专门的 GC 线程负责收集，并将相关信息传给被引用 Region 的 RSet。 参数 -XX:G1ConcRefinementThreads 或者 -XX:ParallelGCThreads 可以控制这个异步的过程。如果并发优化线程跟不上缓冲区的速度，就会在用户进程上完成。 年轻代回收年轻代回收是一个 STW 的过程，它的跨代引用使用 RSet 数据结构来追溯，会一次性回收掉年轻代的所有 Region。 JVM 启动时，G1 会先准备好 Eden 区，程序在运行过程中不断创建对象到 Eden 区，当所有的 Eden 区都满了，G1 会启动一次年轻代垃圾回收过程。 年轻代的收集包括下面的回收阶段： 扫描根 根，可以看作是我们前面介绍的 GC Roots，加上 RSet 记录的其他 Region 的外部引用。 更新 RS 处理 dirty card queue 中的卡页，更新 RSet。此阶段完成后，RSet 可以准确的反映老年代对所在的内存分段中对象的引用。可以看作是第一步的补充。 处理 RS 识别被老年代对象指向的 Eden 中的对象，这些被指向的 Eden 中的对象被认为是存活的对象。 复制对象 没错，收集算法依然使用的是 Copy 算法。 在这个阶段，对象树被遍历，Eden 区内存段中存活的对象会被复制到 Survivor 区中空的 Region。这个过程和其他垃圾回收算法一样，包括对象的年龄和晋升，无需做过多介绍。 处理引用 处理 Soft、Weak、Phantom、Final、JNI Weak 等引用。结束收集。 并发标记（Concurrent Marking）当整个堆内存使用达到一定比例（默认是 45%），并发标记阶段就会被启动。这个比例也是可以调整的，通过参数 -XX:InitiatingHeapOccupancyPercent 进行配置。 Concurrent Marking 是为 Mixed GC 提供标记服务的，并不是一次 GC 过程的一个必须环节。这个过程和 CMS 垃圾回收器的回收过程非常类似，具体标记过程如下： 初始标记（Initial Mark） 这个过程共用了 Minor GC 的暂停，这是因为它们可以复用 root scan 操作。虽然是 STW 的，但是时间通常非常短。 Root 区扫描（Root Region Scan） 并发标记（ Concurrent Mark） 这个阶段从 GC Roots 开始对 heap 中的对象标记，标记线程与应用程序线程并行执行，并且收集各个 Region 的存活对象信息。 重新标记（Remaking） 和 CMS 类似，也是 STW 的。标记那些在并发标记阶段发生变化的对象。 清理阶段（Cleanup） 这个过程不需要 STW。如果发现 Region 里全是垃圾，在这个阶段会立马被清除掉。不全是垃圾的 Region，并不会被立马处理，它会在 Mixed GC 阶段，进行收集。 如果在并发标记阶段，又有新的对象变化，该怎么办？ 这是由算法 SATB 保证的。SATB 的全称是 Snapshot At The Beginning，它作用是保证在并发标记阶段的正确性。 这个快照是逻辑上的，主要是有几个指针，将 Region 分成个多个区段。如图所示，并发标记期间分配的对象，都会在 next TAMS 和 top 之间。 混合回收（Mixed GC）能并发清理老年代中的整个整个的小堆区是一种最优情形。混合收集过程，不只清理年轻代，还会将一部分老年代区域也加入到 CSet 中。 通过 Concurrent Marking 阶段，已经统计了老年代的垃圾占比。在 Minor GC 之后，如果判断这个占比达到了某个阈值，下次就会触发 Mixed GC。这个阈值，由 -XX:G1HeapWastePercent 参数进行设置（默认是堆大小的 5%）。因为这种情况下， GC 会花费很多的时间但是回收到的内存却很少。所以这个参数也是可以调整 Mixed GC 的频率的。 还有参数 G1MixedGCCountTarget，用于控制一次并发标记之后，最多执行 Mixed GC 的次数。","link":"/posts/27028.html"},{"title":"Netty：基础与入门","text":"为什么选择 Netty？Netty 是一款用于高效开发网络应用的 NIO 网络框架，它大大简化了网络应用的开发过程。 既然 Netty 是网络应用框架，那我们永远绕不开以下几个核心关注点： I/O 模型、线程模型和事件处理机制； 易用性 API 接口； 对数据协议、序列化的支持。 之所以会最终选择 Netty，是因为 Netty 围绕这些核心要点可以做到尽善尽美，其健壮性、性能、可扩展性在同领域的框架中都首屈一指。 高性能，低延迟I/O 请求可以分为两个阶段，分别为调用阶段和执行阶段。 第一个阶段为I/O 调用阶段，即用户进程向内核发起系统调用。 第二个阶段为I/O 执行阶段。此时，内核等待 I/O 请求处理完成返回。该阶段分为两个过程：首先等待数据就绪，并写入内核缓冲区；随后将内核缓冲区数据拷贝至用户态缓冲区。 Netty 的 I/O 模型是基于非阻塞 I/O 实现的，底层依赖的是 JDK NIO 框架的多路复用器 Selector。一个多路复用器 Selector 可以同时轮询多个 Channel，采用 epoll 模式后，只需要一个线程负责 Selector 的轮询，就可以接入成千上万的客户端。 在 I/O 多路复用的场景下，当有数据处于就绪状态后，需要一个事件分发器（Event Dispather），它负责将读写事件分发给对应的读写事件处理器（Event Handler）。 事件分发器有两种设计模式：Reactor 和 Proactor，Reactor 采用同步 I/O， Proactor 采用异步 I/O。 Reactor 实现相对简单，适合处理耗时短的场景，对于耗时长的 I/O 操作容易造成阻塞。Proactor 性能更高，但是实现逻辑非常复杂，目前主流的事件驱动模型还是依赖 select 或 epoll 来实现。 上图所描述的便是 Netty 所采用的主从 Reactor 多线程模型，所有的 I/O 事件都注册到一个 I/O 多路复用器上，当有 I/O 事件准备就绪后，I/O 多路复用器会将该 I/O 事件通过事件分发器分发到对应的事件处理器中。该线程模型避免了同步问题以及多线程切换带来的资源开销，真正做到高性能、低延迟。 完美弥补 Java NIO 的缺陷在 JDK 1.4 投入使用之前，只有 BIO 一种模式。开发过程相对简单。新来一个连接就会创建一个新的线程处理。随着请求并发度的提升，BIO 很快遇到了性能瓶颈。JDK 1.4 以后开始引入了 NIO 技术，支持 select 和 poll；JDK 1.5 支持了 epoll；JDK 1.7 发布了 NIO2，支持 AIO 模型。Java 在网络领域取得了长足的进步。 Netty 相比 JDK NIO 有哪些突出的优势？ 易用性 使用 JDK NIO 编程需要了解很多复杂的概念，比如 Channels、Selectors、Sockets、Buffers 等，编码复杂程度令人发指。相反，Netty 在 NIO 基础上进行了更高层次的封装，屏蔽了 NIO 的复杂性；Netty 封装了更加人性化的 API，统一的 API（阻塞/非阻塞） 大大降低了开发者的上手难度；与此同时，Netty 提供了很多开箱即用的工具，例如常用的行解码器、长度域解码器等，而这些在 JDK NIO 中都需要你自己实现。 稳定性 Netty 更加可靠稳定，修复和完善了 JDK NIO 较多已知问题，例如臭名昭著的 select 空转导致 CPU 消耗 100%，TCP 断线重连，keep-alive 检测等问题。 可扩展性 Netty 的可扩展性在很多地方都有体现，这里我主要列举其中的两点：一个是可定制化的线程模型，用户可以通过启动的配置参数选择 Reactor 线程模型；另一个是可扩展的事件驱动模型，将框架层和业务层的关注点分离。大部分情况下，开发者只需要关注 ChannelHandler 的业务逻辑实现。 更低的资源消耗作为网络通信框架，需要处理海量的网络数据，那么必然面临有大量的网络对象需要创建和销毁的问题，对于 JVM GC 并不友好。为了降低 JVM 垃圾回收的压力，Netty 主要采用了两种优化手段： 对象池复用技术。 Netty 通过复用对象，避免频繁创建和销毁带来的开销。 零拷贝技术。 除了操作系统级别的零拷贝技术外，Netty 提供了更多面向用户态的零拷贝技术，例如 Netty 在 I/O 读写时直接使用 DirectBuffer，从而避免了数据在堆内存和堆外内存之间的拷贝 Netty 整体结构Netty 是一个设计非常用心的网络基础组件，Netty 官网给出了有关 Netty 的整体功能模块结构如下： Core 核心层 Core 核心层是 Netty 最精华的内容，它提供了底层网络通信的通用抽象和实现，包括可扩展的事件模型、通用的通信 API、支持零拷贝的 ByteBuf 等。 Protocol Support 协议支持层 协议支持层基本上覆盖了主流协议的编解码实现，如 HTTP、SSL、Protobuf、压缩、大文件传输、WebSocket、文本、二进制等主流协议，此外 Netty 还支持自定义应用层协议。 Netty 丰富的协议支持降低了用户的开发成本，基于 Netty 我们可以快速开发 HTTP、WebSocket 等服务。 Transport Service 传输服务层 传输服务层提供了网络传输能力的定义和实现方法。它支持 Socket、HTTP 隧道、虚拟机管道等传输方式。 Netty 对 TCP、UDP 等数据传输做了抽象和封装，用户可以更聚焦在业务逻辑实现上，而不必关系底层数据传输的细节。 Netty 逻辑架构Netty 的逻辑处理架构为典型网络分层架构设计，共分为网络通信层、事件调度层、服务编排层，每一层各司其职。如下图所示： 网络通信层网络通信层的职责是执行网络 I/O 的操作。它支持多种网络协议和 I/O 模型的连接操作。当网络数据读取到内核缓冲区后，会触发各种网络事件，这些网络事件会分发给事件调度层进行处理。 网络通信层的核心组件包含BootStrap、ServerBootStrap、Channel三个组件。 BootStrap &amp; ServerBootStrapBootstrap 是“引导”的意思，它主要负责整个 Netty 程序的启动、初始化、服务器连接等过程，它相当于一条主线，串联了 Netty 的其他核心组件。 Netty 中的引导器共分为两种类型：一个为用于客户端引导的 Bootstrap，另一个为用于服务端引导的 ServerBootStrap，它们都继承自抽象类 AbstractBootstrap。 Bootstrap 和 ServerBootStrap 十分相似，两者非常重要的区别在于 Bootstrap 可用于连接远端服务器，只绑定一个 EventLoopGroup。而 ServerBootStrap 则用于服务端启动绑定本地端口，会绑定两个 EventLoopGroup，这两个 EventLoopGroup 通常称为 Boss 和 Worker。 ServerBootStrap 中的 Boss 和 Worker 是什么角色呢？它们之间又是什么关系？ Boss 和 Worker 可以理解为“老板”和“员工”的关系。每个服务器中都会有一个 Boss，也会有一群做事情的 Worker。Boss 会不停地接收新的连接，然后将连接分配给一个个 Worker 处理连接。 ChannelChannel 的字面意思是“通道”，它是网络通信的载体。Channel提供了基本的 API 用于网络 I/O 操作，如 register、bind、connect、read、write、flush 等。Netty实现的 Channel 是以 JDK NIO Channel 为基础的，相比较于 JDK NIO，Netty 的 Channel 提供了更高层次的抽象，同时屏蔽了底层 Socket 的复杂性，赋予了 Channel 更加强大的功能，你在使用 Netty 时基本不需要再与 Java Socket 类直接打交道。 AbstractChannel是整个家族的基类，派生出 AbstractNioChannel、AbstractOioChannel、AbstractEpollChannel 等子类，每一种都代表了不同的 I/O 模型和协议类型。常用的 Channel 实现类有： NioServerSocketChannel 异步 TCP 服务端。 NioSocketChannel 异步 TCP 客户端。 OioServerSocketChannel 同步 TCP 服务端。 OioSocketChannel 同步 TCP 客户端。 NioDatagramChannel 异步 UDP 连接。 OioDatagramChannel 同步 UDP 连接。 随着状态的变化，Channel 处于不同的生命周期，每一种状态都会绑定相应的事件回调。 Channel 最常见的状态所对应的事件回调如下表所示： 事件 说明 channelRegistered Channel 创建后被注册到 EventLoop 上 channelUnregistered Channel 创建后未注册或者从 EventLoop 取消注册 channelActive Channel 处于就绪状态，可以被读写 channelInactive Channel 处于非就绪状态 channelRead Channel 可以从远端读取到数据 channelReadComplete Channel 读取数据完成 事件调度层事件调度层的职责是通过 Reactor 线程模型对各类事件进行聚合处理，通过 Selector 主循环线程集成多种事件（ I/O 事件、信号事件、定时事件等），实际的业务处理逻辑是交由服务编排层中相关的 Handler 完成。 事件调度层的核心组件包括 EventLoopGroup、EventLoop。 EventLoopGroup &amp; EventLoopEventLoopGroup 本质是一个线程池，主要负责接收 I/O 请求，并分配线程执行处理请求。在下图中，我为你讲述了 EventLoopGroups、EventLoop 与 Channel 的关系。 一个 EventLoopGroup 往往包含一个或者多个 EventLoop。EventLoop 用于处理 Channel 生命周期内的所有 I/O 事件，如 accept、connect、read、write 等 I/O 事件。 EventLoop 同一时间会与一个线程绑定，每个 EventLoop 负责处理多个 Channel。 每新建一个 Channel，EventLoopGroup 会选择一个 EventLoop 与其绑定。该 Channel 在生命周期内都可以对 EventLoop 进行多次绑定和解绑。 Netty 提供了EventLoopGroup 的多种实现，如下图所示： EventLoop 则是 EventLoopGroup 的子接口，所以也可以把 EventLoop 理解为 EventLoopGroup，但是它只包含一个 EventLoop 。 EventLoopGroup 的实现类是 NioEventLoopGroup，NioEventLoopGroup 也是 Netty 中最被推荐使用的线程模型。 NioEventLoopGroup 继承于 MultithreadEventLoopGroup，是基于 NIO 模型开发的，可以把 NioEventLoopGroup 理解为一个线程池，每个线程负责处理多个 Channel，而同一个 Channel 只会对应一个线程。 EventLoopGroup 是 Netty 的核心处理引擎，那么 EventLoopGroup 和 Reactor 线程模型到底是什么关系呢？ 其实 EventLoopGroup 是 Netty Reactor 线程模型的具体实现方式，Netty 通过创建不同的 EventLoopGroup 参数配置，就可以支持 Reactor 的三种线程模型： 单线程模型：EventLoopGroup 只包含一个 EventLoop，Boss 和 Worker 使用同一个EventLoopGroup； 多线程模型：EventLoopGroup 包含多个 EventLoop，Boss 和 Worker 使用同一个EventLoopGroup； 主从多线程模型：EventLoopGroup 包含多个 EventLoop，Boss 是主 Reactor，Worker 是从 Reactor，它们分别使用不同的 EventLoopGroup，主 Reactor 负责新的网络连接 Channel 创建，然后把 Channel 注册到从 Reactor。 服务编排层服务编排层的职责是负责组装各类服务，它是 Netty 的核心处理链，用以实现网络事件的动态编排和有序传播。 服务编排层的核心组件包括 ChannelPipeline、ChannelHandler、ChannelHandlerContext。 ChannelPipelineChannelPipeline 是 Netty 的核心编排组件，负责组装各种 ChannelHandler，实际数据的编解码以及加工处理操作都是由 ChannelHandler 完成的。 ChannelPipeline 是线程安全的，因为每一个新的 Channel 都会对应绑定一个新的 ChannelPipeline。一个 ChannelPipeline 关联一个 EventLoop，一个 EventLoop 仅会绑定一个线程。 ChannelPipeline 可以理解为ChannelHandler 的实例列表——内部通过双向链表将不同的 ChannelHandler 链接在一起。 当 I/O 读写事件触发时，ChannelPipeline 会依次调用 ChannelHandler 列表对 Channel 的数据进行拦截和处理。 ChannelPipeline 中包含入站 ChannelInboundHandler 和出站 ChannelOutboundHandler 两种处理器，我们结合客户端和服务端的数据收发流程来理解 Netty 的这两个概念。 客户端和服务端都有各自的 ChannelPipeline。以客户端为例，数据从客户端发向服务端，该过程称为出站，反之则称为入站。数据入站会由一系列 InBoundHandler 处理，然后再以相反方向的 OutBoundHandler 处理后完成出站。 经常使用的编码 Encoder 是出站操作，解码 Decoder 是入站操作。服务端接收到客户端数据后，需要先经过 Decoder入站处理后，再通过 Encoder 出站通知客户端。 所以客户端和服务端一次完整的请求应答过程可以分为三个步骤：客户端出站（请求数据）、服务端入站（解析数据并执行业务逻辑）、服务端出站（响应结果）。 ChannelHandler &amp; ChannelHandlerContext数据的编解码工作以及其他转换工作实际都是通过 ChannelHandler处理的。站在开发者的角度，最需要关注的就是 ChannelHandler，我们很少会直接操作 Channel，都是通过 ChannelHandler 间接完成。 Channel 与 ChannelPipeline 的关系如下图所示： 每创建一个 Channel 都会绑定一个新的 ChannelPipeline，ChannelPipeline 中每加入一个 ChannelHandler 都会绑定一个 ChannelHandlerContext。 ChannelHandlerContext 用于保存 ChannelHandler 上下文，通过 ChannelHandlerContext 我们可以知道 ChannelPipeline 和 ChannelHandler 的关联关系。 ChannelHandlerContext 可以实现 ChannelHandler 之间的交互，ChannelHandlerContext 包含了 ChannelHandler 生命周期的所有事件，如 connect、bind、read、flush、write、close 等。 此外，你可以试想这样一个场景，如果每个 ChannelHandler 都有一些通用的逻辑需要实现，没有 ChannelHandlerContext 这层模型抽象，你是不是需要写很多相同的代码呢？ 组件关系梳理当你了解每个 Netty 核心组件的概念后。你会好奇这些组件之间如何协作？结合客户端和服务端的交互流程，为你完整地梳理一遍 Netty 内部逻辑的流转。 服务端启动初始化时有 Boss EventLoopGroup 和 Worker EventLoopGroup 两个组件，其中 Boss 负责监听网络连接事件。当有新的网络连接事件到达时，则将 Channel 注册到 Worker EventLoopGroup。 Worker EventLoopGroup 会被分配一个 EventLoop 负责处理该 Channel 的读写事件。每个 EventLoop 都是单线程的，通过 Selector 进行事件循环。 当客户端发起 I/O 读写事件时，服务端 EventLoop 会进行数据的读取，然后通过 Pipeline 触发各种监听器进行数据的加工处理。 客户端数据会被传递到 ChannelPipeline 的第一个 ChannelInboundHandler 中，数据处理完成后，将加工完成的数据传递给下一个 ChannelInboundHandler。 当数据写回客户端时，会将处理结果在 ChannelPipeline 的 ChannelOutboundHandler 中传播，最后到达客户端。 引导器Bootstrap 作为整个 Netty 客户端和服务端的程序入口，可以把 Netty 的核心组件像搭积木一样组装在一起。先使用Netty实现一个简单的HTTP的服务器，然后代码进行讲解。代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778// 服务器启动类public class HttpServer { public void start(int port) throws Exception { EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); try { ServerBootstrap b = new ServerBootstrap(); b.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .localAddress(new InetSocketAddress(port)) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() { @Override public void initChannel(SocketChannel ch) { ch.pipeline() .addLast(&quot;codec&quot;, new HttpServerCodec()) // HTTP 编解码 .addLast(&quot;compressor&quot;, new HttpContentCompressor()) // HttpContent 压缩 .addLast(&quot;aggregator&quot;, new HttpObjectAggregator(65536)) // HTTP 消息聚合 .addLast(&quot;handler&quot;, new HttpServerHandler()); // 自定义业务逻辑处理器 } }) .childOption(ChannelOption.SO_KEEPALIVE, true); ChannelFuture f = b.bind().sync(); System.out.println(&quot;Http Server started， Listening on &quot; + port); f.channel().closeFuture().sync(); } finally { workerGroup.shutdownGracefully(); bossGroup.shutdownGracefully(); } } public static void main(String[] args) throws Exception { new HttpServer().start(8088); }}// 服务端业务逻辑处理类public class HttpServerHandler extends SimpleChannelInboundHandler&lt;FullHttpRequest&gt; { @Override protected void channelRead0(ChannelHandlerContext ctx, FullHttpRequest msg) { String content = String.format(&quot;Receive http request, uri: %s, method: %s, content: %s%n&quot;, msg.uri(), msg.method(), msg.content().toString(CharsetUtil.UTF_8)); FullHttpResponse response = new DefaultFullHttpResponse( HttpVersion.HTTP_1_1, HttpResponseStatus.OK, Unpooled.wrappedBuffer(content.getBytes())); ctx.writeAndFlush(response).addListener(ChannelFutureListener.CLOSE); }} 通过上面两个类，我们可以完成 HTTP 服务器最基本的请求-响应流程，测试步骤如下： 启动 HttpServer 的 main 函数。 终端或浏览器发起 HTTP 请求。 引导器实践指南配置线程池Netty 是采用 Reactor 模型进行开发的，可以非常容易切换三种 Reactor 模式：单线程模式、多线程模式、主从多线程模式。 单线程模式Reactor 单线程模型所有 I/O 操作都由一个线程完成，所以只需要启动一个 EventLoopGroup 即可。 12345EventLoopGroup group = new NioEventLoopGroup(1);ServerBootstrap b = new ServerBootstrap();b.group(group) 多线程模式Reactor 单线程模型有非常严重的性能瓶颈，因此 Reactor 多线程模型出现了。在 Netty 中使用 Reactor 多线程模型与单线程模型非常相似，区别是 NioEventLoopGroup 可以不需要任何参数，它默认会启动 2 倍 CPU 核数的线程。当然，你也可以自己手动设置固定的线程数。 12345EventLoopGroup group = new NioEventLoopGroup();ServerBootstrap b = new ServerBootstrap();b.group(group) 主从多线程模式在大多数场景下，我们采用的都是主从多线程 Reactor 模型。Boss 是主 Reactor，Worker 是从 Reactor。它们分别使用不同的 NioEventLoopGroup，主 Reactor 负责处理 Accept，然后把 Channel 注册到从 Reactor 上，从 Reactor 主要负责 Channel 生命周期内的所有 I/O 事件。 1234EventLoopGroup bossGroup = new NioEventLoopGroup();EventLoopGroup workerGroup = new NioEventLoopGroup();ServerBootstrap b = new ServerBootstrap();b.group(bossGroup, workerGroup) 从上述三种 Reactor 线程模型的配置方法可以看出：Netty 线程模型的可定制化程度很高。它只需要简单配置不同的参数，便可启用不同的 Reactor 线程模型，而且无需变更其他的代码，很大程度上降低了用户开发和调试的成本。 Channel 初始化设置 Channel 类型NIO 模型是 Netty 中最成熟且被广泛使用的模型。因此，推荐 Netty 服务端采用 NioServerSocketChannel 作为 Channel 的类型，客户端采用 NioSocketChannel。设置方式如下： 1b.channel(NioServerSocketChannel.class); 当然，Netty 提供了多种类型的 Channel 实现类，你可以按需切换，例如 OioServerSocketChannel、EpollServerSocketChannel 等。 注册 ChannelHandler在 Netty 中可以通过 ChannelPipeline 去注册多个 ChannelHandler，每个 ChannelHandler 各司其职，这样就可以实现最大化的代码复用，充分体现了 Netty 设计的优雅之处。 那么如何通过引导器添加多个 ChannelHandler 呢？其实很简单，我们看下 HTTP 服务器代码示例： 12345678910111213141516171819b.childHandler(new ChannelInitializer&lt;SocketChannel&gt;() { @Override public void initChannel(SocketChannel ch) { ch.pipeline() .addLast(&quot;codec&quot;, new HttpServerCodec()) .addLast(&quot;compressor&quot;, new HttpContentCompressor()) .addLast(&quot;aggregator&quot;, new HttpObjectAggregator(65536)) .addLast(&quot;handler&quot;, new HttpServerHandler()); }}) ServerBootstrap 的 childHandler() 方法需要注册一个 ChannelHandler。ChannelInitializer是实现了 ChannelHandler接口的匿名类，通过实例化 ChannelInitializer 作为 ServerBootstrap 的参数。 Channel 初始化时都会绑定一个 Pipeline，Pipeline 管理了多个 ChannelHandler，它主要用于服务编排。I/O 事件依次在 ChannelHandler 中传播，ChannelHandler 负责业务逻辑处理。 上述 HTTP 服务器示例中使用链式的方式加载了多个 ChannelHandler，包含HTTP 编解码处理器、HTTPContent 压缩处理器、HTTP 消息聚合处理器、自定义业务逻辑处理器。 常用的参数设置如下： 参数 含义 SO_KEEPALIVE 设置为 true 代表启用了 TCP SO_KEEPALIVE 属性，TCP 会主动探测连接状态，即连接保活 SO_BACKLOG 已完成三次握手的请求队列最大长度，同一时刻服务端可能会处理多个连接，在高并发海量连接的场景下，该参数应适当调大 TCP_NODELAY Netty 默认是 true，表示立即发送数据。如果设置为 false 表示启用 Nagle 算法，该算法会将 TCP 网络数据包累积到一定量才会发送，虽然可以减少报文发送的数量，但是会造成一定的数据延迟。Netty 为了最小化数据传输的延迟，默认禁用了 Nagle 算法 SO_SNDBUF TCP 数据发送缓冲区大小 SO_RCVBUF TCP数据接收缓冲区大小，TCP数据接收缓冲区大小 SO_LINGER 设置延迟关闭的时间，等待缓冲区中的数据发送完成 CONNECT_TIMEOUT_MILLIS 建立连接的超时时间 设置 Channel 参数Netty 提供了十分便捷的方法，用于设置 Channel 参数。关于 Channel 的参数数量非常多，如果每个参数都需要自己设置，那会非常繁琐。幸运的是 Netty 提供了默认参数设置，实际场景下默认参数已经满足我们的需求，我们仅需要修改自己关心的参数即可。 1b.option(ChannelOption.SO_KEEPALIVE, true); ServerBootstrap 设置 Channel 属性有option和childOption两个方法，option 主要负责设置 Boss 线程组，而 childOption 对应的是 Worker 线程组。 端口绑定在完成上述 Netty 的配置之后，bind() 方法会真正触发启动，sync() 方法则会阻塞，直至整个启动过程完成。 EventLoop实现原理EventLoop是什么EventLoop这个概念其实并不是Netty独有的，它是一种事件等待和处理的程序模型，可以解决多线程资源消耗高的问题。 EventLoop通用的运行模式如下图所示： 每当事件发生时，应用程序都会将产生的事件放入事件队列当中，然后 EventLoop 会轮询从队列中取出事件执行或者将事件分发给相应的事件监听者执行。事件执行的方式通常分为立即执行、延后执行、定期执行几种。 Netty如何实现EventLoop在 Netty 中 EventLoop 可以理解为 Reactor 线程模型的事件处理引擎，每个 EventLoop 线程都维护一个 Selector 选择器和任务队列 taskQueue。它主要负责处理 I/O 事件、普通任务和定时任务。 Netty 中推荐使用 NioEventLoop 作为实现类，那么 Netty 是如何实现 NioEventLoop 的呢？ 事件处理机制EventLoop的事件流转图如下所示： BossEventLoopGroup 和 WorkerEventLoopGroup 包含一个或者多个 NioEventLoop。BossEventLoopGroup 负责监听客户端的 Accept 事件，当事件触发时，将事件注册至 WorkerEventLoopGroup 中的一个 NioEventLoop 上。每新建一个 Channel， 只选择一个 NioEventLoop 与其绑定。所以说 Channel 生命周期的所有事件处理都是线程独立的，不同的 NioEventLoop 线程之间不会发生任何交集。 NioEventLoop 完成数据读取后，会调用绑定的 ChannelPipeline 进行事件传播，ChannelPipeline 也是线程安全的，数据会被传递到 ChannelPipeline 的第一个 ChannelHandler 中。数据处理完成后，将加工完成的数据再传递给下一个 ChannelHandler，整个过程是串行化执行，不会发生线程上下文切换的问题。 NioEventLoop 的事件处理机制采用的是无锁串行化的设计思路，NioEventLoop 无锁串行化的设计不仅使系统吞吐量达到最大化，而且降低了用户开发业务逻辑的难度，不需要花太多精力关心线程安全问题。 虽然单线程执行避免了线程切换，但是它的缺陷就是不能执行时间过长的 I/O 操作，一旦某个 I/O 事件发生阻塞，那么后续的所有 I/O 事件都无法执行，甚至造成事件积压。在使用 Netty 进行程序开发时，我们一定要对 ChannelHandler 的实现逻辑有充分的风险意识。 NioEventLoop 线程的可靠性至关重要，一旦 NioEventLoop 发生阻塞或者陷入空轮询，就会导致整个系统不可用。在 JDK 中， Epoll 的实现是存在漏洞的，即使 Selector 轮询的事件列表为空，NIO 线程一样可以被唤醒，导致 CPU 100% 占用。这就是臭名昭著的 JDK epoll 空轮询的 Bug。Netty 作为一个高性能、高可靠的网络框架，需要保证 I/O 线程的安全性。那么它是如何解决 JDK epoll 空轮询的 Bug 呢？实际上 Netty 并没有从根源上解决该问题，而是巧妙地规避了这个问题。 Netty 提供了一种检测机制判断线程是否可能陷入空轮询，具体的实现方式如下： 每次执行 Select 操作之前记录当前时间 currentTimeNanos。 time - TimeUnit.MILLISECONDS.toNanos(timeoutMillis) &gt;= currentTimeNanos，如果事件轮询的持续时间大于等于 timeoutMillis，那么说明是正常的，否则表明阻塞时间并未达到预期，可能触发了空轮询的 Bug。 Netty 引入了计数变量 selectCnt。在正常情况下，selectCnt 会重置，否则会对 selectCnt 自增计数。当 selectCnt 达到 SELECTOR_AUTO_REBUILD_THRESHOLD（默认512） 阈值时，会触发重建 Selector 对象。 Netty 采用这种方法巧妙地规避了 JDK Bug。异常的 Selector 中所有的 SelectionKey 会重新注册到新建的 Selector 上，重建完成之后异常的 Selector 就可以废弃了。 任务处理机制NioEventLoop 不仅负责处理 I/O 事件，还要兼顾执行任务队列中的任务。任务队列遵循 FIFO 规则，可以保证任务执行的公平性。NioEventLoop 处理的任务类型基本可以分为三类。 普通任务：通过 NioEventLoop 的 execute() 方法向任务队列 taskQueue 中添加任务。例如 Netty 在写数据时会封装 WriteAndFlushTask 提交给 taskQueue。taskQueue 的实现类是多生产者单消费者队列 MpscChunkedArrayQueue，在多线程并发添加任务时，可以保证线程安全。 定时任务：通过调用 NioEventLoop 的 schedule() 方法向定时任务队列 scheduledTaskQueue 添加一个定时任务，用于周期性执行该任务。例如，心跳消息发送等。定时任务队列 scheduledTaskQueue 采用优先队列 PriorityQueue 实现。 尾部队列：tailTasks 相比于普通任务队列优先级较低，在每次执行完 taskQueue 中任务后会去获取尾部队列中任务执行。尾部任务并不常用，主要用于做一些收尾工作，例如统计事件循环的执行时间、监控信息上报等。 可以分为 6 个步骤。 fetchFromScheduledTaskQueue 函数：将定时任务从 scheduledTaskQueue 中取出，聚合放入普通任务队列 taskQueue 中，只有定时任务的截止时间小于当前时间才可以被合并。 从普通任务队列 taskQueue 中取出任务。 计算任务执行的最大超时时间。 safeExecute 函数：安全执行任务，实际直接调用的 Runnable 的 run() 方法。 每执行 64 个任务进行超时时间的检查，如果执行时间大于最大超时时间，则立即停止执行任务，避免影响下一轮的 I/O 事件的处理。 最后获取尾部队列中的任务执行。 EventLoop 最佳实践 网络连接建立过程中三次握手、安全认证的过程会消耗不少时间。这里建议采用 Boss 和 Worker 两个 EventLoopGroup，有助于分担 Reactor 线程的压力。 由于 Reactor 线程模式适合处理耗时短的任务场景，对于耗时较长的 ChannelHandler 可以考虑维护一个业务线程池，将编解码后的数据封装成 Task 进行异步处理，避免 ChannelHandler 阻塞而造成 EventLoop 不可用。 如果业务逻辑执行时间较短，建议直接在 ChannelHandler 中执行。例如编解码操作，这样可以避免过度设计而造成架构的复杂性。 不宜设计过多的 ChannelHandler。对于系统性能和可维护性都会存在问题，在设计业务架构的时候，需要明确业务分层和 Netty 分层之间的界限。不要一味地将业务逻辑都添加到 ChannelHandler 中。 ChannelPipeline 实现原理ChannelPipeline 内部结构ChannelPipeline 可以看作是 ChannelHandler 的容器载体，它是由一组 ChannelHandler 实例组成的，内部通过双向链表将不同的 ChannelHandler 链接在一起，如下图所示。 ChannelPipeline 的双向链表分别维护了 HeadContext 和 TailContext 的头尾节点。自定义的 ChannelHandler 会插入到 Head 和 Tail 之间，这两个节点在 Netty 中已经默认实现了，它们在 ChannelPipeline 中起到了至关重要的作用。首先我们看下 HeadContext 和 TailContext 的继承关系，如下图所示。 HeadContext 既是 Inbound 处理器，也是 Outbound 处理器。它分别实现了 ChannelInboundHandler 和 ChannelOutboundHandler。网络数据写入操作的入口就是由 HeadContext 节点完成的。HeadContext 作为 Pipeline 的头结点负责读取数据并开始传递 InBound 事件，当数据处理完成后，数据会反方向经过 Outbound 处理器，最终传递到 HeadContext，所以 HeadContext 又是处理 Outbound 事件的最后一站。此外 HeadContext 在传递事件之前，还会执行一些前置操作。 TailContext 只实现了 ChannelInboundHandler 接口。它会在 ChannelInboundHandler 调用链路的最后一步执行，主要用于终止 Inbound 事件传播，例如释放 Message 数据资源等。TailContext 节点作为 OutBound 事件传播的第一站，仅仅是将 OutBound 事件传递给上一个节点。 从整个 ChannelPipeline 调用链路来看，如果由 Channel 直接触发事件传播，那么调用链路将贯穿整个 ChannelPipeline。然而也可以在其中某一个 ChannelHandlerContext 触发同样的方法，这样只会从当前的 ChannelHandler 开始执行事件传播，该过程不会从头贯穿到尾，在一定场景下，可以提高程序性能。 ChannelHandler接口设计异常传播机制ChannelPipeline 事件传播的实现采用了经典的责任链模式，调用链路环环相扣。异常按顺序从 Head 节点传播到 Tail 节点。如果用户没有对异常进行拦截处理，最后将由 Tail 节点统一处理，在 TailContext 源码中可以找到具体实现： 12345678910111213141516171819protected void onUnhandledInboundException(Throwable cause) { try { logger.warn( &quot;An exceptionCaught() event was fired, and it reached at the tail of the pipeline. &quot; + &quot;It usually means the last handler in the pipeline did not handle the exception.&quot;, cause); } finally { ReferenceCountUtil.release(cause); }} 虽然 Netty 中 TailContext 提供了兜底的异常处理逻辑，但是在很多场景下，并不能满足我们的需求。 在 Netty 应用开发的过程中，良好的异常处理机制会让排查问题的过程事半功倍。所以推荐用户对异常进行统一拦截，然后根据实际业务场景实现更加完善的异常处理机制。通过异常传播机制的学习，我们应该可以想到最好的方法是在 ChannelPipeline 自定义处理器的末端添加统一的异常处理器，此时 ChannelPipeline 的内部结构如下图所示。 用户自定义的异常处理器代码示例如下： 12345678public class ExceptionHandler extends ChannelDuplexHandler { @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) { if (cause instanceof RuntimeException) { System.out.println(&quot;Handle Business Exception Success.&quot;); } }} 加入统一的异常处理器后，可以看到异常已经被优雅地拦截并处理掉了。这也是 Netty 推荐的最佳异常处理实践。","link":"/posts/37855.html"},{"title":"Netty：编解码","text":"为什么有拆包/粘包？TCP传输协议是面向流的，没有数据包界限。客户端向服务端发送数据时，可能将一个完整的报文拆分成多个小报文进行发送，也可能将多个报文合并成一个大的报文进行发送。因此就有了拆包和粘包。 为什么会出现拆包/粘包现象呢？ 在网络通信的过程中，每次可以发送的数据包大小是受多种因素限制的，如 MTU传输单元大小、MSS 最大分段大小、滑动窗口等。如果一次传输的网络包数据大小超过传输单元大小，那么数据可能会拆分为多个数据包发送出去。如果每次请求的网络包数据都很小，一共请求了10000 次，TCP并不会分别发送 10000 次。因为TCP采用的Nagle算法对此作出了优化。 MTU最大传输单元和MSS最大分段大小MTU（Maxitum Transmission Unit） 是链路层一次最大传输数据的大小。MTU 一般来说大小为 1500 byte。MSS（Maximum Segement Size） 是指 TCP 最大报文段长度，它是传输层一次发送最大数据的大小。如下图所示，MTU 和 MSS 一般的计算关系为：MSS = MTU - IP 首部 - TCP首部，如果 MSS + TCP 首部 + IP 首部 &gt; MTU，那么数据包将会被拆分为多个发送。这就是拆包现象。 滑动窗口滑动窗口是 TCP 传输层用于流量控制的一种有效措施，也被称为通告窗口。滑动窗口是数据接收方设置的窗口大小，随后接收方会把窗口大小告诉发送方，以此限制发送方每次发送数据的大小，从而达到流量控制的目的。这样数据发送方不需要每发送一组数据就阻塞等待接收方确认，允许发送方同时发送多个数据分组，每次发送的数据都会被限制在窗口大小内。由此可见，滑动窗口可以大幅度提升网络吞吐量。 那么 TCP 报文是怎么确保数据包按次序到达且不丢数据呢？首先，所有的数据帧都是有编号的，TCP 并不会为每个报文段都回复 ACK 响应，它会对多个报文段回复一次 ACK。假设有三个报文段 A、B、C，发送方先发送了B、C，接收方则必须等待 A 报文段到达，如果一定时间内仍未等到 A 报文段，那么 B、C 也会被丢弃，发送方会发起重试。如果已接收到 A 报文段，那么将会回复发送方一次 ACK 确认。 Nagle算法Nagle算法于1984年被福特航空和通信公司定义为TCP/IP拥塞控制方法。它主要用于解决频繁发送小数据包而带来的网络拥塞问题。试想如果每次需要发送的数据只有1字节，加上20个字节IP Header和20个字节TCP Header，每次发送的数据包大小为 41 字节，但是只有 1 字节是有效信息，这就造成了非常大的浪费。 Nagle 算法可以理解为批量发送，也是我们平时编程中经常用到的优化思路，它是在数据未得到确认之前先写入缓冲区，等待数据确认或者缓冲区积攒到一定大小再把数据包发送出去。 Linux 在默认情况下是开启 Nagle 算法的，在大量小数据包的场景下可以有效地降低网络开销。但如果你的业务场景每次发送的数据都需要获得及时响应，那么 Nagle 算法就不能满足你的需求了，因为 Nagle 算法会有一定的数据延迟。你可以通过 Linux 提供的 TCP_NODELAY 参数禁用 Nagle 算法。Netty 中为了使数据传输延迟最小化，就默认禁用了 Nagle 算法，这一点与 Linux 操作系统的默认行为是相反的。 拆包/粘包的解决方案 在客户端和服务端通信的过程中，服务端一次读到的数据大小是不确定的。如上图所示，拆包/粘包可能会出现以下五种情况： 服务端恰巧读到了两个完整的数据包 A 和 B，没有出现拆包/粘包问题； 服务端接收到 A 和 B 粘在一起的数据包，服务端需要解析出 A 和 B； 服务端收到完整的 A 和 B 的一部分数据包 B-1，服务端需要解析出完整的 A，并等待读取完整的 B 数据包； 服务端接收到 A 的一部分数据包 A-1，此时需要等待接收到完整的 A 数据包； 数据包 A 较大，服务端需要多次才可以接收完数据包 A。 由于拆包/粘包问题的存在，数据接收方很难界定数据包的边界在哪里，很难识别出一个完整的数据包。所以需要提供一种机制来识别数据包的界限，这也是解决拆包/粘包的唯一方法：定义应用层的通信协议。 主流协议的解决方案消息长度固定每个数据报文都需要一个固定的长度。当接收方累计读取到固定长度的报文后，就认为已经获得一个完整的消息。当发送方的数据小于固定长度时，则需要空位补齐。 假设我们的固定长度为 4 字节，那么如上所示的5条数据一共需要发送4个报文： 消息定长法使用非常简单，但是缺点也非常明显，无法很好设定固定长度的值，如果长度太大会造成字节浪费，长度太小又会影响消息传输，所以在一般情况下消息定长法不会被采用。 特定分隔符在每次发送报文的尾部加上特定分隔符，接收方就可以根据特殊分隔符进行消息拆分。以下报文根据特定分隔符\\n按行解析，即可得到 AB、CDEF、GHIJ、K、LM 五条原始报文。 由于在发送报文时尾部需要添加特定分隔符，所以对于分隔符的选择一定要避免和消息体中字符相同，以免冲突。否则可能出现错误的消息拆分。比较推荐的做法是将消息进行编码，例如 base64 编码，然后可以选择 64 个编码字符之外的字符作为特定分隔符。特定分隔符法在消息协议足够简单的场景下比较高效，例如大名鼎鼎的 Redis 在通信过程中采用的就是换行分隔符。 消息长度 + 消息内容 消息长度 + 消息内容是项目开发中最常用的一种协议，如上展示了该协议的基本格式。消息头中存放消息的总长度，例如使用 4 字节的 int 值记录消息的长度，消息体实际的二进制的字节数据。接收方在解析数据时，首先读取消息头的长度字段 Len，然后紧接着读取长度为 Len 的字节数据，该数据即判定为一个完整的数据报文。依然以上述提到的原始字节数据为例，使用该协议进行编码后的结果如下所示： 消息长度 + 消息内容的使用方式非常灵活，且不会存在消息定长法和特定分隔符法的明显缺陷。当然在消息头中不仅只限于存放消息的长度，而且可以自定义其他必要的扩展字段，例如消息版本、算法类型等。 通信协议设计所谓协议，就是通信双方事先商量好的接口暗语，在TCP网络编程中，发送方和接收方的数据包格式都是二进制，发送方将对象转化成二进制流发送给接收方，接收方获得二进制数据后需要知道如何解析成对象，所以协议是双方能够正常通信的基础。 通用协议目前市面上已经有不少通用的协议，例如 HTTP、HTTPS、JSON-RPC、FTP、IMAP、Protobuf 等。通用协议兼容性好，易于维护，各种异构系统之间可以实现无缝对接。如果在满足业务场景以及性能需求的前提下，推荐采用通用协议的方案。 相比通用协议，自定义协议主要有以下优点： 极致性能：通用的通信协议考虑了很多兼容性的因素，必然在性能方面有所损失。 扩展性：自定义的协议相比通用协议更好扩展，可以更好地满足自己的业务需求。 安全性：通用协议是公开的，很多漏洞已经很多被黑客攻破。自定义协议更加安全，因为黑客需要先破解你的协议内容。 自定义通信协议那么如何设计自定义的通信协议呢？这个答案见仁见智，但是设计通信协议有经验方法可循。结合实战经验我们一起看下一个完备的网络协议需要具备哪些基本要素。 1. 魔数魔数是通信双方协商的一个暗号，通常采用固定的几个字节表示。魔数的作用是防止任何人随便向服务器的端口上发送数据。服务端在接收到数据时会解析出前几个固定字节的魔数，然后做正确性比对。如果和约定的魔数不匹配，则认为是非法数据，可以直接关闭连接或者采取其他措施以增强系统的安全防护。 魔数的思想在压缩算法、Java Class 文件等场景中都有所体现，例如 Class 文件开头就存储了魔数 0xCAFEBABE，在加载 Class 文件时首先会验证魔数的正确性。 2. 协议版本号随着业务需求的变化，协议可能需要对结构或字段进行改动，不同版本的协议对应的解析方法也是不同的。所以在生产级项目中强烈建议预留协议版本号这个字段。 3. 序列化算法序列化算法字段表示数据发送方应该采用何种方法将请求的对象转化为二进制，以及如何再将二进制转化为对象，如 JSON、Hessian、Java 自带序列化等。 4. 报文类型在不同的业务场景中，报文可能存在不同的类型。例如在 RPC 框架中有请求、响应、心跳等类型的报文，在 IM 即时通信的场景中有登陆、创建群聊、发送消息、接收消息、退出群聊等类型的报文。 5. 长度域字段长度域字段代表请求数据的长度，接收方根据长度域字段获取一个完整的报文。 6. 请求数据请求数据通常为序列化之后得到的二进制流，每种请求数据的内容是不一样的。 7. 状态状态字段用于标识请求是否正常。一般由被调用方设置。例如一次 RPC 调用失败，状态字段可被服务提供方设置为异常状态。 8. 保留字段保留字段是可选项，为了应对协议升级的可能性，可以预留若干字节的保留字段，以备不时之需。 通过以上协议基本要素的学习，可以得到一个较为通用的协议示例： Netty如何实现自定义通信协议Netty作为一个非常优秀的网络通信框架，已经为我们提供了非常丰富的编解码抽象基类，帮助我们更方便地基于这些抽象基类扩展实现自定义协议。 首先我们看下Netty中编解码器是如何分类的。 Netty常用编码器类型： MessageToByteEncoder 对象编码成字节流； MessageToMessageEncoder 一种消息类型编码成另外一种消息类型。 Netty常用解码器类型： ByteToMessageDecoder/ReplayingDecoder 将字节流解码为消息对象； MessageToMessageDecoder 将一种消息类型解码为另外一种消息类型。 编解码器可以分为一次解码器和二次解码器，一次解码器用于解决TCP拆包/粘包问题，按协议解析后得到的字节数据。如果你需要对解析后的字节数据做对象模型的转换，这时候便需要用到二次解码器，同理编码器的过程是反过来的。 一次编解码器：MessageToByteEncoder/ByteToMessageDecoder。 二次编解码器：MessageToMessageEncoder/MessageToMessageDecoder。 抽象编码类 通过抽象编码类的继承图可以看出，编码类是ChannelOutboundHandler的抽象类实现，具体操作的是Outbound出站数据。 MessageToByteEncoderMessageToByteEncoder用于将对象编码成字节流，MessageToByteEncoder提供了唯一的encode抽象方法，我们只需要实现encode方法即可完成自定义编码。 encode()方法是在什么时候被调用的呢？我们一起看下MessageToByteEncoder 的核心源码片段，如下所示。 12345678910111213141516171819202122232425262728293031323334353637public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) throws Exception { ByteBuf buf = null; try { // 1. 消息类型是否匹配 if (acceptOutboundMessage(msg)) { @SuppressWarnings(&quot;unchecked&quot;) I cast = (I) msg; // 2. 分配 ByteBuf 资源 buf = allocateBuffer(ctx, cast, preferDirect); try { // 3. 执行 encode 方法完成数据编码 encode(ctx, cast, buf); } finally { ReferenceCountUtil.release(cast); } if (buf.isReadable()) { // 4. 向后传递写事件 ctx.write(buf, promise); } else { buf.release(); ctx.write(Unpooled.EMPTY_BUFFER, promise); } buf = null; } else { ctx.write(msg, promise); } } catch (EncoderException e) { throw e; } catch (Throwable e) { throw new EncoderException(e); } finally { if (buf != null) { buf.release(); } }} MessageToByteEncoder 重写了 ChanneOutboundHandler 的 write() 方法，其主要逻辑分为以下几个步骤： acceptOutboundMessage判断是否有匹配的消息类型，如果匹配需要执行编码流程，如果不匹配直接继续传递给下一个 ChannelOutboundHandler； 分配 ByteBuf 资源，默认使用堆外内存； 调用子类实现的 encode 方法完成数据编码，一旦消息被成功编码，会通过调用 ReferenceCountUtil.release(cast) 自动释放； 如果ByteBuf可读，说明已经成功编码得到数据，然后写入ChannelHandlerContext交到下一个节点；如果ByteBuf不可读，则释放ByteBuf资源，向下传递空的ByteBuf对象。 编码器实现非常简单，不需要关注拆包/粘包问题。如下例子，展示了如何将字符串类型的数据写入到 ByteBuf 实例，ByteBuf 实例将传递给 ChannelPipeline 链表中的下一个 ChannelOutboundHandler。 123456public class StringToByteEncoder extends MessageToByteEncoder&lt;String&gt; { @Override protected void encode(ChannelHandlerContext channelHandlerContext, String data, ByteBuf byteBuf) throws Exception { byteBuf.writeBytes(data.getBytes()); }} MessageToMessageEncoderMessageToMessageEncoder 与 MessageToByteEncoder 类似，同样只需要实现 encode 方法。与 MessageToByteEncoder 不同的是，MessageToMessageEncoder 是将一种格式的消息转换为另外一种格式的消息。其中第二个 Message 所指的可以是任意一个对象，如果该对象是 ByteBuf 类型，那么基本上和 MessageToByteEncoder 的实现原理是一致的。 此外 MessageToMessageEncoder 的输出结果是对象列表，编码后的结果属于中间对象，最终仍然会转化成 ByteBuf 进行传输。 MessageToMessageEncoder常用的实现子类有 StringEncoder、LineEncoder、Base64Encoder 等。以 StringEncoder 为例看下 MessageToMessageEncoder 的用法。源码示例如下： 1234567@Overrideprotected void encode(ChannelHandlerContext ctx, CharSequence msg, List&lt;Object&gt; out) throws Exception { if (msg.length() == 0) { return; } out.add(ByteBufUtil.encodeString(ctx.alloc(), CharBuffer.wrap(msg), charset));} 抽象解码类解码类是 ChanneInboundHandler 的抽象类实现，操作的是Inbound入站数据。解码器实现的难度要远大于编码器，因为解码器需要考虑拆包/粘包问题。由于接收方有可能没有接收到完整的消息，所以解码框架需要对入站的数据做缓冲操作，直至获取到完整的消息。 抽象解码类 ByteToMessageDecoder首先，我们看下ByteToMessageDecoder定义的抽象方法： 12345678public abstract class ByteToMessageDecoder extends ChannelInboundHandlerAdapter { protected abstract void decode(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) throws Exception; protected void decodeLast(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) throws Exception { if (in.isReadable()) { decodeRemovalReentryProtection(ctx, in, out); } }} decode()是用户必须实现的抽象方法，在该方法在调用时需要传入接收的数据 ByteBuf，及用来添加编码后消息的 List。由于 TCP 粘包问题，ByteBuf 中可能包含多个有效的报文，或者不够一个完整的报文。Netty 会重复回调 decode() 方法，直到没有解码出新的完整报文可以添加到 List 当中，或者 ByteBuf 没有更多可读取的数据为止。如果此时 List 的内容不为空，那么会传递给 ChannelPipeline 中的下一个ChannelInboundHandler。 此外 ByteToMessageDecoder 还定义了 decodeLast() 方法。为什么抽象解码器要比编码器多一个 decodeLast() 方法呢？因为 decodeLast 在 Channel 关闭后会被调用一次，主要用于处理 ByteBuf 最后剩余的字节数据。Netty 中 decodeLast 的默认实现只是简单调用了 decode() 方法。如果有特殊的业务需求，则可以通过重写 decodeLast() 方法扩展自定义逻辑。 ByteToMessageDecoder 还有一个抽象子类是 ReplayingDecoder。它封装了缓冲区的管理，在读取缓冲区数据时，你无须再对字节长度进行检查。因为如果没有足够长度的字节数据，ReplayingDecoder 将终止解码操作。ReplayingDecoder 的性能相比直接使用 ByteToMessageDecoder 要慢，大部分情况下并不推荐使用 ReplayingDecoder。 抽象解码类 MessageToMessageDecoderMessageToMessageDecoder与 ByteToMessageDecoder 作用类似，都是将一种消息类型的编码成另外一种消息类型。与 ByteToMessageDecoder 不同的是 MessageToMessageDecoder并不会对数据报文进行缓存，它主要用作转换消息模型。 比较推荐的做法是使用 ByteToMessageDecoder 解析 TCP 协议，解决拆包/粘包问题。解析得到有效的 ByteBuf 数据，然后传递给后续的 MessageToMessageDecoder 做数据对象的转换，具体流程如下图所示。 通信协议实战在实现协议编码器之前，我们首先需要清楚一个问题：如何判断 ByteBuf 是否存在完整的报文？最常用的做法就是通过读取消息长度 dataLength 进行判断。如果 ByteBuf 的可读数据长度小于 dataLength，说明 ByteBuf 还不够获取一个完整的报文。在该协议前面的消息头部分包含了魔数、协议版本号、数据长度等固定字段，共 14 个字节。固定字段长度和数据长度可以作为我们判断消息完整性的依据，具体编码器实现逻辑示例如下： 1234567891011121314151617181920212223242526272829303132333435/*+---------------------------------------------------------------+| 魔数 2byte | 协议版本号 1byte | 序列化算法 1byte | 报文类型 1byte |+---------------------------------------------------------------+| 状态 1byte | 保留字段 4byte | 数据长度 4byte | +---------------------------------------------------------------+| 数据内容 （长度不定） |+---------------------------------------------------------------+ */@Overridepublic final void decode(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) { // 判断 ByteBuf 可读取字节 if (in.readableBytes() &lt; 14) { return; } in.markReaderIndex(); // 标记 ByteBuf 读指针位置 in.skipBytes(2); // 跳过魔数 in.skipBytes(1); // 跳过协议版本号 byte serializeType = in.readByte(); in.skipBytes(1); // 跳过报文类型 in.skipBytes(1); // 跳过状态字段 in.skipBytes(4); // 跳过保留字段 int dataLength = in.readInt(); if (in.readableBytes() &lt; dataLength) { in.resetReaderIndex(); // 重置 ByteBuf 读指针位置 return; } byte[] data = new byte[dataLength]; in.readBytes(data); SerializeService serializeService = getSerializeServiceByType(serializeType); Object obj = serializeService.deserialize(data); if (obj != null) { out.add(obj); }} Netty常用的解码器Netty 提供了很多开箱即用的解码器，这些解码器基本覆盖了 TCP 拆包/粘包的通用解决方案。 固定长度解码器 FixedLengthFrameDecoder固定长度解码器 FixedLengthFrameDecoder 非常简单，直接通过构造函数设置固定长度的大小 frameLength，无论接收方一次获取多大的数据，都会严格按照 frameLength 进行解码。如果累积读取到长度大小为 frameLength 的消息，那么解码器认为已经获取到了一个完整的消息。如果消息长度小于 frameLength，FixedLengthFrameDecoder 解码器会一直等后续数据包的到达，直至获得完整的消息。 123456789101112131415161718192021222324252627282930313233public class EchoServer { public void startEchoServer(int port) throws Exception { EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); try { ServerBootstrap b = new ServerBootstrap(); b.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() { @Override public void initChannel(SocketChannel ch) { ch.pipeline().addLast(new FixedLengthFrameDecoder(10)); ch.pipeline().addLast(new EchoServerHandler()); } }); ChannelFuture f = b.bind(port).sync(); f.channel().closeFuture().sync(); } finally { bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); } } public static void main(String[] args) throws Exception { new EchoServer().startEchoServer(8088); }}@Sharablepublic class EchoServerHandler extends ChannelInboundHandlerAdapter { @Override public void channelRead(ChannelHandlerContext ctx, Object msg) { System.out.println(&quot;Receive client : [&quot; + ((ByteBuf) msg).toString(CharsetUtil.UTF_8) + &quot;]&quot;); }} 在上述服务端的代码中使用了固定 10 字节的解码器，并在解码之后通过 EchoServerHandler 打印结果。我们可以启动服务端，通过 telnet 命令像服务端发送数据，观察代码输出的结果。 客户端输入： 123456telnet localhost 8088Trying ::1...Connected to localhost.Escape character is '^]'.1234567890123456789012 服务端输出： 123Receive client : [1234567890]Receive client : [12345678] 特殊分隔符解码器 DelimiterBasedFrameDecoder delimiters delimiters 指定特殊分隔符，通过写入 ByteBuf 作为参数传入。delimiters 的类型是 ByteBuf 数组，所以我们可以同时指定多个分隔符，但是最终会选择长度最短的分隔符进行消息拆分。 maxLength maxLength 是报文最大长度的限制。如果超过 maxLength 还没有检测到指定分隔符，将会抛出 TooLongFrameException。可以说 maxLength 是对程序在极端情况下的一种保护措施。 failFast failFast 与 maxLength 需要搭配使用，通过设置 failFast 可以控制抛出 TooLongFrameException 的时机，可以说 Netty 在细节上考虑得面面俱到。如果 failFast=true，那么在超出 maxLength 会立即抛出 TooLongFrameException，不再继续进行解码。如果 failFast=false，那么会等到解码出一个完整的消息后才会抛出 TooLongFrameException。 stripDelimiter stripDelimiter 的作用是判断解码后得到的消息是否去除分隔符。 下面我们还是结合代码示例学习 DelimiterBasedFrameDecoder 的用法，依然以固定编码器小节中使用的代码为基础稍做改动，引入特殊分隔符解码器 DelimiterBasedFrameDecoder： 12345678910b.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() { @Override public void initChannel(SocketChannel ch) { ByteBuf delimiter = Unpooled.copiedBuffer(&quot;&amp;&quot;.getBytes()); ch.pipeline().addLast(new DelimiterBasedFrameDecoder(10, true, true, delimiter)); ch.pipeline().addLast(new EchoServerHandler()); } }); 我们依然通过 telnet 模拟客户端发送数据，观察代码输出的结果，可以发现由于 maxLength 设置的只有 10，所以在解析到第三个消息时抛出异常。 客户端输入： 12345telnet localhost 8088Trying ::1...Connected to localhost.Escape character is '^]'.hello&amp;world&amp;1234567890ab 服务端输出： 12345678Receive client : [hello]Receive client : [world]九月 25, 2020 8:46:01 下午 io.netty.channel.DefaultChannelPipeline onUnhandledInboundException警告: An exceptionCaught() event was fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did not handle the exception.io.netty.handler.codec.TooLongFrameException: frame length exceeds 10: 13 - discarded at io.netty.handler.codec.DelimiterBasedFrameDecoder.fail(DelimiterBasedFrameDecoder.java:302) at io.netty.handler.codec.DelimiterBasedFrameDecoder.decode(DelimiterBasedFrameDecoder.java:268) at io.netty.handler.codec.DelimiterBasedFrameDecoder.decode(DelimiterBasedFrameDecoder.java:218) 长度域解码器 LengthFieldBasedFrameDecoder长度域解码器 LengthFieldBasedFrameDecoder 是解决 TCP 拆包/粘包问题最常用的解码器。它基本上可以覆盖大部分基于长度拆包场景，开源消息中间件 RocketMQ 就是使用 LengthFieldBasedFrameDecoder 进行解码的。 delimiters delimiters 指定特殊分隔符，通过写入 ByteBuf 作为参数传入。delimiters 的类型是 ByteBuf 数组，所以我们可以同时指定多个分隔符，但是最终会选择长度最短的分隔符进行消息拆分。 maxLength maxLength 是报文最大长度的限制。如果超过 maxLength 还没有检测到指定分隔符，将会抛出 TooLongFrameException。可以说 maxLength 是对程序在极端情况下的一种保护措施。 failFast failFast 与 maxLength 需要搭配使用，通过设置 failFast 可以控制抛出 TooLongFrameException 的时机，可以说 Netty 在细节上考虑得面面俱到。如果 failFast=true，那么在超出 maxLength 会立即抛出 TooLongFrameException，不再继续进行解码。如果 failFast=false，那么会等到解码出一个完整的消息后才会抛出 TooLongFrameException。 stripDelimiter stripDelimiter 的作用是判断解码后得到的消息是否去除分隔符。 下面我们还是结合代码示例学习DelimiterBasedFrameDecoder的用法，依然以固定编码器小节中使用的代码为基础稍做改动，引入特殊分隔符解码器 DelimiterBasedFrameDecoder： 12345678910b.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() { @Override public void initChannel(SocketChannel ch) { ByteBuf delimiter = Unpooled.copiedBuffer(&quot;&amp;&quot;.getBytes()); ch.pipeline().addLast(new DelimiterBasedFrameDecoder(10, true, true, delimiter)); ch.pipeline().addLast(new EchoServerHandler()); } }); 我们依然通过 telnet 模拟客户端发送数据，观察代码输出的结果，可以发现由于 maxLength 设置的只有 10，所以在解析到第三个消息时抛出异常。 客户端输入： 服务端输出： 属性说明先了解 LengthFieldBasedFrameDecoder 中的几个重要属性，这里我主要把它们分为两个部分：长度域解码器特有属性以及与其他解码器（如特定分隔符解码器）的相似的属性。 长度域解码器特有属性 1234567891011121314151617// 长度字段的偏移量，也就是存放长度数据的起始位置private final int lengthFieldOffset; // 长度字段所占用的字节数private final int lengthFieldLength; /* * 消息长度的修正值 * * 在很多较为复杂一些的协议设计中，长度域不仅仅包含消息的长度，而且包含其他的数据，如版本号、数据类型、数据状态等，那么这时候我们需要使用 lengthAdjustment 进行修正 * * lengthAdjustment = 包体的长度值 - 长度域的值 * */private final int lengthAdjustment; // 解码后需要跳过的初始字节数，也就是消息内容字段的起始位置private final int initialBytesToStrip;// 长度字段结束的偏移量，lengthFieldEndOffset = lengthFieldOffset + lengthFieldLengthprivate final int lengthFieldEndOffset; 与固定长度解码器和特定分隔符解码器相似的属性 12345private final int maxFrameLength; // 报文最大限制长度private final boolean failFast; // 是否立即抛出 TooLongFrameException，与 maxFrameLength 搭配使用private boolean discardingTooLongFrame; // 是否处于丢弃模式private long tooLongFrameLength; // 需要丢弃的字节数private long bytesToDiscard; // 累计丢弃的字节数 示例 1：典型的基于消息长度 + 消息内容的解码 上述协议是最基本的格式，报文只包含消息长度 Length 和消息内容 Content 字段，其中 Length 为 16 进制表示，共占用 2 字节，Length 的值 0x000C 代表 Content 占用 12 字节。该协议对应的解码器参数组合如下： lengthFieldOffset = 0，因为 Length 字段就在报文的开始位置。 lengthFieldLength = 2，协议设计的固定长度。 lengthAdjustment = 0，Length 字段只包含消息长度，不需要做任何修正。 initialBytesToStrip = 0，解码后内容依然是 Length + Content，不需要跳过任何初始字节。 示例 2：解码结果需要截断 示例 2 和示例 1 的区别在于解码后的结果只包含消息内容，其他的部分是不变的。该协议对应的解码器参数组合如下： lengthFieldOffset = 0，因为 Length 字段就在报文的开始位置。 lengthFieldLength = 2，协议设计的固定长度。 lengthAdjustment = 0，Length 字段只包含消息长度，不需要做任何修正。 initialBytesToStrip = 2，跳过 Length 字段的字节长度，解码后 ByteBuf 中只包含 Content字段。 示例 3：长度字段包含消息长度和消息内容所占的字节。 与前两个示例不同的是，示例 3 的 Length 字段包含 Length 字段自身的固定长度以及 Content 字段所占用的字节数，Length 的值为 0x000E（2 + 12 = 14 字节），在 Length 字段值（14 字节）的基础上做 lengthAdjustment（-2）的修正，才能得到真实的 Content 字段长度，所以对应的解码器参数组合如下： lengthFieldOffset = 0，因为 Length 字段就在报文的开始位置。 lengthFieldLength = 2，协议设计的固定长度。 lengthAdjustment = -2，长度字段为 14 字节，需要减 2 才是拆包所需要的长度。 initialBytesToStrip = 0，解码后内容依然是 Length + Content，不需要跳过任何初始字节。 示例 4：基于长度字段偏移的解码12345BEFORE DECODE (17 bytes) AFTER DECODE (17 bytes)+----------+----------+----------------+ +----------+----------+----------------+| Header 1 | Length | Actual Content |-----&gt;| Header 1 | Length | Actual Content || 0xCAFE | 0x00000C | &quot;HELLO, WORLD&quot; | | 0xCAFE | 0x00000C | &quot;HELLO, WORLD&quot; |+----------+----------+----------------+ +----------+----------+----------------+ 示例 4 中 Length 字段不再是报文的起始位置，Length 字段的值为 0x00000C，表示 Content 字段占用 12 字节，该协议对应的解码器参数组合如下： lengthFieldOffset = 2，需要跳过 Header 1 所占用的 2 字节，才是 Length 的起始位置。 lengthFieldLength = 3，协议设计的固定长度。 lengthAdjustment = 0，Length 字段只包含消息长度，不需要做任何修正。 initialBytesToStrip = 0，解码后内容依然是完整的报文，不需要跳过任何初始字节。 示例 5：长度字段与内容字段不再相邻12345BEFORE DECODE (17 bytes) AFTER DECODE (17 bytes)+----------+----------+----------------+ +----------+----------+----------------+| Length | Header 1 | Actual Content |-----&gt;| Length | Header 1 | Actual Content || 0x00000C | 0xCAFE | &quot;HELLO, WORLD&quot; | | 0x00000C | 0xCAFE | &quot;HELLO, WORLD&quot; |+----------+----------+----------------+ +----------+----------+----------------+ 示例 5 中的 Length 字段之后是 Header 1，Length 与 Content 字段不再相邻。Length 字段所表示的内容略过了 Header 1 字段，所以也需要通过 lengthAdjustment 修正才能得到 Header + Content 的内容。示例 5 所对应的解码器参数组合如下： lengthFieldOffset = 0，因为 Length 字段就在报文的开始位置。 lengthFieldLength = 3，协议设计的固定长度。 lengthAdjustment = 2，由于 Header + Content 一共占用 2 + 12 = 14 字节，所以 Length 字段值（12 字节）加上 lengthAdjustment（2 字节）才能得到 Header + Content 的内容（14 字节）。 initialBytesToStrip = 0，解码后内容依然是完整的报文，不需要跳过任何初始字节。 示例 6：基于长度偏移和长度修正的解码12345BEFORE DECODE (16 bytes) AFTER DECODE (13 bytes)+------+--------+------+----------------+ +------+----------------+| HDR1 | Length | HDR2 | Actual Content |-----&gt;| HDR2 | Actual Content || 0xCA | 0x000C | 0xFE | &quot;HELLO, WORLD&quot; | | 0xFE | &quot;HELLO, WORLD&quot; |+------+--------+------+----------------+ +------+----------------+ 示例 6 中 Length 字段前后分为别 HDR1 和 HDR2 字段，各占用 1 字节，所以既需要做长度字段的偏移，也需要做 lengthAdjustment 修正，具体修正的过程与 示例 5 类似。对应的解码器参数组合如下： lengthFieldOffset = 1，需要跳过 HDR1 所占用的 1 字节，才是 Length 的起始位置。 lengthFieldLength = 2，协议设计的固定长度。 lengthAdjustment = 1，由于 HDR2 + Content 一共占用 1 + 12 = 13 字节，所以 Length 字段值（12 字节）加上 lengthAdjustment（1）才能得到 HDR2 + Content 的内容（13 字节）。 initialBytesToStrip = 3，解码后跳过 HDR1 和 Length 字段，共占用 3 字节。 示例 7：长度字段包含除 Content 外的多个其他字段12345BEFORE DECODE (16 bytes) AFTER DECODE (13 bytes)+------+--------+------+----------------+ +------+----------------+| HDR1 | Length | HDR2 | Actual Content |-----&gt;| HDR2 | Actual Content || 0xCA | 0x0010 | 0xFE | &quot;HELLO, WORLD&quot; | | 0xFE | &quot;HELLO, WORLD&quot; |+------+--------+------+----------------+ +------+----------------+ 示例 7 与 示例 6 的区别在于 Length 字段记录了整个报文的长度，包含 Length 自身所占字节、HDR1 、HDR2 以及 Content 字段的长度，解码器需要知道如何进行 lengthAdjustment 调整，才能得到 HDR2 和 Content 的内容。所以我们可以采用如下的解码器参数组合： lengthFieldOffset = 1，需要跳过 HDR1 所占用的 1 字节，才是 Length 的起始位置。 lengthFieldLength = 2，协议设计的固定长度。 lengthAdjustment = -3，Length 字段值（16 字节）需要减去 HDR1（1 字节） 和 Length 自身所占字节长度（2 字节）才能得到 HDR2 和 Content 的内容（1 + 12 = 13 字节）。 initialBytesToStrip = 3，解码后跳过 HDR1 和 Length 字段，共占用 3 字节。","link":"/posts/32223.html"},{"title":"Spring：事务","text":"Spring Framework事务概述Spring Framework为事务管理提供了一致的抽象，具有以下优势： 跨不同事务API的一致编程模型，例如Java Transaction API（JTA），JDBC，Hibernate，Java Persistence API（JPA）和Java Data Objects（JDO）。 支持声明事务管理支持声明式事务管理。 与复杂的事务API（如JTA）相比，用于编程事务管理的API更简单。 与Spring的数据访问抽象集成。 全局事务和本地事务传统上，Java EE开发人员有两种事务管理选择：全局事务或本地事务，两者都有明显的局限性。 全局事务：JTA、EJB优点：可以多资源使用;缺点：JTA API笨重、通过JNDI获取资源。 本地事务：本地事务是资源专用，比如：JDBC连接。优点：简单易用;缺点：不能多资源使用。 Spring Framework的一致编程模型Spring解决了全局和本地事务的缺点。它使应用程序开发人员能够在任何环境中使用一致的编程模型。 Spring Framework提供了声明式和编程式事务管理。大多数情况下，用户更喜欢声明式事务管理。通过编程式事务管理，开发人员可以使用Spring Framework事务抽象，它可以在任何底层事务基础结构上运行。 使用首选的声明式模型，开发人员通常很少或根本不编写与事务管理相关的代码，因此不依赖于Spring Framework事务API或任何其他事务API。 理解Spring Framework事务抽象Spring事务抽象的关键是事务策略的概念。事务策略由org.springframework.transaction.PlatformTransactionManager接口定义： 123456789public interface PlatformTransactionManager { TransactionStatus getTransaction(TransactionDefinition definition) throws TransactionException; void commit(TransactionStatus status) throws TransactionException; void rollback(TransactionStatus status) throws TransactionException;} getTransaction（..）方法返回TransactionStatus对象，具体取决于TransactionDefinition参数。 返回的TransactionStatus可能表示新事务，或者如果当前调用堆栈中存在匹配的事务，则可以表示现有事务。 后一种情况的含义是，与Java EE事务上下文一样，TransactionStatus与执行线程相关联。 TransactionDefinitionTransactionDefinition用于描述事务的隔离级别、超时时间、是否只读事务和事务传播规则等控制事务具体行为的事务属性。 123456789101112131415161718public interface TransactionDefinition { // 事务传播 int getPropagationBehavior(); // 事务隔离级别 int getIsolationLevel(); // 事务超时事务 int getTimeout(); // 是否只读 boolean isReadOnly(); // 事务名称 String getName();} TransactionStatusTransactionStatus接口为事务代码提供了一种控制事务执行和查询事务状态的简单方法。这些概念应该是熟悉的，因为它们对于所有事务API都是通用的： 123456789101112131415public interface TransactionStatus extends SavepointManager { boolean isNewTransaction(); boolean hasSavepoint(); void setRollbackOnly(); boolean isRollbackOnly(); void flush(); boolean isCompleted();} PlatformTransactionManager无论您是在Spring中选择声明式还是程序化事务管理，定义正确的PlatformTransactionManager实现都是绝对必要的。 您通常通过依赖注入来定义此实现。 PlatformTransactionManager实现通常需要了解它们工作的环境：JDBC，JTA，Hibernate等。以下示例显示了如何定义本地PlatformTransactionManager实现。 （此示例适用于普通JDBC。） 1、首先，定义JDBC数据源。 123456&lt;bean id=&quot;dataSource&quot; class=&quot;org.apache.commons.dbcp.BasicDataSource&quot; destroy-method=&quot;close&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;${jdbc.driverClassName}&quot; /&gt; &lt;property name=&quot;url&quot; value=&quot;${jdbc.url}&quot; /&gt; &lt;property name=&quot;username&quot; value=&quot;${jdbc.username}&quot; /&gt; &lt;property name=&quot;password&quot; value=&quot;${jdbc.password}&quot; /&gt;&lt;/bean&gt; 2、然后，相关的PlatformTransactionManager bean定义将引用DataSource定义。 123&lt;bean id=&quot;txManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt;&lt;/bean&gt; 如果在Java EE容器中使用JTA，则使用通过JNDI获得的容器DataSource以及Spring的JtaTransactionManager。则配置如下： 1234567891011121314151617&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:jee=&quot;http://www.springframework.org/schema/jee&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/jee http://www.springframework.org/schema/jee/spring-jee.xsd&quot;&gt; &lt;jee:jndi-lookup id=&quot;dataSource&quot; jndi-name=&quot;jdbc/jpetstore&quot;/&gt; &lt;bean id=&quot;txManager&quot; class=&quot;org.springframework.transaction.jta.JtaTransactionManager&quot; /&gt; &lt;!-- other &lt;bean/&gt; definitions here --&gt;&lt;/beans&gt; Spring为不同的持久化框架提供了PlatformTransactionManager接口的实现类。如下表所示： org.springframework.orm.jpa.JpaTransactionManager：使用JPA进行持久化，使用该事务管理器 org.springframework.orm.hibernateX.HibernateTransactionManager：使用Hibernate X.0(X可为3,4,5)版本进行持久化时，使用该事务管理器 org.springframework.jdbc.datasource.DataSourceTransactionManager：使用Spring JDBC或MyBatis等基于DataSource数据源技术的持久化技术时，使用该事务管理器 org.springframework.orm.jdo.JdoTransactionManager：使用JDO进行持久化时，使用该事务管理器 org.springframework.transaction.jta.JtaTransactionManager：具有多个数据源的全局事务使用该事务管理器（不管采用何种持久化技术） Synchronizing resources with transactions本节描述直接或间接使用持久性API（如JDBC，Hibernate或JDO）的应用程序代码如何确保正确创建，重用和清理这些资源。 High-level synchronization approach 首选方法是使用Spring基于最高级别模板的持久性集成API，或者将ORM API与事务感知工厂bean或代理一起使用，以管理本地资源工厂。这些事务感知解决方案在内部处理资源创建和重用，清理，资源的可选事务同步以及异常映射。 因此，用户数据访问代码不必解决这些任务，但可以完全专注于非样板持久性逻辑。 通常，您使用本机ORM API或使用模板方法通过使用JdbcTemplate进行JDBC访问。 Low-level synchronization approach Classes such as DataSourceUtils (for JDBC), EntityManagerFactoryUtils (for JPA), SessionFactoryUtils (for Hibernate), PersistenceManagerFactoryUtils (for JDO), and so on exist at a lower level. When you want the application code to deal directly with the resource types of the native persistence APIs, you use these classes to ensure that proper Spring Framework-managed instances are obtained, transactions are (optionally) synchronized, and exceptions that occur in the process are properly mapped to a consistent API. For example, in the case of JDBC, instead of the traditional JDBC approach of calling the getConnection() method on the DataSource, you instead use Spring’s org.springframework.jdbc.datasource.DataSourceUtils class as follows: 1Connection conn = DataSourceUtils.getConnection(dataSource); If an existing transaction already has a connection synchronized (linked) to it, that instance is returned. Otherwise, the method call triggers the creation of a new connection, which is (optionally) synchronized to any existing transaction, and made available for subsequent reuse in that same transaction. As mentioned, any SQLException is wrapped in a Spring Framework CannotGetJdbcConnectionException, one of the Spring Framework’s hierarchy of unchecked DataAccessExceptions. This approach gives you more information than can be obtained easily from the SQLException, and ensures portability across databases, even across different persistence technologies. This approach also works without Spring transaction management (transaction synchronization is optional), so you can use it whether or not you are using Spring for transaction management. 声明式事务管理大多数Spring Framework用户选择声明式事务管理。此选项对应用程序代码的影响最小，因此最符合非侵入式轻量级容器的理想。 关于Spring Framework的声明式事务支持，最重要的概念是通过AOP代理启用此支持，并且事务切面由元数据（当前基于XML或基于注释）驱动。 AOP与事务元数据的组合产生一个AOP代理，该代理使用TransactionInterceptor和适当的PlatformTransactionManager实现来驱动围绕方法调用的事务。 从概念上讲，在事务代理上调用方法如下图所示： 基于XML使用示例123456789101112131415// the service interface that we want to make transactionalpackage x.y.service;public interface FooService { Foo getFoo(String fooName); Foo getFoo(String fooName, String barName); void insertFoo(Foo foo); void updateFoo(Foo foo);} 1234567891011121314151617181920212223// an implementation of the above interfacepackage x.y.service;public class DefaultFooService implements FooService { public Foo getFoo(String fooName) { throw new UnsupportedOperationException(); } public Foo getFoo(String fooName, String barName) { throw new UnsupportedOperationException(); } public void insertFoo(Foo foo) { throw new UnsupportedOperationException(); } public void updateFoo(Foo foo) { throw new UnsupportedOperationException(); }} 假设FooService接口的前两个方法getFoo（String）和getFoo（String，String）必须在具有只读语义的事务的上下文中执行，并且其他方法，insertFoo（Foo）和updateFoo（ Foo），必须在具有读写语义的事务的上下文中执行。则配置文件如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&lt;!-- from the file 'context.xml' --&gt;&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd&quot;&gt; &lt;!-- this is the service object that we want to make transactional --&gt; &lt;bean id=&quot;fooService&quot; class=&quot;x.y.service.DefaultFooService&quot;/&gt; &lt;!-- the transactional advice (what 'happens'; see the &lt;aop:advisor/&gt; bean below) --&gt; &lt;tx:advice id=&quot;txAdvice&quot; transaction-manager=&quot;txManager&quot;&gt; &lt;!-- the transactional semantics... --&gt; &lt;tx:attributes&gt; &lt;!-- all methods starting with 'get' are read-only --&gt; &lt;tx:method name=&quot;get*&quot; read-only=&quot;true&quot;/&gt; &lt;!-- other methods use the default transaction settings (see below) --&gt; &lt;tx:method name=&quot;*&quot;/&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;!-- ensure that the above transactional advice runs for any execution of an operation defined by the FooService interface --&gt; &lt;aop:config&gt; &lt;aop:pointcut id=&quot;fooServiceOperation&quot; expression=&quot;execution(* x.y.service.FooService.*(..))&quot;/&gt; &lt;aop:advisor advice-ref=&quot;txAdvice&quot; pointcut-ref=&quot;fooServiceOperation&quot;/&gt; &lt;/aop:config&gt; &lt;!-- don't forget the DataSource --&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;org.apache.commons.dbcp.BasicDataSource&quot; destroy-method=&quot;close&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;oracle.jdbc.driver.OracleDriver&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:oracle:thin:@rj-t42:1521:elvis&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;scott&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;tiger&quot;/&gt; &lt;/bean&gt; &lt;!-- similarly, don't forget the PlatformTransactionManager --&gt; &lt;bean id=&quot;txManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;/bean&gt; &lt;!-- other &lt;bean/&gt; definitions here --&gt;&lt;/beans&gt; &lt;tx:advice/&gt;标记的transaction-manager属性设置为将驱动事务的PlatformTransactionManager bean的名称，在本例中为txManager bean。 &lt;aop:config/&gt;定义确保txAdvice bean定义的事务切面在程序中的适当位置执行。 首先，定义一个切入点，该切入点与FooService接口（fooServiceOperation）中定义的任何操作的执行相匹配。 然后使用事务切面将切入点与txAdvice相关联。 结果表明，在执行fooServiceOperation时，将运行txAdvice定义的切面。 验证代码如下： 12345678public final class Boot { public static void main(final String[] args) throws Exception { ApplicationContext ctx = new ClassPathXmlApplicationContext(&quot;context.xml&quot;, Boot.class); FooService fooService = (FooService) ctx.getBean(&quot;fooService&quot;); fooService.insertFoo (new Foo()); }} 日志信息如下： 1234567891011121314151617181920212223242526&lt;!-- the Spring container is starting up... --&gt;[AspectJInvocationContextExposingAdvisorAutoProxyCreator] - Creating implicit proxy for bean 'fooService' with 0 common interceptors and 1 specific interceptors&lt;!-- the DefaultFooService is actually proxied --&gt;[JdkDynamicAopProxy] - Creating JDK dynamic proxy for [x.y.service.DefaultFooService]&lt;!-- ... the insertFoo(..) method is now being invoked on the proxy --&gt;[TransactionInterceptor] - Getting transaction for x.y.service.FooService.insertFoo&lt;!-- the transactional advice kicks in here... --&gt;[DataSourceTransactionManager] - Creating new transaction with name [x.y.service.FooService.insertFoo][DataSourceTransactionManager] - Acquired Connection [org.apache.commons.dbcp.PoolableConnection@a53de4] for JDBC transaction&lt;!-- the insertFoo(..) method from DefaultFooService throws an exception... --&gt;[RuleBasedTransactionAttribute] - Applying rules to determine whether transaction should rollback on java.lang.UnsupportedOperationException[TransactionInterceptor] - Invoking rollback for transaction on x.y.service.FooService.insertFoo due to throwable [java.lang.UnsupportedOperationException]&lt;!-- and the transaction is rolled back (by default, RuntimeException instances cause rollback) --&gt;[DataSourceTransactionManager] - Rolling back JDBC transaction on Connection [org.apache.commons.dbcp.PoolableConnection@a53de4][DataSourceTransactionManager] - Releasing JDBC Connection after transaction[DataSourceUtils] - Returning JDBC Connection to DataSourceException in thread &quot;main&quot; java.lang.UnsupportedOperationException at x.y.service.DefaultFooService.insertFoo(DefaultFooService.java:14)&lt;!-- AOP infrastructure stack trace elements removed for clarity --&gt;at $Proxy0.insertFoo(Unknown Source)at Boot.main(Boot.java:11) 回滚声明式事务Spring Framework的事务回滚的推荐方法是从当前在事务上下文中执行的代码中抛出异常。 Spring Framework的事务捕获任何未处理的异常，因为它会使调用堆栈冒泡，并确定是否将事务标记为回滚。 默认配置中，Spring Framework的事务仅在运行时未经检查的异常情况下标记用于回滚的事务; 也就是说，抛出的异常是RuntimeException的实例或子类。从事务方法抛出的已检查异常不会导致在默认配置中回滚。 您可以准确配置哪些Exception类型标记用于回滚的事务，包括已检查的异常。以下XML代码段演示了如何为已检查的特定于应用程序的Exception类型配置回滚。 123456&lt;tx:advice id=&quot;txAdvice&quot; transaction-manager=&quot;txManager&quot;&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;get*&quot; read-only=&quot;true&quot; rollback-for=&quot;NoProductInStockException&quot;/&gt; &lt;tx:method name=&quot;*&quot;/&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt; 如果您不希望在抛出异常时回滚事务，也可以指定“无回滚规则”。 以下示例告诉Spring Framework的事务基础结构即使面对未处理的InstrumentNotFoundException也要提交事务。 123456&lt;tx:advice id=&quot;txAdvice&quot;&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;updateStock&quot; no-rollback-for=&quot;InstrumentNotFoundException&quot;/&gt; &lt;tx:method name=&quot;*&quot;/&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt; 当Spring Framework的事务捕获异常并参考配置的回滚规则以确定是否将事务标记为回滚时，最强匹配规则将获胜。 因此，在以下配置的情况下，除InstrumentNotFoundException之外的任何异常都会导致后续事务的回滚。 12345&lt;tx:advice id=&quot;txAdvice&quot;&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;*&quot; rollback-for=&quot;Throwable&quot; no-rollback-for=&quot;InstrumentNotFoundException&quot;/&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt; 还可以以编程方式指示所需的回滚。虽然非常简单，但这个过程非常具有侵入性，并且将您的代码紧密地耦合到Spring Framework的事务中： 12345678public void resolvePosition() { try { // some business logic... } catch (NoProductInStockException ex) { // trigger rollback programmatically TransactionAspectSupport.currentTransactionStatus().setRollbackOnly(); }} 不同的bean配置不同的事务语义12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd&quot;&gt; &lt;aop:config&gt; &lt;aop:pointcut id=&quot;defaultServiceOperation&quot; expression=&quot;execution(* x.y.service.*Service.*(..))&quot;/&gt; &lt;aop:pointcut id=&quot;noTxServiceOperation&quot; expression=&quot;execution(* x.y.service.ddl.DefaultDdlManager.*(..))&quot;/&gt; &lt;aop:advisor pointcut-ref=&quot;defaultServiceOperation&quot; advice-ref=&quot;defaultTxAdvice&quot;/&gt; &lt;aop:advisor pointcut-ref=&quot;noTxServiceOperation&quot; advice-ref=&quot;noTxAdvice&quot;/&gt; &lt;/aop:config&gt; &lt;!-- this bean will be transactional (see the 'defaultServiceOperation' pointcut) --&gt; &lt;bean id=&quot;fooService&quot; class=&quot;x.y.service.DefaultFooService&quot;/&gt; &lt;!-- this bean will also be transactional, but with totally different transactional settings --&gt; &lt;bean id=&quot;anotherFooService&quot; class=&quot;x.y.service.ddl.DefaultDdlManager&quot;/&gt; &lt;tx:advice id=&quot;defaultTxAdvice&quot;&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;get*&quot; read-only=&quot;true&quot;/&gt; &lt;tx:method name=&quot;*&quot;/&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;tx:advice id=&quot;noTxAdvice&quot;&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;*&quot; propagation=&quot;NEVER&quot;/&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;!-- other transaction infrastructure beans such as a PlatformTransactionManager omitted... --&gt;&lt;/beans&gt; Using @Transactional除了基于XML的事务配置声明方法之外，您还可以使用基于注释的方法。 123456789101112// the service class that we want to make transactional@Transactionalpublic class DefaultFooService implements FooService { Foo getFoo(String fooName); Foo getFoo(String fooName, String barName); void insertFoo(Foo foo); void updateFoo(Foo foo);} 当上面的POJO被定义为Spring IoC容器中的bean时，可以通过仅添加一行XML配置来使bean实例成为事务性的： 12345678910111213141516171819202122232425262728&lt;!-- from the file 'context.xml' --&gt;&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd&quot;&gt; &lt;!-- this is the service object that we want to make transactional --&gt; &lt;bean id=&quot;fooService&quot; class=&quot;x.y.service.DefaultFooService&quot;/&gt; &lt;!-- enable the configuration of transactional behavior based on annotations --&gt; &lt;!-- a PlatformTransactionManager is still required --&gt; &lt;tx:annotation-driven transaction-manager=&quot;txManager&quot;/&gt; &lt;bean id=&quot;txManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;!-- (this dependency is defined somewhere else) --&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;/bean&gt; &lt;!-- other &lt;bean/&gt; definitions here --&gt;&lt;/beans&gt; 如果要连接的PlatformTransactionManager的bean名称具有名称transactionManager，则可以省略&lt;tx：annotation-driven /&gt;标记中的transaction-manager属性。 如果要依赖注入的PlatformTransactionManager bean具有任何其他名称，则必须显式使用transaction-manager属性，如前面的示例所示。 如果使用基于Java的配置，则@EnableTransactionManagement批注提供等效支持。 只需将注释添加到@Configuration类即可。 Method visibility and @Transactional： 使用代理时，应仅将@Transactional注释应用于具有公共可见性的方法。 如果使用@Transactional注释对带保护的，私有的或包可见的方法进行注释，则不会引发错误，但带注释的方法不会显示已配置的事务设置。 如果需要注释非公共方法，请考虑使用AspectJ。 可以在接口定义，接口上的方法，类定义或类的公共方法之前放置@Transactional注释。 但是，仅仅存在@Transactional注释不足以激活事务行为。 @Transactional注释只是元数据，可由@Transactional-aware的某些运行时基础结构使用，并且可以使用元数据来配置具有事务行为的适当bean。 如果您希望自我调用也包含在事务中，请考虑使用AspectJ模式。 在这种情况下，首先不会有代理; 相反，目标类将被编织（即，它的字节代码将被修改），以便在任何类型的方法上将@Transactional转换为运行时行为。 Multiple Transaction Managers with @Transactional大多数Spring应用程序只需要一个事务管理器，但在某些情况下，您可能需要在单个应用程序中使用多个独立的事务管理器。 @Transactional注释的value属性可用于选择性地指定要使用的PlatformTransactionManager的标识。 12345678public class TransactionalService { @Transactional(&quot;order&quot;) public void setSomething(String name) { ... } @Transactional(&quot;account&quot;) public void doSomething() { ... }} 1234567891011&lt;tx:annotation-driven/&gt; &lt;bean id=&quot;transactionManager1&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; ... &lt;qualifier value=&quot;order&quot;/&gt; &lt;/bean&gt; &lt;bean id=&quot;transactionManager2&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; ... &lt;qualifier value=&quot;account&quot;/&gt; &lt;/bean&gt; 如果您发现许多不同方法在@Transactional上重复使用相同的属性，那么Spring的元注释支持允许您为特定用例定义自定义快捷方式注释。例如，定义以下注释： 1234567891011@Target({ElementType.METHOD, ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)@Transactional(&quot;order&quot;)public @interface OrderTx {}@Target({ElementType.METHOD, ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)@Transactional(&quot;account&quot;)public @interface AccountTx {} 应用自定义注释到TransactionalService，如下： 12345678public class TransactionalService { @OrderTx public void setSomething(String name) { ... } @AccountTx public void doSomething() { ... }} 事务传播Spring通过事务传播行为控制当前的事务如何传播到被嵌套调用的目标服务接口方法中。TransactionDefinition接口中规定了7种类型的事务传播行为，如下： PROPAGATION_REQUIRED如果当前没有事务，则新建一个事务；如果已经存在一个事务，则加入到这个事务中。这是最常见的选择。 PROPAGATION_SUPPORTS支持当前事务。如果当前没有事务，则以非事务方式执行。 PROPAGATION_MANDATORY使用当前事务。如果当前没有事务，则抛出异常。 PROPAGATION_REQUIRES_NEW新建事务。如果当前存在事务，则把当前事务挂起。 PROPAGATION_NOT_SUPPORTED以非事务方式执行。如果当前存在事务，则抛出异常。 PROPAGATION_NEVER以非事务方式执行。如果当前存在事务，则抛出异常。 PROPAGATION_NESTED如果当前存在事务，则在嵌套事务内执行；如果当前没有事务，则执行与PROPAGATION_REQUIRED类似的操作。 在Spring管理的事务中，请注意物理和逻辑事务之间的区别，以及传播设置如何应用于此差异。 Required 当传播设置为PROPAGATION_REQUIRED时，将为应用该设置的每个方法创建逻辑事务范围。每个这样的逻辑事务范围可以单独设置仅回滚状态，外部事务范围在逻辑上独立于内部事务范围。当然，在标准PROPAGATION_REQUIRED行为的情况下，所有这些范围将映射到同一物理事务。因此，内部事务范围中的回滚标记确实会影响外部事务实际提交的机会。 但是，在内部事务作用域设置仅回滚标记的情况下，外部事务尚未决定回滚本身，因此回滚（由内部事务作用域静默触发）是意外的。此时抛出相应的UnexpectedRollbackException。这是预期的行为，因此事务的调用者永远不会被误导，假设在实际上没有执行提交。因此，如果内部事务（外部调用者不知道）以静默方式将事务标记为仅回滚，则外部调用者仍会调用commit。外部调用者需要接收UnexpectedRollbackException以清楚地指示已执行回滚。 RequiresNew 与PROPAGATION_REQUIRED相比，PROPAGATION_REQUIRES_NEW对每个受影响的事务范围使用完全独立的事务。 在这种情况下，底层物理事务是不同的，因此可以独立提交或回滚，外部事务不受内部事务的回滚状态的影响。 NestedPROPAGATION_NESTED使用具有多个保存点的单个物理事务，它可以回滚到该事务。 这种部分回滚允许内部事务作用域触发其作用域的回滚，外部事务能够继续物理事务，尽管已经回滚了一些操作。 此设置通常映射到JDBC保存点，因此仅适用于JDBC资源事务。 Advising transactional operationsSuppose you want to execute both transactional and some basic profiling advice. How do you effect this in the context of &lt;tx:annotation-driven/&gt;? When you invoke the updateFoo(Foo) method, you want to see the following actions: Configured profiling aspect starts up. Transactional advice executes. Method on the advised object executes. Transaction commits. Profiling aspect reports exact duration of the whole transactional method invocation. Here is the code for a simple profiling aspect discussed above. The ordering of advice is controlled through the Ordered interface. 123456789101112131415161718192021222324252627282930313233package x.y;import org.aspectj.lang.ProceedingJoinPoint;import org.springframework.util.StopWatch;import org.springframework.core.Ordered;public class SimpleProfiler implements Ordered { private int order; // allows us to control the ordering of advice public int getOrder() { return this.order; } public void setOrder(int order) { this.order = order; } // this method is the around advice public Object profile(ProceedingJoinPoint call) throws Throwable { Object returnValue; StopWatch clock = new StopWatch(getClass().getName()); try { clock.start(call.toShortString()); returnValue = call.proceed(); } finally { clock.stop(); System.out.println(clock.prettyPrint()); } return returnValue; }} 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd&quot;&gt; &lt;bean id=&quot;fooService&quot; class=&quot;x.y.service.DefaultFooService&quot;/&gt; &lt;!-- this is the aspect --&gt; &lt;bean id=&quot;profiler&quot; class=&quot;x.y.SimpleProfiler&quot;&gt; &lt;!-- execute before the transactional advice (hence the lower order number) --&gt; &lt;property name=&quot;order&quot; value=&quot;1&quot;/&gt; &lt;/bean&gt; &lt;tx:annotation-driven transaction-manager=&quot;txManager&quot; order=&quot;200&quot;/&gt; &lt;aop:config&gt; &lt;!-- this advice will execute around the transactional advice --&gt; &lt;aop:aspect id=&quot;profilingAspect&quot; ref=&quot;profiler&quot;&gt; &lt;aop:pointcut id=&quot;serviceMethodWithReturnValue&quot; expression=&quot;execution(!void x.y..*Service.*(..))&quot;/&gt; &lt;aop:around method=&quot;profile&quot; pointcut-ref=&quot;serviceMethodWithReturnValue&quot;/&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;org.apache.commons.dbcp.BasicDataSource&quot; destroy-method=&quot;close&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;oracle.jdbc.driver.OracleDriver&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:oracle:thin:@rj-t42:1521:elvis&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;scott&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;tiger&quot;/&gt; &lt;/bean&gt; &lt;bean id=&quot;txManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; The following example effects the same setup as above, but uses the purely XML declarative approach. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd&quot;&gt; &lt;bean id=&quot;fooService&quot; class=&quot;x.y.service.DefaultFooService&quot;/&gt; &lt;!-- the profiling advice --&gt; &lt;bean id=&quot;profiler&quot; class=&quot;x.y.SimpleProfiler&quot;&gt; &lt;!-- execute before the transactional advice (hence the lower order number) --&gt; &lt;property name=&quot;order&quot; value=&quot;1&quot;/&gt; &lt;/bean&gt; &lt;aop:config&gt; &lt;aop:pointcut id=&quot;entryPointMethod&quot; expression=&quot;execution(* x.y..*Service.*(..))&quot;/&gt; &lt;!-- will execute after the profiling advice (c.f. the order attribute) --&gt; &lt;aop:advisor advice-ref=&quot;txAdvice&quot; pointcut-ref=&quot;entryPointMethod&quot; __order=&quot;2&quot;/&gt; &lt;!-- order value is higher than the profiling aspect --&gt; &lt;aop:aspect id=&quot;profilingAspect&quot; ref=&quot;profiler&quot;&gt; &lt;aop:pointcut id=&quot;serviceMethodWithReturnValue&quot; expression=&quot;execution(!void x.y..*Service.*(..))&quot;/&gt; &lt;aop:around method=&quot;profile&quot; pointcut-ref=&quot;serviceMethodWithReturnValue&quot;/&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt; &lt;tx:advice id=&quot;txAdvice&quot; transaction-manager=&quot;txManager&quot;&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;get*&quot; read-only=&quot;true&quot;/&gt; &lt;tx:method name=&quot;*&quot;/&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;!-- other &lt;bean/&gt; definitions such as a DataSource and a PlatformTransactionManager here --&gt;&lt;/beans&gt; Transaction bound event从Spring 4.2开始，事件的监听器可以绑定到事务的一个阶段。 典型的例子是在事务成功完成时处理事件：当前事务的结果 对于监听器实际上很重要时，这允许更灵活地使用事件。 注册常规事件侦听器是通过@EventListener注释完成的。如果需要将其绑定到事务，请使用@TransactionalEventListener。执行此操作时，默认情况下，侦听器将绑定到事务的提交阶段。 1234567891011121314151617181920212223242526@Servicepublic class DefaultFooService implements FooService { public Foo getFoo(String fooName) { return jdbcTemplate.queryForObject(&quot;SELECT * FROM foo where name = ?&quot;, new Object[]{fooName}, Foo.class); } @Transactional public void insertFoo(Foo foo) { jdbcTemplate.update(&quot;INSERT INTO foo(name) VALUES (?)&quot;, new Object[]{foo.getName()}, new int[]{Types.VARCHAR}); // 如果Foo没有继承ApplicationEvent, 则内部会包装为PayloadApplicationEvent。 // 发布事件, 事务提交后, 记录日志, 或发送消息等操作。 applicationEventPublisher.publishEvent(foo); //当事务提交后, 才会真正的执行@TransactionalEventListener配置的Listener, 因此Listener抛异常, 方法返回失败, 但事务不会回滚. } @Resource private JdbcTemplate jdbcTemplate; @Resource private ApplicationEventPublisher applicationEventPublisher;} 12345678910@Componentpublic class FooServiceListener { @TransactionalEventListener public void handler(PayloadApplicationEvent&lt;FooService.Foo&gt; creationEvent) { FooService.Foo foo = creationEvent.getPayload(); System.out.println(&quot;======&quot;+foo.getName()); System.out.println(1/0); }} TransactionalEventListener注释提供了一个阶段属性，该属性允许自定义侦听器应绑定到的事务的哪个阶段。 有效阶段是BEFORE_COMMIT，AFTER_COMMIT（默认值），AFTER_ROLLBACK和AFTER_COMPLETION，它们聚合事务完成（无论是提交还是回滚）。 如果没有正在运行的事务，则根本不调用侦听器，因为我们无法遵守所需的语义。 但是，可以通过将注释的fallbackExecution属性设置为true来覆盖该行为。 参考资料 Spring官方文档 Spring4.2新特性(一)","link":"/posts/1749.html"},{"title":"计算机网络：TCP&#x2F;IP","text":"OSI 7层模型和TCP/IP四层模型为了增强通用性和兼容性，计算机网络都被设计成层次结构，每一层都遵守一定的规则。因此有了OSI这样一个抽象的网络通信参考模型，按照这个标准使计算机网络系统可以互相连接。 物理层 通过网线、光缆等这种物理方式将电脑设备连接起来。传递的数据是比特流，0101010100。 数据链路层 首先，把比特流封装成数据帧的格式，对0、1进行分组。电脑设备连接起来之后，数据都经过网卡来传输，而网卡上定义了全世界唯一的MAC地址。然后再通过广播的形式向局域网内所有电脑发送数据，再根据数据中MAC地址和自身对比判断是否是发给自己的。 网络层 广播的形式太低效，为了区分哪些MAC地址属于同一个子网，网络层定义了IP和子网掩码，通过对IP和子网掩码进行与运算就知道是否是同一个子网，再通过路由器和交换机进行传输。IP协议属于网络层的协议。 传输层 有了网络层的MAC+IP地址之后，为了确定数据包是从哪个进程发送过来的，就需要端口号，通过端口来建立通信，比如TCP和UDP属于这一层的协议。 会话层 负责建立和断开连接。 表示层 为了使得数据能够被其他的计算机理解，再次将数据转换成另外一种格式，比如文字、视频、图片等。 应用层 最高层，面对用户，提供计算机网络与最终呈现给用户的界面。 TCP/IP是四层的结构，相当于是对OSI模型的简化。 数据链路层 也有称作网络访问层、网络接口层。它包含了OSI模型的物理层和数据链路层，把电脑设备连接起来。 网络层 也叫做IP层，处理IP数据包的传输、路由，建立主机间的通信。 传输层 为两台主机设备提供端到端的通信。 应用层 包含OSI的会话层、表示层和应用层，提供了一些常用的协议规范，比如FTP、SMPT、HTTP等。 总结下来，就是物理层通过物理手段把电脑设备连接起来，数据链路层则对比特流的数据进行分组，网络层来建立主机到主机的通信，传输层建立端口到端口的通信，应用层最终负责建立连接，数据格式转换，最终呈现给用户。 什么是TCP？TCP 是面向连接的、可靠的、基于字节流的传输层通信协议。 面向连接：一定是「一对一」才能连接，不能像 UDP 协议可以一个主机同时向多个主机发送消息，也就是一对多是无法做到的； 可靠的：无论的网络链路中出现了怎样的链路变化，TCP 都可以保证一个报文一定能够到达接收端； 字节流：消息是「没有边界」的，所以无论我们消息有多大都可以进行传输。并且消息是「有序的」，当「前一个」消息没有收到的时候，即使它先收到了后面的字节，那么也不能扔给应用层去处理，同时对「重复」的报文会自动丢弃。 什么是TCP连接？RFC 793 对连接定义如下： Connections: The reliability and flow control mechanisms described above require that TCPs initialize and maintain certain status information for each data stream. The combination of this information, including sockets, sequence numbers, and window sizes, is called a connection. 简单来说就是，用于保证可靠性和流量控制维护的某些状态信息，这些信息的组合，包括Socket、序列号和窗口大小称为连接。 建立一个 TCP 连接是需要客户端与服务器端达成上述三个信息的共识。 Socket：由 IP 地址和端口号组成。 序列号：用来追踪通信发起方发送的数据包序号，接收方可以通过序列号向发送方确认某个数据包的成功接收。 窗口大小：用来做流量控制。 如何唯一确定一个TCP连接呢？TCP 四元组可以唯一的确定一个连接，四元组包括如下： 源地址和目的地址的字段（32位）是在 IP 头部中，作用是通过 IP 协议发送报文给对方主机。 源端口和目的端口的字段（16位）是在 TCP 头部中，作用是告诉 TCP 协议应该把报文发给哪个进程。 服务器监听了一个端口，它的TCP 的最大连接数是多少？客户端 IP 和 端口是可变的，其理论值计算公式如下: 1最大TCP连接数 = 客户端的IP数 * 客户端的端口数 当然，服务端最大并发 TCP 连接数远不能达到理论上限。 首先主要是文件描述符限制，Socket 都是文件，所以首先要通过 ulimit 配置文件描述符的数目； 另一个是内存限制，每个 TCP 连接都要占用一定内存，操作系统的内存是有限的。 TCP头部格式 序列号：在建立连接时由计算机生成的随机数作为其初始值，通过 SYN 包传给接收端主机，每发送一次数据，就「累加」一次该「数据字节数」的大小。用来解决网络包乱序问题。 确认应答号：指下一次「期望」收到的数据的序列号，发送端收到这个确认应答以后可以认为在这个序号以前的数据都已经被正常接收。用来解决不丢包的问题。 控制位： ACK：该位为 1 时，「确认应答」的字段变为有效，TCP 规定除了最初建立连接时的 SYN 包之外该位必须设置为 1 。 RST：该位为 1 时，表示 TCP 连接中出现异常必须强制断开连接。 SYN：该位为 1 时，表示希望建立连接，并在其「序列号」的字段进行序列号初始值的设定。 FIN：该位为 1 时，表示今后不会再有数据发送，希望断开连接。当通信结束希望断开连接时，通信双方的主机之间就可以相互交换 FIN 位为 1 的 TCP 段。 窗口大小 (Window)： TCP的流量控制由连接的每一端通过声明的窗口大小来提供。窗口大小为字节数，起始于确认序号字段指明的值，这个值是接收端正期望接收的字节。窗口大小是一个 16 bit 字段，单位是字节， 因而窗口大小最大为 65535 字节。 检验和 (Checksum)：功能类似于数字签名，用于验证数据完整性，也就是确保数据未被修改。检验和覆盖了整个 TCP 报文段，包括 TCP 首部和 TCP 数据，发送端根据特定算法对整个报文段计算出一个检验和，接收端会进行计算并验证。 紧急指针 (Urgent Pointer)：当 URG 控制位值为 1 时，此字段生效，紧急指针是一个正的偏移量，和序号字段中的值相加表示紧急数据最后一个字节的序号。 TCP 的紧急方式是发送端向另一端发送紧急数据的一种方式。 选项 (Options)：这一部分是可选字段，也就是非必须字段，最常见的可选字段是“最长报文大小 (MSS，Maximum Segment Size)”。 UDP和TCP有什么区别？UDP 不提供复杂的控制机制，利用 IP 提供面向「无连接」的通信服务。UDP 协议真的非常简，头部只有 8 个字节（ 64 位），UDP 的头部格式如下： UDP 头部格式 目标和源端口：主要是告诉 UDP 协议应该把报文发给哪个进程。 包长度：该字段保存了 UDP 首部的长度跟数据的长度之和。 校验和：校验和是为了提供可靠的 UDP 首部和数据而设计。 TCP 和 UDP 区别： 1. 连接 TCP 是面向连接的传输层协议，传输数据前先要建立连接。 UDP 是不需要连接，即刻传输数据。 2. 服务对象 TCP 是一对一的两点服务，即一条连接只有两个端点。 UDP 支持一对一、一对多、多对多的交互通信 3. 可靠性 TCP 是可靠交付数据的，数据可以无差错、不丢失、不重复、按需到达。 UDP 是尽最大努力交付，不保证可靠交付数据。 4. 拥塞控制、流量控制 TCP 有拥塞控制和流量控制机制，保证数据传输的安全性。 UDP 则没有，即使网络非常拥堵了，也不会影响 UDP 的发送速率。 5. 首部开销 TCP 首部长度较长，会有一定的开销，首部在没有使用「选项」字段时是 20 个字节，如果使用了「选项」字段则会变长的。 UDP 首部只有 8 个字节，并且是固定不变的，开销较小。 6. 传输方式 TCP 是流式传输，没有边界，但保证顺序和可靠。 UDP 是一个包一个包的发送，是有边界的，但可能会丢包和乱序。 7. 分片不同 TCP 的数据大小如果大于 MSS 大小，则会在传输层进行分片，目标主机收到后，也同样在传输层组装 TCP 数据包，如果中途丢失了一个分片，只需要传输丢失的这个分片。 UDP 的数据大小如果大于 MTU 大小，则会在 IP 层进行分片，目标主机收到后，在 IP 层组装完数据，接着再传给传输层，但是如果中途丢了一个分片，则就需要重传所有的数据包，这样传输效率非常差，所以通常 UDP 的报文应该小于 MTU。 TCP和UDP应用场景由于 TCP 是面向连接，能保证数据的可靠性交付，因此经常用于： FTP 文件传输 HTTP / HTTPS 由于 UDP 面向无连接，它可以随时发送数据，再加上UDP本身的处理既简单又高效，因此经常用于： 包总量较少的通信，如 DNS 、SNMP 等 视频、音频等多媒体通信 广播通信 TCP三次握手概述 客户端和服务端都处于 CLOSED 状态，先是服务端主动监听某个端口，处于 LISTEN 状态。 客户端会随机初始化序号（client_isn），将此序号置于 TCP 首部的「序号」字段中，同时把 SYN 标志位置为 1 ，表示 SYN 报文。接着把第一个 SYN 报文发送给服务端，表示向服务端发起连接，该报文不包含应用层数据，之后客户端处于 SYN-SENT 状态。 服务端收到客户端的 SYN 报文后，首先服务端也随机初始化自己的序号（server_isn），将此序号填入 TCP 首部的「序号」字段中，其次把 TCP 首部的「确认应答号」字段填入 client_isn + 1, 接着把 SYN 和 ACK 标志位置为 1。最后把该报文发给客户端，该报文也不包含应用层数据，之后服务端处于 SYN-RCVD 状态。 客户端收到服务端报文后，还要向服务端回应最后一个应答报文，首先该应答报文 TCP 首部 ACK 标志位置为 1 ，其次「确认应答号」字段填入 server_isn + 1 ，最后把报文发送给服务端，这次报文可以携带客户到服务器的数据，之后客户端处于 ESTABLISHED 状态。 服务器收到客户端的应答报文后，也进入 ESTABLISHED 状态。 一旦完成三次握手，双方都处于 ESTABLISHED 状态，此时连接就已建立完成，客户端和服务端就可以相互发送数据了。 如何在 Linux 系统中查看 TCP 状态？ TCP 的连接状态查看，在 Linux 可以通过 netstat -napt 命令查看。 为什么是三次握手？不是两次、四次？ 比较常回答的是：“因为三次握手才能保证双方具有接收和发送的能力。” 这回答是没问题，但这回答是片面的，并没有说出主要的原因。 可以以下三个方面分析三次握手的原因： 三次握手才可以阻止重复历史连接的初始化（主要原因） 三次握手才可以同步双方的初始序列号 三次握手才可以避免资源浪费 避免历史连接RFC 793 指出的 TCP 连接使用三次握手的首要原因： The principle reason for the three-way handshake is to prevent old duplicate connection initiations from causing confusion. 首要原因是为了防止旧的重复连接初始化造成混乱 网络环境是错综复杂的，往往并不是如我们期望的一样，先发送的数据包，就先到达目标主机，可能会由于网络拥堵等乱七八糟的原因，会使得旧的数据包，先到达目标主机，那么这种情况下 TCP 三次握手是如何避免的呢？ 客户端连续发送多次 SYN 建立连接的报文，在网络拥堵情况下： 一个「旧 SYN 报文」比「最新的 SYN 」 报文早到达了服务端； 那么此时服务端就会回一个 SYN + ACK 报文给客户端； 客户端收到后可以根据自身的上下文，判断这是一个历史连接（序列号过期或超时），那么客户端就会发送 RST 报文给服务端，表示中止这一次连接。 如果是两次握手连接，就不能判断当前连接是否是历史连接，三次握手则可以在客户端（发送方）准备发送第三次报文时，客户端因有足够的上下文来判断当前连接是否是历史连接： 如果是历史连接（序列号过期或超时），则第三次握手发送的报文是 RST 报文，以此中止历史连接； 如果不是历史连接，则第三次发送的报文是 ACK 报文，通信双方就会成功建立连接； 同步双方初始序列号TCP 协议的通信双方， 都必须维护一个「序列号」， 序列号是可靠传输的一个关键因素，它的作用： 接收方可以去除重复的数据； 接收方可以根据数据包的序列号按序接收； 可以标识发送出去的数据包中， 哪些是已经被对方收到的； 可见，序列号在 TCP 连接中占据着非常重要的作用，所以当客户端发送携带「初始序列号」的 SYN 报文的时候，需要服务端回一个 ACK 应答报文，表示客户端的 SYN 报文已被服务端成功接收，那当服务端发送「初始序列号」给客户端的时候，依然也要得到客户端的应答回应，这样一来一回，才能确保双方的初始序列号能被可靠的同步。 四次握手其实也能够可靠的同步双方的初始化序号，但由于第二步和第三步可以优化成一步，所以就成了「三次握手」。 而两次握手只保证了一方的初始序列号能被对方成功接收，没办法保证双方的初始序列号都能被确认接收。 避免资源浪费如果只有「两次握手」，当客户端的 SYN 请求连接在网络中阻塞，客户端没有接收到 ACK 报文，就会重新发送 SYN ，由于没有第三次握手，服务器不清楚客户端是否收到了自己发送的建立连接的 ACK 确认信号，所以每收到一个 SYN 就只能先主动建立一个连接，这会造成什么情况呢？ 两次握手在消息滞留情况下，服务器重复接受无用的连接请求 SYN 报文，而造成重复分配资源。 SYN 攻击假设攻击者短时间伪造不同 IP 地址的 SYN 报文，服务端每接收到一个 SYN 报文，就进入SYN_RCVD 状态，但服务端发送出去的 ACK + SYN 报文，无法得到未知 IP 主机的 ACK 应答，久而久之就会占满服务端的 SYN 接收队列（未连接队列），使得服务器不能为正常用户服务。 如何避免 SYN 攻击？方式一通过修改 Linux 内核参数，控制队列大小和当队列满时应做什么处理。 当网卡接收数据包的速度大于内核处理的速度时，会有一个队列保存这些数据包。可以通过设置net.core.netdev_max_backlog参数控制该队列的最大值。 SYN_RCVD 状态连接的最大个数：net.ipv4.tcp_max_syn_backlog 超出处理能时，对新的 SYN 直接回报 RST，丢弃连接。net.ipv4.tcp_abort_on_overflow 方式二Linux 内核的 SYN （未完成连接建立）队列与 Accpet （已完成连接建立）队列的工作的流程，如下： 当服务端接收到客户端的 SYN 报文时，会将其加入到内核的「 SYN 队列」； 接着发送 SYN + ACK 给客户端，等待客户端回应 ACK 报文； 服务端接收到 ACK 报文后，从「 SYN 队列」移除放入到「 Accept 队列」； 应用通过调用 accpet() socket 接口，从「 Accept 队列」取出连接。 如果应用程序过慢时，就会导致「 Accept 队列」被占满。如下图所示： 如果不断受到 SYN 攻击，就会导致「 SYN 队列」被占满。如下图所示： tcp_syncookies（net.ipv4.tcp_syncookies = 1）方式应对 SYN 攻击。如下图所示： 当 「 SYN 队列」满之后，后续服务器收到 SYN 包，不进入「 SYN 队列」； 计算出一个 cookie 值，再以 SYN + ACK 中的「序列号」返回客户端， 服务端接收到客户端的应答报文时，服务器会检查这个 ACK 包的合法性。如果合法，直接放入到「 Accept 队列」。 最后应用通过调用 accpet() socket 接口，从「 Accept 队列」取出的连接。 已经建立了连接，但客户端突然出现故障了怎么办？TCP具有保活机制，该机制的原理如下： 定义一个时间段，在这个时间段内，如果没有任何连接相关的活动，TCP 保活机制会开始作用，每隔一个时间间隔，发送一个探测报文，该探测报文包含的数据非常少，如果连续几个探测报文都没有得到响应，则认为当前的 TCP 连接已经死亡，系统内核将错误信息通知给上层应用程序。 在 Linux 内核可以有对应的参数可以设置保活时间、保活探测的次数、保活探测的时间间隔，以下都为默认值： 123net.ipv4.tcp_keepalive_time=7200net.ipv4.tcp_keepalive_intvl=75 net.ipv4.tcp_keepalive_probes=9 tcp_keepalive_time=7200：表示保活时间是 7200 秒（2小时），也就 2 小时内如果没有任何连接相关的活动，则会启动保活机制 tcp_keepalive_intvl=75：表示每次检测间隔 75 秒； tcp_keepalive_probes=9：表示检测 9 次无响应，认为对方是不可达的，从而中断本次的连接。 如果开启了 TCP 保活，需要考虑以下几种情况： 对端程序是正常工作的。当 TCP 保活的探测报文发送给对端, 对端会正常响应，这样 TCP 保活时间会被重置，等待下一个 TCP 保活时间的到来。 对端程序崩溃并重启。当 TCP 保活的探测报文发送给对端后，对端是可以响应的，但由于没有该连接的有效信息，会产生一个 RST 报文，这样很快就会发现 TCP 连接已经被重置。 对端程序崩溃，或对端由于其他原因导致报文不可达。当 TCP 保活的探测报文发送给对端后，石沉大海，没有响应，连续几次，达到保活探测次数后，TCP 会报告该 TCP 连接已经死亡。 四次挥手客户单和服务端都可以主动断开连接，断开连接后主机中的「资源」将被释放。 概述下图是以客户端主动关闭连接过程为例 客户端打算关闭连接，此时会发送一个 TCP 首部 FIN 标志位被置为 1 的报文，也即 FIN 报文，之后客户端进入 FIN_WAIT_1 状态。 服务端收到该报文后，就向客户端发送 ACK 应答报文，接着服务端进入 CLOSED_WAIT 状态。 客户端收到服务端的 ACK 应答报文后，之后进入 FIN_WAIT_2 状态。 等待服务端处理完数据后，也向客户端发送 FIN 报文，之后服务端进入 LAST_ACK 状态。 客户端收到服务端的 FIN 报文后，回一个 ACK 应答报文，之后进入 TIME_WAIT 状态 服务器收到了 ACK 应答报文后，就进入了 CLOSED 状态，至此服务端已经完成连接的关闭。 客户端在经过 2MSL 一段时间后，自动进入 CLOSED 状态，至此客户端也完成连接的关闭。 tips：主动关闭连接的，才有 TIME_WAIT 状态 为什么挥手需要四次？ 关闭连接时，客户端向服务端发送 FIN 时，仅仅表示客户端不再发送数据了但是还能接收数据。 服务器收到客户端的 FIN 报文时，先回一个 ACK 应答报文，而服务端可能还有数据需要处理和发送，等服务端不再发送数据时，才发送 FIN 报文给客户端来表示同意现在关闭连接。 服务端通常需要等待完成数据的发送和处理，所以服务端的 ACK 和 FIN 一般都会分开发送，从而比三次握手导致多了一次。 为什么TIME_WAIT等待的时间是2MSL？MSL 是 Maximum Segment Lifetime，报文最大生存时间，它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。因为 TCP 报文基于是 IP 协议的，而 IP 头中有一个 TTL 字段，是 IP 数据报可以经过的最大路由数，每经过一个处理他的路由器此值就减 1，当此值为 0 则数据报将被丢弃，同时发送 ICMP 报文通知源主机。 MSL 与 TTL 的区别： MSL 的单位是时间，而 TTL 是经过路由跳数。所以 MSL 应该要大于等于 TTL 消耗为 0 的时间，以确保报文已被自然消亡。 TIME_WAIT 为什么需要等待 2 倍的MSL ？ 网络中可能存在来自发送方的数据包，当这些发送方的数据包被接收方处理后又会向对方发送响应，所以一来一回需要等待 2 倍的时间。 如果被动关闭方没有收到断开连接的最后的 ACK 报文，就会触发超时重发 Fin 报文，另一方接收到 FIN 后，会重发 ACK 给被动关闭方， 一来一去正好 2 个 MSL。 2MSL 的时间是从客户端接收到 FIN 后发送 ACK 开始计时的。如果在 TIME-WAIT 时间内，因为客户端的 ACK 没有传输到服务端，客户端又接收到了服务端重发的 FIN 报文，那么 2MSL 时间将重新计时。 在 Linux 系统里 2MSL 默认是 60 秒，那么一个 MSL 也就是 30 秒。Linux 系统停留在 TIME_WAIT 的时间为固定的 60 秒。 为什么需要 TIME_WAIT 状态？防止旧连接的数据包假设TIME-WAIT没有等待时间或时间过短，被延迟的数据包抵达后会发生什么呢？ 所以，TCP 就设计出了这么一个机制，经过 2MSL 这个时间，足以让两个方向上的数据包都被丢弃，使得原来连接的数据包在网络中都自然消失，再出现的数据包一定都是新建立连接所产生的。 保证连接正确关闭在 RFC 793 指出 TIME-WAIT 另一个重要的作用是： TIME-WAIT - represents waiting for enough time to pass to be sure the remote TCP received the acknowledgment of its connection termination request. 也就是说，TIME-WAIT 作用是等待足够的时间以确保最后的 ACK 能让被动关闭方接收，从而帮助其正常关闭。 假设 TIME-WAIT 没有等待时间或时间过短，断开连接会造成什么问题呢？ TIME_WAIT 过多有什么危害？如果服务器有处于 TIME-WAIT 状态的 TCP，则说明是由服务器方主动发起的断开请求。 过多的 TIME-WAIT 状态主要的危害有两种： 内存资源占用； 端口资源的占用，一个 TCP 连接至少消耗一个本地端口。 第2个危害造成后果更加严重，端口资源也是有限的。一般可以开启的端口为 32768～61000，也可以通过如下参数设置指定net.ipv4.ip_local_port_range。 客户端受端口资源限制 客户端TIME_WAIT过多，就会导致端口资源被占用，因为端口就65536个，被占满就会导致无法创建新的连接。 服务端受系统资源限制 由于一个四元组表示 TCP 连接，理论上服务端可以建立很多连接，服务端确实只监听一个端口，但是会把连接扔给处理线程，所以理论上监听的端口可以继续监听。但是线程池处理不了那么多一直不断的连接了。所以当服务端出现大量 TIME_WAIT 时，系统资源被占满时，会导致处理不过来新的连接。 如何优化TIME_WAIT？打开 net.ipv4.tcp_tw_reuse 和 net.ipv4.tcp_timestamps 选项开启Linux 内核参数，则可以复用处于 TIME_WAIT 的 socket 为新的连接所用。 12net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_timestamps=1（默认即为 1） tcp_tw_reuse 功能只能用客户端（连接发起方），因为开启了该功能，在调用 connect() 函数时，内核会随机找一个 time_wait 状态超过 1 秒的连接给新的连接复用。 这个时间戳的字段是在 TCP 头部的「选项」里，用于记录 TCP 发送方的当前时间戳和从对端接收到的最新时间戳。 由于引入了时间戳，我们在前面提到的 2MSL 问题就不复存在了，因为重复的数据包会因为时间戳过期被自然丢弃。 net.ipv4.tcp_max_tw_buckets这个值默认为 18000，当系统中处于 TIME_WAIT 的连接一旦超过这个值时，系统就会将后面的 TIME_WAIT 连接状态重置。 程序中使用 SO_LINGER可以通过设置 socket 选项，来设置调用 close 关闭连接行为。 1234struct linger so_linger;so_linger.l_onoff = 1;so_linger.l_linger = 0;setsockopt(s, SOL_SOCKET, SO_LINGER, &amp;so_linger,sizeof(so_linger)); 如果l_onoff为非 0， 且l_linger值为 0，那么调用close后，会立该发送一个RST标志给对端，该 TCP 连接将跳过四次挥手，也就跳过了TIME_WAIT状态，直接关闭。 但这为跨越TIME_WAIT状态提供了一个可能，不过是一个非常危险的行为，不值得提倡。 重传机制TCP 实现可靠传输的方式之一，是通过序列号与确认应答。 在 TCP 中，当发送端的数据到达接收主机时，接收端主机会返回一个确认应答消息，表示已收到消息。 正常的数据传输 但在错综复杂的网络，并不一定能如上图那么顺利能正常的数据传输，万一数据在传输过程中丢失了呢？ TCP 针对数据包丢失的情况，会用重传机制解决。 超时重传在发送数据时，设定一个定时器，当超过指定的时间后，没有收到对方的 ACK 确认应答报文，就会重发该数据，这就是超时重传。 TCP 会在以下两种情况发生超时重传： 数据包丢失 确认应答丢失 如果超时重发的数据，再次超时的时候，又需要重传的时候，TCP 的策略是超时间隔加倍。也就是每当遇到一次超时重传的时候，都会将下一次超时时间间隔设为先前值的两倍。两次超时，就说明网络环境差，不宜频繁反复发送。 超时时间应该设置为多少呢？ 我们先来了解一下什么是 RTT（Round-Trip Time 往返时延），从下图我们就可以知道： RTT RTT 就是数据从网络一端传送到另一端所需的时间，也就是包的往返时间。 超时重传时间是以 RTO （Retransmission Timeout 超时重传时间）表示。 假设在重传的情况下，超时时间 RTO 「较长或较短」时，会发生什么事情呢？ 超时时间较长与较短 上图中有两种超时时间不同的情况： 当超时时间 RTO 较大时，重发就慢，丢了老半天才重发，没有效率，性能差； 当超时时间 RTO 较小时，会导致可能并没有丢就重发，于是重发的就快，会增加网络拥塞，导致更多的超时，更多的超时导致更多的重发。 精确的测量超时时间 RTO 的值是非常重要的，这可让我们的重传机制更高效。 根据上述的两种情况，我们可以得知，超时重传时间 RTO 的值应该略大于报文往返 RTT 的值。 RTO 应略大于 RTT 至此，可能大家觉得超时重传时间 RTO 的值计算，也不是很复杂嘛。 好像就是在发送端发包时记下 t0 ，然后接收端再把这个 ack 回来时再记一个 t1，于是 RTT = t1 – t0。没那么简单，这只是一个采样，不能代表普遍情况。 实际上「报文往返 RTT 的值」是经常变化的，因为我们的网络也是时常变化的。也就因为「报文往返 RTT 的值」 是经常波动变化的，所以「超时重传时间 RTO 的值」应该是一个动态变化的值。 我们来看看 Linux 是如何计算 RTO 的呢？ 估计往返时间，通常需要采样以下两个： 需要 TCP 通过采样 RTT 的时间，然后进行加权平均，算出一个平滑 RTT 的值，而且这个值还是要不断变化的，因为网络状况不断地变化。 除了采样 RTT，还要采样 RTT 的波动范围，这样就避免如果 RTT 有一个大的波动的话，很难被发现的情况。 RFC6289 建议使用以下的公式计算 RTO： RFC6289 建议的 RTO 计算 其中 SRTT 是计算平滑的RTT ，DevRTR 是计算平滑的RTT 与 最新 RTT 的差距。 在 Linux 下，α = 0.125，β = 0.25， μ = 1，∂ = 4。别问怎么来的，问就是大量实验中调出来的。 如果超时重发的数据，再次超时的时候，又需要重传的时候，TCP 的策略是超时间隔加倍。 也就是每当遇到一次超时重传的时候，都会将下一次超时时间间隔设为先前值的两倍。两次超时，就说明网络环境差，不宜频繁反复发送。 超时触发重传存在的问题是，超时周期可能相对较长。那是不是可以有更快的方式呢？ 于是就可以用「快速重传」机制来解决超时重发的时间等待。 快速重传超时触发重传存在的问题是，超时周期可能相对较长。那是不是可以有更快的方式呢？可以用「快速重传」机制来解决超时重发的时间等待。 快速重传机制，是如何工作的呢? 在上图，发送方发出了 1，2，3，4，5 份数据： 第一份 Seq1 先送到了，于是就 Ack 回 2； 结果 Seq2 因为某些原因没收到，Seq3 到达了，于是还是 Ack 回 2； 后面的 Seq4 和 Seq5 都到了，但还是 Ack 回 2，因为 Seq2 还是没有收到； 发送端收到了三个 Ack = 2 的确认，知道了 Seq2 还没有收到，就会在定时器过期之前，重传丢失的 Seq2。 最后，收到了 Seq2，此时因为 Seq3，Seq4，Seq5 都收到了，于是 Ack 回 6 。 快速重传的工作方式是当收到三个相同的 ACK 报文时，会在定时器过期之前，重传丢失的报文段。 快速重传机制只解决了一个问题，就是超时时间的问题，但是它依然面临着另外一个问题。就是重传的时候，是重传之前的一个，还是重传所有的问题。 SACKSACK（ Selective Acknowledgment 选择性确认），该方式需要在 TCP 头部「选项」字段里加一个 SACK 的字段，它可以将缓存的数据发送给发送方，这样发送方就可以知道哪些数据收到了，哪些数据没收到，知道了这些信息，就可以只重传丢失的数据。 如下图，发送方收到了三次同样的 ACK 确认报文，于是就会触发快速重发机制，通过 SACK 信息发现只有 200~299 这段数据丢失，则重发时，就只选择了这个 TCP 段进行重复。 如果要支持 SACK，必须双方都要支持。在 Linux 下，可以通过 net.ipv4.tcp_sack 参数打开这个功能（Linux 2.4 后默认打开）。 D-SACKDuplicate SACK 又称 D-SACK，其主要使用了 SACK 来告诉「发送方」有哪些数据被重复接收了。 下面举例两个栗子，来说明 D-SACK 的作用。 栗子一号：ACK 丢包 ACK 丢包 「接收方」发给「发送方」的两个 ACK 确认应答都丢失了，所以发送方超时后，重传第一个数据包（3000 ~ 3499） 于是「接收方」发现数据是重复收到的，于是回了一个 SACK = 3000~3500，告诉「发送方」 3000~3500 的数据早已被接收了，因为 ACK 都到了 4000 了，已经意味着 4000 之前的所有数据都已收到，所以这个 SACK 就代表着 D-SACK。 这样「发送方」就知道了，数据没有丢，是「接收方」的 ACK 确认报文丢了。 栗子二号：网络延时 网络延时 数据包（1000~1499） 被网络延迟了，导致「发送方」没有收到 Ack 1500 的确认报文。 而后面报文到达的三个相同的 ACK 确认报文，就触发了快速重传机制，但是在重传后，被延迟的数据包（1000~1499）又到了「接收方」； 所以「接收方」回了一个 SACK=1000~1500，因为 ACK 已经到了 3000，所以这个 SACK 是 D-SACK，表示收到了重复的包。 这样发送方就知道快速重传触发的原因不是发出去的包丢了，也不是因为回应的 ACK 包丢了，而是因为网络延迟了。 可见，D-SACK 有这么几个好处： 可以让「发送方」知道，是发出去的包丢了，还是接收方回应的 ACK 包丢了; 可以知道是不是「发送方」的数据包被网络延迟了; 可以知道网络中是不是把「发送方」的数据包给复制了; 在 Linux 下可以通过 net.ipv4.tcp_dsack 参数开启/关闭这个功能（Linux 2.4 后默认打开）。 滑动窗口 TCP 是每发送一个数据，都要进行一次确认应答。当上一个数据包收到了应答了， 再发送下一个。如下图： 这样的传输方式有一个缺点：数据包的往返时间越长，通信的效率就越低。 为解决这个问题，TCP 引入了窗口这个概念。即使在往返时间较长的情况下，它也不会降低网络通信的效率。 那么有了窗口，就可以指定窗口大小，窗口大小就是指无需等待确认应答，而可以继续发送数据的最大值。 窗口的实现实际上是操作系统开辟的一个缓存空间，发送方主机在等到确认应答返回之前，必须在缓冲区中保留已发送的数据。如果按期收到确认应答，此时数据就可以从缓存区清除。 假设窗口大小为 3 个 TCP 段，那么发送方就可以「连续发送」 3 个 TCP 段，并且中途若有 ACK 丢失，可以通过「下一个确认应答进行确认」。如下图： 用滑动窗口方式并行处理 图中的 ACK 600 确认应答报文丢失，也没关系，因为可以通过下一个确认应答进行确认，只要发送方收到了 ACK 700 确认应答，就意味着 700 之前的所有数据「接收方」都收到了。这个模式就叫累计确认或者累计应答。 窗口大小由哪一方决定？ TCP 头里有一个字段叫 Window，也就是窗口大小。这个字段是接收端告诉发送端自己还有多少缓冲区可以接收数据。于是发送端就可以根据这个接收端的处理能力来发送数据，而不会导致接收端处理不过来。 所以，通常窗口的大小是由接收方的窗口大小来决定的。发送方发送的数据大小不能超过接收方的窗口大小，否则接收方就无法正常接收到数据。 发送方的滑动窗口下图就是发送方缓存的数据，根据处理的情况分成四个部分，其中深蓝色方框是发送窗口，紫色方框是可用窗口： #1 是已发送并收到 ACK确认的数据：1~31 字节 #2 是已发送但未收到 ACK确认的数据：32~45 字节 #3 是未发送但总大小在接收方处理范围内（接收方还有空间）：46~51字节 #4 是未发送但总大小超过接收方处理范围（接收方没有空间）：52字节以后 当发送方把数据「全部」都一下发送出去后，可用窗口的大小就为 0 了，表明可用窗口耗尽，在没收到 ACK 确认之前是无法继续发送数据了。 当收到之前发送的数据 32~36 字节的 ACK 确认应答后，如果发送窗口的大小没有变化，则滑动窗口往右边移动 5 个字节，因为有 5 个字节的数据被应答确认，接下来 52~56 字节又变成了可用窗口，那么后续也就可以发送 52~56 这 5 个字节的数据了。 程序是如何表示发送方的四个部分的呢？TCP 滑动窗口方案使用三个指针来跟踪在四个传输类别中的每一个类别中的字节。其中两个指针是绝对指针（指特定的序列号），一个是相对指针（需要做偏移）。 SND.WND：表示发送窗口的大小（大小是由接收方指定的）； SND.UNA：是一个绝对指针，它指向的是已发送但未收到确认的第一个字节的序列号，也就是 #2 的第一个字节。 SND.NXT：也是一个绝对指针，它指向未发送但可发送范围的第一个字节的序列号，也就是 #3 的第一个字节。 指向 #4 的第一个字节是个相对指针，它需要 SND.UNA 指针加上 SND.WND 大小的偏移量，就可以指向 #4 的第一个字节了。 可用窗口大 = SND.WND -（SND.NXT - SND.UNA） 接收方的滑动窗口接收窗口相对简单一些，根据处理的情况划分成三个部分： #1 + #2 是已成功接收并确认的数据（等待应用进程读取）； #3 是未收到数据但可以接收的数据； #4 未收到数据并不可以接收的数据； 其中三个接收部分，使用两个指针进行划分: RCV.WND：表示接收窗口的大小，它会通告给发送方。 RCV.NXT：是一个指针，它指向期望从发送方发送来的下一个数据字节的序列号，也就是 #3 的第一个字节。 指向 #4 的第一个字节是个相对指针，它需要 RCV.NXT 指针加上 RCV.WND 大小的偏移量，就可以指向 #4 的第一个字节了。 接收窗口和发送窗口的大小是相等的吗？ 并不是完全相等，接收窗口的大小是约等于发送窗口的大小的。因为滑动窗口并不是一成不变的。比如，当接收方的应用进程读取数据的速度非常快的话，这样的话接收窗口可以很快的就空缺出来。那么新的接收窗口大小，是通过 TCP 报文中的 Windows 字段来告诉发送方。那么这个传输过程是存在时延的，所以接收窗口和发送窗口是约等于的关系。 流量控制发送方不能无脑的发数据给接收方，要考虑接收方处理能力。如果一直无脑的发数据给对方，但对方处理不过来，那么就会导致触发重发机制，从而导致网络流量的无端的浪费。 为了解决这种现象发生，TCP 提供一种机制可以让「发送方」根据「接收方」的实际接收能力控制发送的数据量，这就是所谓的流量控制。 为了简单起见，假设以下场景： 客户端是接收方，服务端是发送方 假设接收窗口和发送窗口相同，都为 200 假设两个设备在整个传输过程中都保持相同的窗口大小，不受外界影响 根据上图的流量控制，说明下每个过程： 客户端向服务端发送请求数据报文。 服务端收到请求报文后，发送确认报文和 80 字节的数据，于是可用窗口 Usable 减少为 120 字节，同时 SND.NXT 指针也向右偏移 80 字节后，指向 321，这意味着下次发送数据的时候，序列号是 321。 客户端收到 80 字节数据后，于是接收窗口往右移动 80 字节，RCV.NXT 也就指向 321，这意味着客户端期望的下一个报文的序列号是 321，接着发送确认报文给服务端。 服务端再次发送了 120 字节数据，于是可用窗口耗尽为 0，服务端无法再继续发送数据。 客户端收到 120 字节的数据后，于是接收窗口往右移动 120 字节，RCV.NXT 也就指向 441，接着发送确认报文给服务端。 服务端收到对 80 字节数据的确认报文后，SND.UNA 指针往右偏移后指向 321，于是可用窗口 Usable 增大到 80。 服务端收到对 120 字节数据的确认报文后，SND.UNA 指针往右偏移后指向 441，于是可用窗口 Usable 增大到 200。 服务端可以继续发送了，于是发送了 160 字节的数据后，SND.NXT 指向 601，于是可用窗口 Usable 减少到 40。 客户端收到 160 字节后，接收窗口往右移动了 160 字节，RCV.NXT 也就是指向了 601，接着发送确认报文给服务端。 服务端收到对 160 字节数据的确认报文后，发送窗口往右移动了 160 字节，于是 SND.UNA 指针偏移了 160 后指向 601，可用窗口 Usable 也就增大至了 200。 操作系统缓冲区与滑动窗口的关系前面的流量控制例子，我们假定了发送窗口和接收窗口是不变的，但是实际上，发送窗口和接收窗口中所存放的字节数，都是放在操作系统内存缓冲区中的，而操作系统的缓冲区，会被操作系统调整。 当应用进程没办法及时读取缓冲区的内容时，也会对我们的缓冲区造成影响。那操作系统的缓冲区，是如何影响发送窗口和接收窗口的呢？ 当应用程序没有及时读取缓存时，发送窗口和接收窗口的变化。 考虑以下场景： 客户端作为发送方，服务端作为接收方，发送窗口和接收窗口初始大小为 360； 服务端非常的繁忙，当收到客户端的数据时，应用层不能及时读取数据。 根据上图的流量控制，说明下每个过程： 客户端发送 140 字节数据后，可用窗口变为 220 （360 - 140）。 服务端收到 140 字节数据，但是服务端非常繁忙，应用进程只读取了 40 个字节，还有 100 字节占用着缓冲区，于是接收窗口收缩到了 260 （360 - 100），最后发送确认信息时，将窗口大小通告给客户端。 客户端收到确认和窗口通告报文后，发送窗口减少为 260。 客户端发送 180 字节数据，此时可用窗口减少到 80。 服务端收到 180 字节数据，但是应用程序没有读取任何数据，这 180 字节直接就留在了缓冲区，于是接收窗口收缩到了 80 （260 - 180），并在发送确认信息时，通过窗口大小给客户端。 客户端收到确认和窗口通告报文后，发送窗口减少为 80。 客户端发送 80 字节数据后，可用窗口耗尽。 服务端收到 80 字节数据，但是应用程序依然没有读取任何数据，这 80 字节留在了缓冲区，于是接收窗口收缩到了 0，并在发送确认信息时，通过窗口大小给客户端。 客户端收到确认和窗口通告报文后，发送窗口减少为 0。 可见最后窗口都收缩为 0 了，也就是发生了窗口关闭。当发送方可用窗口变为 0 时，发送方实际上会定时发送窗口探测报文，以便知道接收方的窗口是否发生了改变。 当服务端系统资源非常紧张的时候，操作系统可能会直接减少了接收缓冲区大小，这时应用程序又无法及时读取缓存数据，那么这时候就有严重的事情发生了，会出现数据包丢失的现象。 说明下每个过程： 客户端发送 140 字节的数据，于是可用窗口减少到了 220。 服务端因为现在非常的繁忙，操作系统于是就把接收缓存减少了 120 字节，当收到 140 字节数据后，又因为应用程序没有读取任何数据，所以 140 字节留在了缓冲区中，于是接收窗口大小从 360 收缩成了 100，最后发送确认信息时，通告窗口大小给对方。 此时客户端因为还没有收到服务端的通告窗口报文，所以不知道此时接收窗口收缩成了 100，客户端只会看自己的可用窗口还有 220，所以客户端就发送了 180 字节数据，于是可用窗口减少到 40。 服务端收到了 180 字节数据时，发现数据大小超过了接收窗口的大小，于是就把数据包丢失了。 客户端收到第 2 步时，服务端发送的确认报文和通告窗口报文，尝试减少发送窗口到 100，把窗口的右端向左收缩了 80，此时可用窗口的大小就会出现诡异的负值。 所以，如果发生了先减少缓存，再收缩窗口，就会出现丢包的现象。 为了防止这种情况发生，TCP 规定是不允许同时减少缓存又收缩窗口的，而是采用先收缩窗口，过段时间再减少缓存，这样就可以避免了丢包情况。 窗口关闭如果窗口大小为 0 时，就会阻止发送方给接收方传递数据，直到窗口变为非 0 为止，这就是窗口关闭。 接收方向发送方通告窗口大小时，是通过 ACK 报文来通告的。那么，当发生窗口关闭时，接收方处理完数据后，会向发送方通告一个窗口非 0 的 ACK 报文，如果这个通告窗口的 ACK 报文在网络中丢失了，那麻烦就大了。 窗 这会导致发送方一直等待接收方的非 0 窗口通知，接收方也一直等待发送方的数据，如不采取措施，这种相互等待的过程，会造成了死锁的现象。 TCP 是如何解决窗口关闭时，潜在的死锁现象呢？ 为了解决这个问题，TCP 为每个连接设有一个持续定时器，只要 TCP 连接一方收到对方的零窗口通知，就启动持续计时器。 如果持续计时器超时，就会发送窗口探测 ( Window probe ) 报文，而对方在确认这个探测报文时，给出自己现在的接收窗口大小。 窗口探测 如果接收窗口仍然为 0，那么收到这个报文的一方就会重新启动持续计时器； 如果接收窗口不是 0，那么死锁的局面就可以被打破了。 窗口探测的次数一般为 3 次，每次大约 30-60 秒（不同的实现可能会不一样）。如果 3 次过后接收窗口还是 0 的话，有的 TCP 实现就会发 RST 报文来中断连接。 糊涂窗口综合症如果接收方太忙了，来不及取走接收窗口里的数据，那么就会导致发送方的发送窗口越来越小。 到最后，如果接收方腾出几个字节并告诉发送方现在有几个字节的窗口，而发送方会义无反顾地发送这几个字节，这就是糊涂窗口综合症。 要知道，我们的 TCP + IP 头有 40 个字节，为了传输那几个字节的数据，要达上这么大的开销，这太不经济了。 就好像一个可以承载 50 人的大巴车，每次来了一两个人，就直接发车。除非家里有矿的大巴司机，才敢这样玩，不然迟早破产。要解决这个问题也不难，大巴司机等乘客数量超过了 25 个，才认定可以发车。 现举个糊涂窗口综合症的栗子，考虑以下场景： 接收方的窗口大小是 360 字节，但接收方由于某些原因陷入困境，假设接收方的应用层读取的能力如下： 接收方每接收 3 个字节，应用程序就只能从缓冲区中读取 1 个字节的数据； 在下一个发送方的 TCP 段到达之前，应用程序还从缓冲区中读取了 40 个额外的字节； 每个过程的窗口大小的变化，在图中都描述的很清楚了，可以发现窗口不断减少了，并且发送的数据都是比较小的了。 所以，糊涂窗口综合症的现象是可以发生在发送方和接收方： 接收方可以通告一个小的窗口 而发送方可以发送小数据 于是，要解决糊涂窗口综合症，就解决上面两个问题就可以了 让接收方不通告小窗口给发送方 让发送方避免发送小数据 怎么让接收方不通告小窗口呢？ 接收方通常的策略如下: 当「窗口大小」小于 min( MSS，缓存空间/2 ) ，也就是小于 MSS 与 1/2 缓存大小中的最小值时，就会向发送方通告窗口为 0，也就阻止了发送方再发数据过来。 等到接收方处理了一些数据后，窗口大小 &gt;= MSS，或者接收方缓存空间有一半可以使用，就可以把窗口打开让发送方发送数据过来。 怎么让发送方避免发送小数据呢？ 发送方通常的策略: 使用 Nagle 算法，该算法的思路是延时处理，它满足以下两个条件中的一条才可以发送数据： 要等到窗口大小 &gt;= MSS 或是 数据大小 &gt;= MSS 收到之前发送数据的 ack 回包 只要没满足上面条件中的一条，发送方一直在囤积数据，直到满足上面的发送条件。 另外，Nagle 算法默认是打开的，如果对于一些需要小数据包交互的场景的程序，比如，telnet 或 ssh 这样的交互性比较强的程序，则需要关闭 Nagle 算法。 可以在 Socket 设置 TCP_NODELAY 选项来关闭这个算法（关闭 Nagle 算法没有全局参数，需要根据每个应用自己的特点来关闭） 1setsockopt(sock_fd, IPPROTO_TCP, TCP_NODELAY, (char *)&amp;value, sizeof(int)); 拥塞控制流量控制是避免【发送方】的数据填满【接收方】的缓存，但是并不知道网络的中发生了什么。 一般来说，计算机网络都处在一个共享的环境。因此也有可能会因为其他主机之间的通信使得网络拥堵。 在网络出现拥堵时，如果继续发送大量数据包，可能会导致数据包时延、丢失等，这时 TCP 就会重传数据，但是一重传就会导致网络的负担更重，于是会导致更大的延迟以及更多的丢包，这个情况就会进入恶性循环被不断地放大…. 所以，TCP 不能忽略网络上发生的事，它被设计成一个无私的协议，当网络发送拥塞时，TCP 会自我牺牲，降低发送的数据量。于是，就有了拥塞控制，控制的目的就是避免「发送方」的数据填满整个网络。 拥塞窗口为了在【发送方】调节所要发送数据的量，定义了一个叫做「拥塞窗口」的概念。 拥塞窗口cwnd是发送方维护的一个的状态变量，它会根据网络的拥塞程度动态变化的。前面提到过发送窗口 swnd 和接收窗口 rwnd 是约等于的关系，那么由于加入了拥塞窗口的概念后，此时发送窗口的值是swnd = min(cwnd, rwnd)，也就是拥塞窗口和接收窗口中的最小值。 拥塞窗口 cwnd 变化的规则： 只要网络中没有出现拥塞，cwnd 就会增大； 但网络中出现了拥塞，cwnd 就减少 那么知道当前网络是否出现了拥塞呢？ 其实只要「发送方」没有在规定时间内接收到 ACK 应答报文，也就是发生了超时重传，就会认为网络出现了用拥塞。 拥塞控制算法慢启动TCP 在刚建立连接完成后，首先是有个慢启动的过程，就是一点一点的提高发送数据包的数量，如果一上来就发大量的数据，这不是给网络添堵吗？ 慢启动的算法记住一个规则就行：当发送方每收到一个 ACK，拥塞窗口 cwnd 的大小就会加 1。 假定拥塞窗口 cwnd 和发送窗口 swnd 相等，如下图所示： 连接建立完成后，一开始初始化 cwnd = 1，表示可以传一个 MSS 大小的数据； 当收到一个 ACK 确认应答后，cwnd 增加 1，于是一次能够发送 2 个； 当收到 2 个的 ACK 确认应答后， cwnd 增加 2，于是就可以比之前多发2 个，所以这一次能够发送 4 个； 当这 4 个的 ACK 确认到来的时候，每个确认 cwnd 增加 1， 4 个确认 cwnd 增加 4，于是就可以比之前多发 4 个，所以这一次能够发送 8 个。 可以看出慢启动算法，发包的个数是指数性的增长。当cwnd &gt;= ssthresh（慢启动门限状态变量） 时，就会使用「拥塞避免算法」。 拥塞避免当网络出现拥塞，也就是会发生数据包重传，重传机制主要超时重传和快速重传。当发生了超时重传，则就会使用拥塞避免算法。 重新开始慢启动，慢启动是会突然减少数据流的。这真是一旦「超时重传」，马上回到解放前。但是这种方式太激进了，反应也很强烈，会造成网络卡顿。 快速恢复快速重传和快速恢复算法一般同时使用，快速恢复算法是认为，还能收到 3 个重复 ACK 说明网络也不那么糟糕。 在进入快速恢复之前，cwnd 和 ssthresh 已被更新了： cwnd = cwnd/2 ，也就是设置为原来的一半; ssthresh = cwnd; 然后，进入快速恢复算法如下： 拥塞窗口 cwnd = ssthresh + 3 （ 3 的意思是确认有 3 个数据包被收到了）； 重传丢失的数据包； 如果再收到重复的 ACK，那么 cwnd 增加 1； 如果收到新数据的 ACK 后，把 cwnd 设置为第一步中的 ssthresh 的值，原因是该 ACK 确认了新的数据，说明从 duplicated ACK 时的数据都已收到，该恢复过程已经结束，可以回到恢复之前的状态了，也即再次进入拥塞避免状态；","link":"/posts/58678.html"}],"tags":[{"name":"分布式事务","slug":"分布式事务","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"name":"面试题","slug":"面试题","link":"/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"name":"JVM","slug":"JVM","link":"/tags/JVM/"},{"name":"Java 8","slug":"Java-8","link":"/tags/Java-8/"},{"name":"TODO","slug":"TODO","link":"/tags/TODO/"},{"name":"Mybatis","slug":"Mybatis","link":"/tags/Mybatis/"},{"name":"分布式","slug":"分布式","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"分布式缓存","slug":"分布式缓存","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"name":"微服务","slug":"微服务","link":"/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"排序","slug":"排序","link":"/tags/%E6%8E%92%E5%BA%8F/"},{"name":"微服务安全","slug":"微服务安全","link":"/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%AE%89%E5%85%A8/"},{"name":"OAuth2","slug":"OAuth2","link":"/tags/OAuth2/"},{"name":"行为型模式","slug":"行为型模式","link":"/tags/%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F/"},{"name":"结构型模式","slug":"结构型模式","link":"/tags/%E7%BB%93%E6%9E%84%E5%9E%8B%E6%A8%A1%E5%BC%8F/"},{"name":"创建型模式","slug":"创建型模式","link":"/tags/%E5%88%9B%E5%BB%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F/"},{"name":"零拷贝","slug":"零拷贝","link":"/tags/%E9%9B%B6%E6%8B%B7%E8%B4%9D/"},{"name":"泛型","slug":"泛型","link":"/tags/%E6%B3%9B%E5%9E%8B/"},{"name":"Netty","slug":"Netty","link":"/tags/Netty/"}],"categories":[{"name":"MySQL","slug":"MySQL","link":"/categories/MySQL/"},{"name":"SpringCloud","slug":"SpringCloud","link":"/categories/SpringCloud/"},{"name":"分布式","slug":"分布式","link":"/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"数据结构与算法","slug":"数据结构与算法","link":"/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"},{"name":"设计模式","slug":"设计模式","link":"/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"JavaSE","slug":"JavaSE","link":"/categories/JavaSE/"},{"name":"Dubbo","slug":"Dubbo","link":"/categories/Dubbo/"},{"name":"DDD","slug":"DDD","link":"/categories/DDD/"},{"name":"Git","slug":"Git","link":"/categories/Git/"},{"name":"JVM","slug":"JVM","link":"/categories/JVM/"},{"name":"Java并发编程","slug":"Java并发编程","link":"/categories/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"},{"name":"Mybatis","slug":"Mybatis","link":"/categories/Mybatis/"},{"name":"日志","slug":"日志","link":"/categories/%E6%97%A5%E5%BF%97/"},{"name":"Netty","slug":"Netty","link":"/categories/Netty/"},{"name":"Spring","slug":"Spring","link":"/categories/Spring/"},{"name":"微服务架构","slug":"微服务架构","link":"/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84/"},{"name":"计算机网络","slug":"计算机网络","link":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"设计","slug":"设计","link":"/categories/%E8%AE%BE%E8%AE%A1/"},{"name":"Hbase","slug":"Hbase","link":"/categories/Hbase/"}]}